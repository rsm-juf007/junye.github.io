---
title: "Poisson Regression Examples"
author: "Junye Fan"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data

This dataset contains information on 1,500 mature engineering firms and includes variables on patenting activity, geographic location, firm age, and Blueprinty software usage. The primary goal is to assess whether firms using Blueprinty's software are more successful in obtaining patents.

::: {.callout-note collapse="true"}
### Variable Definitions

| Variable      | Description                                                        |
|---------------|--------------------------------------------------------------------|
| `patents`     | Number of patents awarded to the firm over the past 5 years        |
| `region`      | Region where the firm is located (e.g., Northeast, Midwest, etc.)  |
| `age`         | Number of years since the firm's incorporation                     |
| `iscustomer`  | Blueprinty customer indicator (1 = firm uses Blueprinty, 0 = not)  |

:::

<!-- _todo: Compare histograms and means of number of patents by customer status. What do you observe?_ -->

Based on the bar chart comparing the average number of patents, we observe that Blueprinty customers have a higher average number of patents awarded over the past five years compared to non-customers. Specifically, customers average **4.13 patents**, while non-customers average **closer to 3.5 (3.47)**. This suggests that firms using Blueprinty’s software may be more successful in obtaining patents. 

This conclusion is further supported by the histogram comparing the full distribution of patent counts across customer groups. The distribution for Blueprinty customers is visibly shifted to the right, indicating a greater concentration of firms with 4 or more patents. In contrast, non-customers are more heavily represented in the 0 to 2 patent range, with relatively fewer firms reaching the higher patent counts observed among customers. The histogram also shows a longer right tail for customers, with more firms reaching double-digit patent counts.

Taken together, both the difference in means and the shape of the distribution suggest that Blueprinty customers, on average, have higher patenting activity. However, it is important to note that this is a descriptive comparison. These differences may be influenced by other factors, such as firm age or geographic region, which are not yet accounted for in this analysis.

![Patent Count Distribution by Customer Status](output1.png)

:::: {.callout-note collapse="true"}
### plot code
```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("blueprinty.csv")
sns.set_style("white")

plt.figure(figsize=(10, 6))
ax = sns.histplot(
    data=df,
    x='patents',
    hue='iscustomer',
    bins=30,
    palette='Set2',
    multiple='dodge'
)

for bar in ax.patches:
    bar.set_edgecolor("black")
    bar.set_linewidth(1)

plt.title("Patent Count Distribution by Customer Status")
plt.xlabel("Number of Patents (past 5 years)")
plt.ylabel("Number of Firms")
plt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])
plt.tight_layout()
plt.show()
```
::::

Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

<!-- _todo: Compare regions and ages by customer status. What do you observe?_ -->

When comparing company age by customer status, we observe that Blueprinty customers are slightly older than non-customers. On average, customer firms have been incorporated for about 26.9 years, while non-customers average 26.1 years. Although the difference is small, it may suggest that Blueprinty customers are marginally more established or experienced.

![Company Age Distribution by Customer Status](output2.png)

:::: {.callout-note collapse="true"}
### plot code
```python
age_means = df.groupby("iscustomer")["age"].mean().rename(index={0: "Non-Customer", 1: "Customer"})
print("Mean Company Age:\n", age_means)

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='age', hue='iscustomer', bins=30, palette='Set2', multiple='dodge')
plt.title("Company Age Distribution by Customer Status")
plt.xlabel("Company Age (Years)")
plt.ylabel("Number of Firms")
plt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])
plt.tight_layout()
plt.show()
```
::::

Regional differences, however, are much more pronounced. In the Northeast, more than 54% of firms are Blueprinty customers, making it the only region where customers outnumber non-customers. In all other regions—such as the Midwest, South, Southwest, and Northwest—Blueprinty customers represent less than 20% of firms. This shows that Blueprinty has a particularly strong presence in the Northeast, while adoption is much lower in other parts of the country.

![Distribution of Firms by Region and Customer Status](output3.png)

:::: {.callout-note collapse="true"}
### plot code
```python
plt.figure(figsize=(10, 6))
sns.histplot(
    data=df,
    x='region',
    hue='iscustomer',
    multiple='dodge',
    shrink=0.8,
    palette='Set2',
    stat='count',
    edgecolor='black'
)

plt.title("Distribution of Firms by Region and Customer Status")
plt.xlabel("Region")
plt.ylabel("Number of Firms")
plt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])

plt.grid(False)

plt.tight_layout()
plt.show()
```
:::

These observations highlight important systematic differences between customers and non-customers. Since customer status is not randomly assigned, it’s essential to account for age and regional factors when evaluating the effect of Blueprinty software on patent outcomes.


### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

<!-- _todo: Write down mathematically the likelihood for_ $Y \sim \text{Poisson}(\lambda)$. Note that $f(Y|\lambda) = e^{-\lambda}\lambda^Y/Y!$. -->

Given that $Y \sim \text{Poisson}(\lambda)$, the **likelihood function** is:

$$
L(\lambda \mid Y) = \prod_{i=1}^n \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
$$

Taking logs, the **log-likelihood function** becomes:

$$
\ell(\lambda) = \sum_{i=1}^n \left[ -\lambda + Y_i \log(\lambda) - \log(Y_i!) \right]
$$

<!-- _todo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:_ -->


```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import gammaln  # log(y!) for numerical stability

def poisson_loglikelihood(lmbda, Y):
    if lmbda <= 0:
        return -np.inf  # log likelihood is undefined for λ <= 0
    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))

lambda_vals = np.linspace(0.1, 10, 200)
loglik_vals = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_vals]

plt.figure(figsize=(8, 5))
plt.plot(lambda_vals, loglik_vals, color='darkblue')
plt.title("Log-Likelihood of Poisson Model")
plt.xlabel("Lambda (λ)")
plt.ylabel("Log-Likelihood")
plt.axvline(np.mean(Y), color='red', linestyle='--', label='Mean of Y')
plt.legend()
plt.tight_layout()
plt.show()
```

<!-- _todo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y)._ -->

![Log-Likelihood of Poisson Model](output4.png)


<!-- _todo: If you're feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which "feels right" because the mean of a Poisson distribution is lambda._ -->

Let's consider the log-likelihood of a Poisson model where $Y_i \sim \text{Poisson}(\lambda)$:

$$
\ell(\lambda) = \sum_{i=1}^{n} \left( -\lambda + Y_i \log(\lambda) - \log(Y_i!) \right)
$$

Taking the first derivative with respect to $\lambda$:

$$
\frac{d\ell(\lambda)}{d\lambda} = \sum_{i=1}^n \left( -1 + \frac{Y_i}{\lambda} \right)
= -n + \frac{1}{\lambda} \sum_{i=1}^n Y_i
$$

Setting the derivative equal to zero to find the maximum likelihood estimate (MLE):

$$
-n + \frac{1}{\lambda} \sum_{i=1}^n Y_i = 0
$$

Solve for $\lambda$:

$$
\lambda_{\text{MLE}} = \frac{1}{n} \sum_{i=1}^n Y_i = \bar{Y}
$$

Thus, the MLE for $\lambda$ is simply the **sample mean** of $Y$, which intuitively makes sense since the Poisson distribution has its mean equal to $\lambda$.

<!-- _todo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python._ -->

We used numerical optimization to estimate the maximum likelihood value of λ in the Poisson model. The result, λ̂ = 3.6847, matches exactly with the sample mean of the observed data. This confirms our analytical result that the MLE of λ is simply the average of Y in a Poisson setting.

:::: {.callout-note collapse="true"}
### code
```python
import numpy as np
import pandas as pd
from scipy.optimize import minimize_scalar
from scipy.special import gammaln

def poisson_loglikelihood(lmbda, Y):
    if lmbda <= 0:
        return -np.inf
    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))

neg_loglik = lambda lmbda: -poisson_loglikelihood(lmbda, Y)

result = minimize_scalar(neg_loglik, bounds=(0.01, 20), method='bounded')

lambda_mle = result.x
print(f"MLE of λ (via optimization): {lambda_mle:.4f}")
print(f"Sample mean of Y (baseline): {np.mean(Y):.4f}")
```
:::

### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

<!-- _todo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that_ $\lambda_i = e^{X_i'\beta}$._For example:_

```
poisson_regression_likelihood <- function(beta, Y, X){
   ...
}
``` -->


$$
Y_i \sim \text{Poisson}(\lambda_i), \quad \text{where} \quad \lambda_i = \exp(X_i^\top \beta)
$$

This ensures that $\lambda_i > 0$ for all $i$. The log-likelihood function for this model is:

$$
\ell(\beta) = \sum_{i=1}^n \left( -\lambda_i + Y_i \log(\lambda_i) - \log(Y_i!) \right)
= \sum_{i=1}^n \left( -\exp(X_i^\top \beta) + Y_i X_i^\top \beta - \log(Y_i!) \right)
$$

We now implement this in Python.

### Code: Poisson Log-Likelihood Function with Covariates

```python
import numpy as np
import pandas as pd
from scipy.special import gammaln

# Feature engineering
df["age_scaled"] = df["age"] / 10
df["age_sq_scaled"] = (df["age"] ** 2) / 100

# Construct design matrix X
region_dummies = pd.get_dummies(df["region"], drop_first=True)
X = pd.concat([
    pd.Series(1, index=df.index, name="intercept"),
    df[["age_scaled", "age_sq_scaled", "iscustomer"]],
    region_dummies
], axis=1)
X_matrix = X.values
Y = df["patents"].values
```

<!-- _todo: Use your function along with R's optim() or Python's sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1's to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors._ -->

<!-- _todo: Check your results using R's glm() function or Python sm.GLM() function._

_todo: Interpret the results._  -->


I estimated a Poisson regression model where the number of patents is modeled as a function of firm age (scaled), age squared, customer status, and regional dummy variables. The fitted coefficients and their standard errors are shown in the table below:

| Variable        | Coefficient | Std. Error |
|----------------|-------------|------------|
| Intercept      | -0.509      | 0.183      |
| Age (scaled)   | 1.486       | 0.139      |
| Age² (scaled)  | -0.297      | 0.026      |
| IsCustomer     | **0.208**   | **0.031**  |
| Northeast      | 0.029       | 0.044      |
| Northwest      | -0.018      | 0.054      |
| South          | 0.057       | 0.053      |
| Southwest      | 0.051       | 0.047      |

:::: {.callout-note collapse="true"}
### code
```python
# Feature scaling
df["age_scaled"] = df["age"] / 10
df["age_sq_scaled"] = (df["age"] ** 2) / 100

# Construct design matrix
region_dummies = pd.get_dummies(df["region"], drop_first=True)
X = pd.concat([
    pd.Series(1.0, index=df.index, name="intercept"),
    df[["age_scaled", "age_sq_scaled", "iscustomer"]],
    region_dummies
], axis=1).astype(float)
X_matrix = X.values
Y = df["patents"].values

# Define Poisson log-likelihood
def poisson_loglikelihood_beta(beta, Y, X):
    XB = X @ beta
    lambdas = np.exp(XB)
    if np.any(np.isnan(lambdas)) or np.any(np.isinf(lambdas)):
        return np.inf
    loglik = np.sum(-lambdas + Y * XB - gammaln(Y + 1))
    return -loglik  # minimize negative log-likelihood

# Optimize
beta_start = np.zeros(X.shape[1])
res = minimize(poisson_loglikelihood_beta, beta_start, args=(Y, X_matrix), method="BFGS")

# Extract coefficient estimates and standard errors
beta_hat = res.x
hessian_inv = res.hess_inv
standard_errors = np.sqrt(np.diag(hessian_inv))

# Create result table
results = pd.DataFrame({
    "Coefficient": beta_hat,
    "Std. Error": standard_errors
}, index=X.columns)

print(results)
```
::::


### Model Validation Using `sm.GLM()`

To validate our MLE results, I also fit the same Poisson regression model using Python's `statsmodels.api.GLM()` function. The results closely match the custom optimization estimates, confirming the consistency and correctness of the likelihood-based approach.

The `iscustomer` coefficient is positive and statistically significant. Since the model uses a log link, we interpret the coefficient of 0.208 as follows:

> Firms using Blueprinty are expected to have approximately **23% more patents**, all else equal, since $e^{0.208} \approx 1.231$.

This suggests that using Blueprinty's software is associated with increased patent success.


In addition, the model suggests that company age has a positive effect on patenting up to a point (since the coefficient for `age_scaled` is positive), but the negative coefficient on `age_sq_scaled` indicates diminishing returns as firms get older. Regional effects appear small and are not statistically significant.

Overall, the model supports the hypothesis that using Blueprinty's software is associated with higher patenting activity, even after controlling for age and region.

### Counterfactual Analysis: What if Every Firm Used Blueprinty?

To better interpret the practical effect of Blueprinty's software on patenting outcomes, I simulate a counterfactual scenario. We use our estimated Poisson regression model to compare predicted outcomes for each firm under two conditions:

- **Scenario 1 (`X_0`)**: All firms are treated as **non-customers** (`iscustomer = 0`)
- **Scenario 2 (`X_1`)**: All firms are treated as **Blueprinty customers** (`iscustomer = 1`)

I then compute the expected number of patents under each condition and take the average difference.

:::: {.callout-note collapse="true"}
### code

```python
# Create counterfactual design matrices
X_0 = X.copy()
X_1 = X.copy()
X_0["iscustomer"] = 0
X_1["iscustomer"] = 1

# Predict expected number of patents under each scenario
y_pred_0 = np.exp(X_0 @ beta_hat)
y_pred_1 = np.exp(X_1 @ beta_hat)

# Calculate average effect
effect_vector = y_pred_1 - y_pred_0
average_effect = np.mean(effect_vector)
print(f"Average predicted increase in patents due to Blueprinty: {average_effect:.4f}")
```
::::

<!-- _todo: What do you conclude about the effect of Blueprinty's software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences._ -->

Based on our counterfactual simulation using the estimated Poisson regression model, firms that use Blueprinty are predicted to receive 0.79 more patents, on average, over a five-year period compared to if they did not use the software.

![Predicted Patents under Counterfactual Scenarios](output5.png)

:::: {.callout-note collapse="true"}
### plot code
```python
y_pred_0 = np.exp(X_0 @ beta_hat)
y_pred_1 = np.exp(X_1 @ beta_hat)

avg_y_0 = np.mean(y_pred_0)
avg_y_1 = np.mean(y_pred_1)


comparison_df = pd.DataFrame({
    "Scenario": ["All Non-Customers", "All Customers"],
    "Average Predicted Patents": [avg_y_0, avg_y_1]
})


colors = sns.color_palette("Set2")

plt.figure(figsize=(6, 5))
bars = plt.bar(
    comparison_df["Scenario"],
    comparison_df["Average Predicted Patents"],
    color=[colors[0], colors[1]],
    edgecolor="black"
)

for bar in bars:
    height = bar.get_height()
    plt.text(
        bar.get_x() + bar.get_width() / 2,
        height + 0.05,
        f"{height:.2f}",
        ha='center',
        va='bottom',
        fontsize=11
    )

plt.title("Predicted Average Number of Patents\nUnder Counterfactual Scenarios")
plt.ylabel("Average Predicted Patents")
plt.ylim(0, max(avg_y_0, avg_y_1) + 1)
plt.tight_layout()
plt.show()
```
:::

This effect holds after controlling for firm age and regional differences, suggesting that Blueprinty’s software is associated with a meaningful increase in patenting success.


## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::


<!-- _todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._ -->

### Exploratory Data Analysis

- The distribution of the number of reviews is highly right-skewed, with most listings receiving fewer than 20 reviews.
- Listings that are **instant bookable** appear to have **more reviews**, based on a comparison of medians in the boxplot.

![Histogram of Reviews](output6.png)

:::: {.callout-note collapse="true"}
### code

```python
vars_used = [
    "number_of_reviews", "room_type", "bathrooms", "bedrooms", "price",
    "review_scores_cleanliness", "review_scores_location", "review_scores_value",
    "instant_bookable"
]
df = df[vars_used]

# Drop rows with missing values
df_clean = df.dropna()

# Convert variables
df_clean["instant_bookable"] = (df_clean["instant_bookable"] == "t").astype(int)
df_dummies = pd.get_dummies(df_clean, columns=["room_type"], drop_first=True)
df_dummies.columns = df_dummies.columns.str.replace(" ", "_")

# Poisson regression
formula = (
    "number_of_reviews ~ bathrooms + bedrooms + price + "
    "review_scores_cleanliness + review_scores_location + "
    "review_scores_value + instant_bookable + "
    "room_type_Private_room + room_type_Shared_room"
)
model = glm(formula=formula, data=df_dummies, family=sm.families.Poisson()).fit()
results_df = pd.DataFrame({
    "Coefficient": model.params,
    "Std. Error": model.bse
})
print(results_df)

reviews = df_clean["number_of_reviews"]

plt.figure(figsize=(8, 5))
sns.histplot(reviews, bins=50, kde=True)
plt.title("Distribution of Number of Reviews")
plt.xlabel("Number of Reviews")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()
```
::::

![Boxplot by Instant Bookable](output7.png)

:::: {.callout-note collapse="true"}
### code

```python
plt.figure(figsize=(8, 5))
sns.boxplot(x="instant_bookable", y="number_of_reviews", data=df_clean)
plt.title("Number of Reviews by Instant Bookable")
plt.xlabel("Instant Bookable (0 = No, 1 = Yes)")
plt.ylabel("Number of Reviews")
plt.tight_layout()
plt.show()
```
::::

### Data Cleaning

- Dropped listings with missing values in any relevant modeling variable.
- Converted `instant_bookable` to a binary variable (1 for 't', 0 for 'f').
- Encoded `room_type` as dummy variables, using "Entire home/apt" as the reference category.

### Poisson Regression Model

I used the following covariates to explain variation in the number of reviews:
- `bathrooms`, `bedrooms`, `price`
- `review_scores_cleanliness`, `review_scores_location`, `review_scores_value`
- `instant_bookable` (binary)
- `room_type_Private_room`, `room_type_Shared_room` (dummies)

:::: {.callout-note collapse="true"}
### code

```python
import pandas as pd
import statsmodels.api as sm
from statsmodels.formula.api import glm

df = pd.read_csv("airbnb.csv")
vars_used = [
    "number_of_reviews", "room_type", "bathrooms", "bedrooms", "price",
    "review_scores_cleanliness", "review_scores_location", "review_scores_value",
    "instant_bookable"
]
df = df[vars_used]

# Drop rows with missing values
df_clean = df.dropna()

# Convert variables
df_clean["instant_bookable"] = (df_clean["instant_bookable"] == "t").astype(int)
df_dummies = pd.get_dummies(df_clean, columns=["room_type"], drop_first=True)
df_dummies.columns = df_dummies.columns.str.replace(" ", "_")

# Poisson regression
formula = (
    "number_of_reviews ~ bathrooms + bedrooms + price + "
    "review_scores_cleanliness + review_scores_location + "
    "review_scores_value + instant_bookable + "
    "room_type_Private_room + room_type_Shared_room"
)
model = glm(formula=formula, data=df_dummies, family=sm.families.Poisson()).fit()
results_df = pd.DataFrame({
    "Coefficient": model.params,
    "Std. Error": model.bse
})
print(results_df)
```
::::


### Variable Interpretations
The estimated model is:


| Variable                      | Coefficient | Interpretation                                                                 |
|------------------------------|-------------|--------------------------------------------------------------------------------|
| **Intercept**                | 3.572       | The baseline log expected number of reviews for a reference listing            |
| **bathrooms**                | -0.124      | Each additional bathroom is associated with a ~11.6% **decrease** in expected reviews: $e^{-0.124} \approx 0.883$ |
| **bedrooms**                 | +0.075      | Each additional bedroom is associated with ~7.8% **increase** in reviews: $e^{0.075} \approx 1.078$ |
| **price**                    | -0.000014   | The effect is small and negative; more expensive listings receive slightly fewer reviews |
| **review_scores_cleanliness**| +0.113      | A one-point increase in cleanliness score leads to ~12% more reviews: $e^{0.113} \approx 1.12$ |
| **review_scores_location**   | -0.077      | A higher location score is associated with slightly fewer reviews; possible saturation in popular areas |
| **review_scores_value**      | -0.092      | Higher value scores correspond to fewer reviews; possibly reflects different guest expectations |
| **instant_bookable**         | +0.334      | Instant booking listings receive ~40% more reviews: $e^{0.334} \approx 1.40$ |
| **room_type_Private_room**   | -0.015      | Little difference from entire homes; slightly fewer reviews |
| **room_type_Shared_room**    | -0.252      | Shared rooms get ~22% fewer reviews: $e^{-0.252} \approx 0.78$ |

### Summary

- The strongest positive driver of review volume is **instant booking**, which increases expected review counts by roughly 40%.
- Cleanliness and number of bedrooms are also positively associated with reviews.
- Shared rooms significantly underperform compared to entire homes.
- Price, location, and value scores show weak or negative associations, possibly due to nonlinear effects or omitted variables.

> Because the model uses a log link, the effect of each variable is multiplicative. A coefficient $\beta$ implies that the number of reviews changes by a factor of $e^\beta$ for a one-unit increase in that variable, holding all others constant.



