[
  {
    "objectID": "HW1/project2/hw2_questions.html",
    "href": "HW1/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nThis dataset contains information on 1,500 mature engineering firms and includes variables on patenting activity, geographic location, firm age, and Blueprinty software usage. The primary goal is to assess whether firms using Blueprinty’s software are more successful in obtaining patents.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\npatents\nNumber of patents awarded to the firm over the past 5 years\n\n\nregion\nRegion where the firm is located (e.g., Northeast, Midwest, etc.)\n\n\nage\nNumber of years since the firm’s incorporation\n\n\niscustomer\nBlueprinty customer indicator (1 = firm uses Blueprinty, 0 = not)\n\n\n\n\n\n\n\nBased on the bar chart comparing the average number of patents, we observe that Blueprinty customers have a higher average number of patents awarded over the past five years compared to non-customers. Specifically, customers average 4.13 patents, while non-customers average closer to 3.5 (3.47). This suggests that firms using Blueprinty’s software may be more successful in obtaining patents.\nThis conclusion is further supported by the histogram comparing the full distribution of patent counts across customer groups. The distribution for Blueprinty customers is visibly shifted to the right, indicating a greater concentration of firms with 4 or more patents. In contrast, non-customers are more heavily represented in the 0 to 2 patent range, with relatively fewer firms reaching the higher patent counts observed among customers. The histogram also shows a longer right tail for customers, with more firms reaching double-digit patent counts.\nTaken together, both the difference in means and the shape of the distribution suggest that Blueprinty customers, on average, have higher patenting activity. However, it is important to note that this is a descriptive comparison. These differences may be influenced by other factors, such as firm age or geographic region, which are not yet accounted for in this analysis.\n\n\n\nPatent Count Distribution by Customer Status\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"blueprinty.csv\")\nsns.set_style(\"white\")\n\nplt.figure(figsize=(10, 6))\nax = sns.histplot(\n    data=df,\n    x='patents',\n    hue='iscustomer',\n    bins=30,\n    palette='Set2',\n    multiple='dodge'\n)\n\nfor bar in ax.patches:\n    bar.set_edgecolor(\"black\")\n    bar.set_linewidth(1)\n\nplt.title(\"Patent Count Distribution by Customer Status\")\nplt.xlabel(\"Number of Patents (past 5 years)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\nplt.tight_layout()\nplt.show()\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nWhen comparing company age by customer status, we observe that Blueprinty customers are slightly older than non-customers. On average, customer firms have been incorporated for about 26.9 years, while non-customers average 26.1 years. Although the difference is small, it may suggest that Blueprinty customers are marginally more established or experienced.\n\n\n\nCompany Age Distribution by Customer Status\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nage_means = df.groupby(\"iscustomer\")[\"age\"].mean().rename(index={0: \"Non-Customer\", 1: \"Customer\"})\nprint(\"Mean Company Age:\\n\", age_means)\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x='age', hue='iscustomer', bins=30, palette='Set2', multiple='dodge')\nplt.title(\"Company Age Distribution by Customer Status\")\nplt.xlabel(\"Company Age (Years)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\nplt.tight_layout()\nplt.show()\n\n\n\nRegional differences, however, are much more pronounced. In the Northeast, more than 54% of firms are Blueprinty customers, making it the only region where customers outnumber non-customers. In all other regions—such as the Midwest, South, Southwest, and Northwest—Blueprinty customers represent less than 20% of firms. This shows that Blueprinty has a particularly strong presence in the Northeast, while adoption is much lower in other parts of the country.\n\n\n\nDistribution of Firms by Region and Customer Status\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nsns.histplot(\n    data=df,\n    x='region',\n    hue='iscustomer',\n    multiple='dodge',\n    shrink=0.8,\n    palette='Set2',\n    stat='count',\n    edgecolor='black'\n)\n\nplt.title(\"Distribution of Firms by Region and Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\n\nplt.grid(False)\n\nplt.tight_layout()\nplt.show()\n\n\n\nThese observations highlight important systematic differences between customers and non-customers. Since customer status is not randomly assigned, it’s essential to account for age and regional factors when evaluating the effect of Blueprinty software on patent outcomes.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nGiven that \\(Y \\sim \\text{Poisson}(\\lambda)\\), the likelihood function is:\n\\[\nL(\\lambda \\mid Y) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking logs, the log-likelihood function becomes:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln  # log(y!) for numerical stability\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf  # log likelihood is undefined for λ &lt;= 0\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='darkblue')\nplt.title(\"Log-Likelihood of Poisson Model\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.axvline(np.mean(Y), color='red', linestyle='--', label='Mean of Y')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\nLog-Likelihood of Poisson Model\n\n\n\nLet’s consider the log-likelihood of a Poisson model where \\(Y_i \\sim \\text{Poisson}(\\lambda)\\):\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n\\]\nTaking the first derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell(\\lambda)}{d\\lambda} = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSetting the derivative equal to zero to find the maximum likelihood estimate (MLE):\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\]\nSolve for \\(\\lambda\\):\n\\[\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThus, the MLE for \\(\\lambda\\) is simply the sample mean of \\(Y\\), which intuitively makes sense since the Poisson distribution has its mean equal to \\(\\lambda\\).\n\nWe used numerical optimization to estimate the maximum likelihood value of λ in the Poisson model. The result, λ̂ = 3.6847, matches exactly with the sample mean of the observed data. This confirms our analytical result that the MLE of λ is simply the average of Y in a Poisson setting.\n\n\n\n\n\n\ncode\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\nneg_loglik = lambda lmbda: -poisson_loglikelihood(lmbda, Y)\n\nresult = minimize_scalar(neg_loglik, bounds=(0.01, 20), method='bounded')\n\nlambda_mle = result.x\nprint(f\"MLE of λ (via optimization): {lambda_mle:.4f}\")\nprint(f\"Sample mean of Y (baseline): {np.mean(Y):.4f}\")\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\text{where} \\quad \\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nThis ensures that \\(\\lambda_i &gt; 0\\) for all \\(i\\). The log-likelihood function for this model is:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\left( -\\lambda_i + Y_i \\log(\\lambda_i) - \\log(Y_i!) \\right)\n= \\sum_{i=1}^n \\left( -\\exp(X_i^\\top \\beta) + Y_i X_i^\\top \\beta - \\log(Y_i!) \\right)\n\\]\nWe now implement this in Python.\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\n\n# Feature engineering\ndf[\"age_scaled\"] = df[\"age\"] / 10\ndf[\"age_sq_scaled\"] = (df[\"age\"] ** 2) / 100\n\n# Construct design matrix X\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age_scaled\", \"age_sq_scaled\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\nX_matrix = X.values\nY = df[\"patents\"].values\n\n\nI estimated a Poisson regression model where the number of patents is modeled as a function of firm age (scaled), age squared, customer status, and regional dummy variables. The fitted coefficients and their standard errors are shown in the table below:\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\nIntercept\n-0.509\n0.183\n\n\nAge (scaled)\n1.486\n0.139\n\n\nAge² (scaled)\n-0.297\n0.026\n\n\nIsCustomer\n0.208\n0.031\n\n\nNortheast\n0.029\n0.044\n\n\nNorthwest\n-0.018\n0.054\n\n\nSouth\n0.057\n0.053\n\n\nSouthwest\n0.051\n0.047\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n# Feature scaling\ndf[\"age_scaled\"] = df[\"age\"] / 10\ndf[\"age_sq_scaled\"] = (df[\"age\"] ** 2) / 100\n\n# Construct design matrix\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX = pd.concat([\n    pd.Series(1.0, index=df.index, name=\"intercept\"),\n    df[[\"age_scaled\", \"age_sq_scaled\", \"iscustomer\"]],\n    region_dummies\n], axis=1).astype(float)\nX_matrix = X.values\nY = df[\"patents\"].values\n\n# Define Poisson log-likelihood\ndef poisson_loglikelihood_beta(beta, Y, X):\n    XB = X @ beta\n    lambdas = np.exp(XB)\n    if np.any(np.isnan(lambdas)) or np.any(np.isinf(lambdas)):\n        return np.inf\n    loglik = np.sum(-lambdas + Y * XB - gammaln(Y + 1))\n    return -loglik  # minimize negative log-likelihood\n\n# Optimize\nbeta_start = np.zeros(X.shape[1])\nres = minimize(poisson_loglikelihood_beta, beta_start, args=(Y, X_matrix), method=\"BFGS\")\n\n# Extract coefficient estimates and standard errors\nbeta_hat = res.x\nhessian_inv = res.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Create result table\nresults = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X.columns)\n\nprint(results)\n\n\n\n\n\n\nTo validate our MLE results, I also fit the same Poisson regression model using Python’s statsmodels.api.GLM() function. The results closely match the custom optimization estimates, confirming the consistency and correctness of the likelihood-based approach.\nThe iscustomer coefficient is positive and statistically significant. Since the model uses a log link, we interpret the coefficient of 0.208 as follows:\n\nFirms using Blueprinty are expected to have approximately 23% more patents, all else equal, since \\(e^{0.208} \\approx 1.231\\).\n\nThis suggests that using Blueprinty’s software is associated with increased patent success.\nIn addition, the model suggests that company age has a positive effect on patenting up to a point (since the coefficient for age_scaled is positive), but the negative coefficient on age_sq_scaled indicates diminishing returns as firms get older. Regional effects appear small and are not statistically significant.\nOverall, the model supports the hypothesis that using Blueprinty’s software is associated with higher patenting activity, even after controlling for age and region.\n\n\n\nTo better interpret the practical effect of Blueprinty’s software on patenting outcomes, I simulate a counterfactual scenario. We use our estimated Poisson regression model to compare predicted outcomes for each firm under two conditions:\n\nScenario 1 (X_0): All firms are treated as non-customers (iscustomer = 0)\nScenario 2 (X_1): All firms are treated as Blueprinty customers (iscustomer = 1)\n\nI then compute the expected number of patents under each condition and take the average difference.\n\n\n\n\n\n\ncode\n\n\n\n\n\n# Create counterfactual design matrices\nX_0 = X.copy()\nX_1 = X.copy()\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected number of patents under each scenario\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\n# Calculate average effect\neffect_vector = y_pred_1 - y_pred_0\naverage_effect = np.mean(effect_vector)\nprint(f\"Average predicted increase in patents due to Blueprinty: {average_effect:.4f}\")\n\n\n\n\nBased on our counterfactual simulation using the estimated Poisson regression model, firms that use Blueprinty are predicted to receive 0.79 more patents, on average, over a five-year period compared to if they did not use the software.\n\n\n\nPredicted Patents under Counterfactual Scenarios\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\navg_y_0 = np.mean(y_pred_0)\navg_y_1 = np.mean(y_pred_1)\n\n\ncomparison_df = pd.DataFrame({\n    \"Scenario\": [\"All Non-Customers\", \"All Customers\"],\n    \"Average Predicted Patents\": [avg_y_0, avg_y_1]\n})\n\n\ncolors = sns.color_palette(\"Set2\")\n\nplt.figure(figsize=(6, 5))\nbars = plt.bar(\n    comparison_df[\"Scenario\"],\n    comparison_df[\"Average Predicted Patents\"],\n    color=[colors[0], colors[1]],\n    edgecolor=\"black\"\n)\n\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(\n        bar.get_x() + bar.get_width() / 2,\n        height + 0.05,\n        f\"{height:.2f}\",\n        ha='center',\n        va='bottom',\n        fontsize=11\n    )\n\nplt.title(\"Predicted Average Number of Patents\\nUnder Counterfactual Scenarios\")\nplt.ylabel(\"Average Predicted Patents\")\nplt.ylim(0, max(avg_y_0, avg_y_1) + 1)\nplt.tight_layout()\nplt.show()\n\n\n\nThis effect holds after controlling for firm age and regional differences, suggesting that Blueprinty’s software is associated with a meaningful increase in patenting success."
  },
  {
    "objectID": "HW1/project2/hw2_questions.html#blueprinty-case-study",
    "href": "HW1/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nThis dataset contains information on 1,500 mature engineering firms and includes variables on patenting activity, geographic location, firm age, and Blueprinty software usage. The primary goal is to assess whether firms using Blueprinty’s software are more successful in obtaining patents.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\npatents\nNumber of patents awarded to the firm over the past 5 years\n\n\nregion\nRegion where the firm is located (e.g., Northeast, Midwest, etc.)\n\n\nage\nNumber of years since the firm’s incorporation\n\n\niscustomer\nBlueprinty customer indicator (1 = firm uses Blueprinty, 0 = not)\n\n\n\n\n\n\n\nBased on the bar chart comparing the average number of patents, we observe that Blueprinty customers have a higher average number of patents awarded over the past five years compared to non-customers. Specifically, customers average 4.13 patents, while non-customers average closer to 3.5 (3.47). This suggests that firms using Blueprinty’s software may be more successful in obtaining patents.\nThis conclusion is further supported by the histogram comparing the full distribution of patent counts across customer groups. The distribution for Blueprinty customers is visibly shifted to the right, indicating a greater concentration of firms with 4 or more patents. In contrast, non-customers are more heavily represented in the 0 to 2 patent range, with relatively fewer firms reaching the higher patent counts observed among customers. The histogram also shows a longer right tail for customers, with more firms reaching double-digit patent counts.\nTaken together, both the difference in means and the shape of the distribution suggest that Blueprinty customers, on average, have higher patenting activity. However, it is important to note that this is a descriptive comparison. These differences may be influenced by other factors, such as firm age or geographic region, which are not yet accounted for in this analysis.\n\n\n\nPatent Count Distribution by Customer Status\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"blueprinty.csv\")\nsns.set_style(\"white\")\n\nplt.figure(figsize=(10, 6))\nax = sns.histplot(\n    data=df,\n    x='patents',\n    hue='iscustomer',\n    bins=30,\n    palette='Set2',\n    multiple='dodge'\n)\n\nfor bar in ax.patches:\n    bar.set_edgecolor(\"black\")\n    bar.set_linewidth(1)\n\nplt.title(\"Patent Count Distribution by Customer Status\")\nplt.xlabel(\"Number of Patents (past 5 years)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\nplt.tight_layout()\nplt.show()\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nWhen comparing company age by customer status, we observe that Blueprinty customers are slightly older than non-customers. On average, customer firms have been incorporated for about 26.9 years, while non-customers average 26.1 years. Although the difference is small, it may suggest that Blueprinty customers are marginally more established or experienced.\n\n\n\nCompany Age Distribution by Customer Status\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nage_means = df.groupby(\"iscustomer\")[\"age\"].mean().rename(index={0: \"Non-Customer\", 1: \"Customer\"})\nprint(\"Mean Company Age:\\n\", age_means)\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x='age', hue='iscustomer', bins=30, palette='Set2', multiple='dodge')\nplt.title(\"Company Age Distribution by Customer Status\")\nplt.xlabel(\"Company Age (Years)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\nplt.tight_layout()\nplt.show()\n\n\n\nRegional differences, however, are much more pronounced. In the Northeast, more than 54% of firms are Blueprinty customers, making it the only region where customers outnumber non-customers. In all other regions—such as the Midwest, South, Southwest, and Northwest—Blueprinty customers represent less than 20% of firms. This shows that Blueprinty has a particularly strong presence in the Northeast, while adoption is much lower in other parts of the country.\n\n\n\nDistribution of Firms by Region and Customer Status\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nsns.histplot(\n    data=df,\n    x='region',\n    hue='iscustomer',\n    multiple='dodge',\n    shrink=0.8,\n    palette='Set2',\n    stat='count',\n    edgecolor='black'\n)\n\nplt.title(\"Distribution of Firms by Region and Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\n\nplt.grid(False)\n\nplt.tight_layout()\nplt.show()\n\n\n\nThese observations highlight important systematic differences between customers and non-customers. Since customer status is not randomly assigned, it’s essential to account for age and regional factors when evaluating the effect of Blueprinty software on patent outcomes.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nGiven that \\(Y \\sim \\text{Poisson}(\\lambda)\\), the likelihood function is:\n\\[\nL(\\lambda \\mid Y) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking logs, the log-likelihood function becomes:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln  # log(y!) for numerical stability\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf  # log likelihood is undefined for λ &lt;= 0\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='darkblue')\nplt.title(\"Log-Likelihood of Poisson Model\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.axvline(np.mean(Y), color='red', linestyle='--', label='Mean of Y')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\nLog-Likelihood of Poisson Model\n\n\n\nLet’s consider the log-likelihood of a Poisson model where \\(Y_i \\sim \\text{Poisson}(\\lambda)\\):\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n\\]\nTaking the first derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell(\\lambda)}{d\\lambda} = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSetting the derivative equal to zero to find the maximum likelihood estimate (MLE):\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\]\nSolve for \\(\\lambda\\):\n\\[\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThus, the MLE for \\(\\lambda\\) is simply the sample mean of \\(Y\\), which intuitively makes sense since the Poisson distribution has its mean equal to \\(\\lambda\\).\n\nWe used numerical optimization to estimate the maximum likelihood value of λ in the Poisson model. The result, λ̂ = 3.6847, matches exactly with the sample mean of the observed data. This confirms our analytical result that the MLE of λ is simply the average of Y in a Poisson setting.\n\n\n\n\n\n\ncode\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\nneg_loglik = lambda lmbda: -poisson_loglikelihood(lmbda, Y)\n\nresult = minimize_scalar(neg_loglik, bounds=(0.01, 20), method='bounded')\n\nlambda_mle = result.x\nprint(f\"MLE of λ (via optimization): {lambda_mle:.4f}\")\nprint(f\"Sample mean of Y (baseline): {np.mean(Y):.4f}\")\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\text{where} \\quad \\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nThis ensures that \\(\\lambda_i &gt; 0\\) for all \\(i\\). The log-likelihood function for this model is:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\left( -\\lambda_i + Y_i \\log(\\lambda_i) - \\log(Y_i!) \\right)\n= \\sum_{i=1}^n \\left( -\\exp(X_i^\\top \\beta) + Y_i X_i^\\top \\beta - \\log(Y_i!) \\right)\n\\]\nWe now implement this in Python.\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\n\n# Feature engineering\ndf[\"age_scaled\"] = df[\"age\"] / 10\ndf[\"age_sq_scaled\"] = (df[\"age\"] ** 2) / 100\n\n# Construct design matrix X\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age_scaled\", \"age_sq_scaled\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\nX_matrix = X.values\nY = df[\"patents\"].values\n\n\nI estimated a Poisson regression model where the number of patents is modeled as a function of firm age (scaled), age squared, customer status, and regional dummy variables. The fitted coefficients and their standard errors are shown in the table below:\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\nIntercept\n-0.509\n0.183\n\n\nAge (scaled)\n1.486\n0.139\n\n\nAge² (scaled)\n-0.297\n0.026\n\n\nIsCustomer\n0.208\n0.031\n\n\nNortheast\n0.029\n0.044\n\n\nNorthwest\n-0.018\n0.054\n\n\nSouth\n0.057\n0.053\n\n\nSouthwest\n0.051\n0.047\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n# Feature scaling\ndf[\"age_scaled\"] = df[\"age\"] / 10\ndf[\"age_sq_scaled\"] = (df[\"age\"] ** 2) / 100\n\n# Construct design matrix\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX = pd.concat([\n    pd.Series(1.0, index=df.index, name=\"intercept\"),\n    df[[\"age_scaled\", \"age_sq_scaled\", \"iscustomer\"]],\n    region_dummies\n], axis=1).astype(float)\nX_matrix = X.values\nY = df[\"patents\"].values\n\n# Define Poisson log-likelihood\ndef poisson_loglikelihood_beta(beta, Y, X):\n    XB = X @ beta\n    lambdas = np.exp(XB)\n    if np.any(np.isnan(lambdas)) or np.any(np.isinf(lambdas)):\n        return np.inf\n    loglik = np.sum(-lambdas + Y * XB - gammaln(Y + 1))\n    return -loglik  # minimize negative log-likelihood\n\n# Optimize\nbeta_start = np.zeros(X.shape[1])\nres = minimize(poisson_loglikelihood_beta, beta_start, args=(Y, X_matrix), method=\"BFGS\")\n\n# Extract coefficient estimates and standard errors\nbeta_hat = res.x\nhessian_inv = res.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Create result table\nresults = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X.columns)\n\nprint(results)\n\n\n\n\n\n\nTo validate our MLE results, I also fit the same Poisson regression model using Python’s statsmodels.api.GLM() function. The results closely match the custom optimization estimates, confirming the consistency and correctness of the likelihood-based approach.\nThe iscustomer coefficient is positive and statistically significant. Since the model uses a log link, we interpret the coefficient of 0.208 as follows:\n\nFirms using Blueprinty are expected to have approximately 23% more patents, all else equal, since \\(e^{0.208} \\approx 1.231\\).\n\nThis suggests that using Blueprinty’s software is associated with increased patent success.\nIn addition, the model suggests that company age has a positive effect on patenting up to a point (since the coefficient for age_scaled is positive), but the negative coefficient on age_sq_scaled indicates diminishing returns as firms get older. Regional effects appear small and are not statistically significant.\nOverall, the model supports the hypothesis that using Blueprinty’s software is associated with higher patenting activity, even after controlling for age and region.\n\n\n\nTo better interpret the practical effect of Blueprinty’s software on patenting outcomes, I simulate a counterfactual scenario. We use our estimated Poisson regression model to compare predicted outcomes for each firm under two conditions:\n\nScenario 1 (X_0): All firms are treated as non-customers (iscustomer = 0)\nScenario 2 (X_1): All firms are treated as Blueprinty customers (iscustomer = 1)\n\nI then compute the expected number of patents under each condition and take the average difference.\n\n\n\n\n\n\ncode\n\n\n\n\n\n# Create counterfactual design matrices\nX_0 = X.copy()\nX_1 = X.copy()\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected number of patents under each scenario\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\n# Calculate average effect\neffect_vector = y_pred_1 - y_pred_0\naverage_effect = np.mean(effect_vector)\nprint(f\"Average predicted increase in patents due to Blueprinty: {average_effect:.4f}\")\n\n\n\n\nBased on our counterfactual simulation using the estimated Poisson regression model, firms that use Blueprinty are predicted to receive 0.79 more patents, on average, over a five-year period compared to if they did not use the software.\n\n\n\nPredicted Patents under Counterfactual Scenarios\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\navg_y_0 = np.mean(y_pred_0)\navg_y_1 = np.mean(y_pred_1)\n\n\ncomparison_df = pd.DataFrame({\n    \"Scenario\": [\"All Non-Customers\", \"All Customers\"],\n    \"Average Predicted Patents\": [avg_y_0, avg_y_1]\n})\n\n\ncolors = sns.color_palette(\"Set2\")\n\nplt.figure(figsize=(6, 5))\nbars = plt.bar(\n    comparison_df[\"Scenario\"],\n    comparison_df[\"Average Predicted Patents\"],\n    color=[colors[0], colors[1]],\n    edgecolor=\"black\"\n)\n\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(\n        bar.get_x() + bar.get_width() / 2,\n        height + 0.05,\n        f\"{height:.2f}\",\n        ha='center',\n        va='bottom',\n        fontsize=11\n    )\n\nplt.title(\"Predicted Average Number of Patents\\nUnder Counterfactual Scenarios\")\nplt.ylabel(\"Average Predicted Patents\")\nplt.ylim(0, max(avg_y_0, avg_y_1) + 1)\nplt.tight_layout()\nplt.show()\n\n\n\nThis effect holds after controlling for firm age and regional differences, suggesting that Blueprinty’s software is associated with a meaningful increase in patenting success."
  },
  {
    "objectID": "HW1/project2/hw2_questions.html#airbnb-case-study",
    "href": "HW1/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\n\nExploratory Data Analysis\n\nThe distribution of the number of reviews is highly right-skewed, with most listings receiving fewer than 20 reviews.\nListings that are instant bookable appear to have more reviews, based on a comparison of medians in the boxplot.\n\n\n\n\nHistogram of Reviews\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nvars_used = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\ndf = df[vars_used]\n\n# Drop rows with missing values\ndf_clean = df.dropna()\n\n# Convert variables\ndf_clean[\"instant_bookable\"] = (df_clean[\"instant_bookable\"] == \"t\").astype(int)\ndf_dummies = pd.get_dummies(df_clean, columns=[\"room_type\"], drop_first=True)\ndf_dummies.columns = df_dummies.columns.str.replace(\" \", \"_\")\n\n# Poisson regression\nformula = (\n    \"number_of_reviews ~ bathrooms + bedrooms + price + \"\n    \"review_scores_cleanliness + review_scores_location + \"\n    \"review_scores_value + instant_bookable + \"\n    \"room_type_Private_room + room_type_Shared_room\"\n)\nmodel = glm(formula=formula, data=df_dummies, family=sm.families.Poisson()).fit()\nresults_df = pd.DataFrame({\n    \"Coefficient\": model.params,\n    \"Std. Error\": model.bse\n})\nprint(results_df)\n\nreviews = df_clean[\"number_of_reviews\"]\n\nplt.figure(figsize=(8, 5))\nsns.histplot(reviews, bins=50, kde=True)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Frequency\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nBoxplot by Instant Bookable\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(x=\"instant_bookable\", y=\"number_of_reviews\", data=df_clean)\nplt.title(\"Number of Reviews by Instant Bookable\")\nplt.xlabel(\"Instant Bookable (0 = No, 1 = Yes)\")\nplt.ylabel(\"Number of Reviews\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nData Cleaning\n\nDropped listings with missing values in any relevant modeling variable.\nConverted instant_bookable to a binary variable (1 for ‘t’, 0 for ‘f’).\nEncoded room_type as dummy variables, using “Entire home/apt” as the reference category.\n\n\n\nPoisson Regression Model\nI used the following covariates to explain variation in the number of reviews: - bathrooms, bedrooms, price - review_scores_cleanliness, review_scores_location, review_scores_value - instant_bookable (binary) - room_type_Private_room, room_type_Shared_room (dummies)\n\n\n\n\n\n\ncode\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import glm\n\ndf = pd.read_csv(\"airbnb.csv\")\nvars_used = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\ndf = df[vars_used]\n\n# Drop rows with missing values\ndf_clean = df.dropna()\n\n# Convert variables\ndf_clean[\"instant_bookable\"] = (df_clean[\"instant_bookable\"] == \"t\").astype(int)\ndf_dummies = pd.get_dummies(df_clean, columns=[\"room_type\"], drop_first=True)\ndf_dummies.columns = df_dummies.columns.str.replace(\" \", \"_\")\n\n# Poisson regression\nformula = (\n    \"number_of_reviews ~ bathrooms + bedrooms + price + \"\n    \"review_scores_cleanliness + review_scores_location + \"\n    \"review_scores_value + instant_bookable + \"\n    \"room_type_Private_room + room_type_Shared_room\"\n)\nmodel = glm(formula=formula, data=df_dummies, family=sm.families.Poisson()).fit()\nresults_df = pd.DataFrame({\n    \"Coefficient\": model.params,\n    \"Std. Error\": model.bse\n})\nprint(results_df)\n\n\n\n\n\nVariable Interpretations\nThe estimated model is:\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nInterpretation\n\n\n\n\nIntercept\n3.572\nThe baseline log expected number of reviews for a reference listing\n\n\nbathrooms\n-0.124\nEach additional bathroom is associated with a ~11.6% decrease in expected reviews: \\(e^{-0.124} \\approx 0.883\\)\n\n\nbedrooms\n+0.075\nEach additional bedroom is associated with ~7.8% increase in reviews: \\(e^{0.075} \\approx 1.078\\)\n\n\nprice\n-0.000014\nThe effect is small and negative; more expensive listings receive slightly fewer reviews\n\n\nreview_scores_cleanliness\n+0.113\nA one-point increase in cleanliness score leads to ~12% more reviews: \\(e^{0.113} \\approx 1.12\\)\n\n\nreview_scores_location\n-0.077\nA higher location score is associated with slightly fewer reviews; possible saturation in popular areas\n\n\nreview_scores_value\n-0.092\nHigher value scores correspond to fewer reviews; possibly reflects different guest expectations\n\n\ninstant_bookable\n+0.334\nInstant booking listings receive ~40% more reviews: \\(e^{0.334} \\approx 1.40\\)\n\n\nroom_type_Private_room\n-0.015\nLittle difference from entire homes; slightly fewer reviews\n\n\nroom_type_Shared_room\n-0.252\nShared rooms get ~22% fewer reviews: \\(e^{-0.252} \\approx 0.78\\)\n\n\n\n\n\nSummary\n\nThe strongest positive driver of review volume is instant booking, which increases expected review counts by roughly 40%.\nCleanliness and number of bedrooms are also positively associated with reviews.\nShared rooms significantly underperform compared to entire homes.\nPrice, location, and value scores show weak or negative associations, possibly due to nonlinear effects or omitted variables.\n\n\nBecause the model uses a log link, the effect of each variable is multiplicative. A coefficient \\(\\beta\\) implies that the number of reviews changes by a factor of \\(e^\\beta\\) for a one-unit increase in that variable, holding all others constant."
  },
  {
    "objectID": "HW1.html",
    "href": "HW1.html",
    "title": "My Projects",
    "section": "",
    "text": "Poisson Regression Examples\n\n\n\n\n\n\nJunye Fan\n\n\nMay 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nJunye Fan\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project1/hw1_questions.html",
    "href": "blog/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was designed to test how different types of matching grants—offers to match the donor’s contribution at different rates—would influence both the likelihood of giving and the amount donated. The treatments varied in three main dimensions:\n\nMatch ratio: The donor’s contribution was matched by a leadership donor at a ratio of $1:$1, $2:$1, or $3:$1.\nMaximum match amount: The cap on the matching gift was randomly set at $25,000, $50,000, $100,000, or left unstated.\nSuggested donation amount: Based on the recipient’s highest previous contribution (HPC), the letters included one of three suggestions: HPC × 1.00, HPC × 1.25, or HPC × 1.50.\n\nThese variations were fully randomized, making this a natural field experiment that allows for strong causal inference.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#introduction",
    "href": "blog/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was designed to test how different types of matching grants—offers to match the donor’s contribution at different rates—would influence both the likelihood of giving and the amount donated. The treatments varied in three main dimensions:\n\nMatch ratio: The donor’s contribution was matched by a leadership donor at a ratio of $1:$1, $2:$1, or $3:$1.\nMaximum match amount: The cap on the matching gift was randomly set at $25,000, $50,000, $100,000, or left unstated.\nSuggested donation amount: Based on the recipient’s highest previous contribution (HPC), the letters included one of three suggestions: HPC × 1.00, HPC × 1.25, or HPC × 1.50.\n\nThese variations were fully randomized, making this a natural field experiment that allows for strong causal inference.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#data",
    "href": "blog/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset provided by Karlan and List (2007) is in .dta (Stata) format and includes over 50,000 observations, one for each individual who received a fundraising letter. Each row represents a donor and contains information about the treatment they were assigned to, their prior donation history, demographic characteristics, and whether they donated after receiving the letter.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nTo assess whether the randomization created comparable groups, I performed balance checks on several background variables. These include donation history (mrm2, freq, years), demographics (female, couple), and income indicators (ltmedmra, median_hhincome).\nFor each variable, I compared:\n\nthe mean in the treatment vs control group,\na t-test of the difference in means,\nand a regression coefficient from variable ~ treatment.\n\n\n\n\n\n\n\nbasic code\n\n\n\n\n\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n\n\nvars_to_test = ['mrm2', 'freq', 'years', 'female', 'ltmedmra', 'median_hhincome', 'couple']\n\nresults = []\n\nfor var in vars_to_test:\n    treat = df[df['treatment'] == 1][var]\n    control = df[df['treatment'] == 0][var]\n    t_stat, p_val = ttest_ind(treat, control, nan_policy='omit')\n    \n    model = smf.ols(f'{var} ~ treatment', data=df).fit()\n    coef = model.params['treatment']\n    reg_p = model.pvalues['treatment']\n    \n    results.append({\n        \"Variable\": var,\n        \"Treatment Mean\": round(treat.mean(), 3),\n        \"Control Mean\": round(control.mean(), 3),\n        \"T-Statistic\": round(t_stat, 3),\n        \"P-Value\": round(p_val, 3),\n        \"OLS Coefficient\": round(coef, 3),\n        \"OLS P-Value\": round(reg_p, 3)\n    })\n\nbalance_summary = pd.DataFrame(results)\n\nbalance_summary.loc[balance_summary['Variable'] == 'female', ['Treatment Mean', 'Control Mean']] *= 100\nbalance_summary.loc[balance_summary['Variable'] == 'couple', ['Treatment Mean', 'Control Mean']] *= 100\nbalance_summary.loc[balance_summary['Variable'] == 'ltmedmra', ['Treatment Mean', 'Control Mean']] *= 100\n\nbalance_summary.loc[balance_summary['Variable'] == 'median_hhincome', ['Treatment Mean', 'Control Mean']] = \\\n    balance_summary.loc[balance_summary['Variable'] == 'median_hhincome', ['Treatment Mean', 'Control Mean']].applymap(lambda x: f\"${x:,.0f}\")\n\nbalance_summary\nThe table below summarizes these tests:\n\n\nBalance Test Summaries\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nTreatment Mean\nControl Mean\nT-Statistic\nP-Value\nOLS Coefficient\nOLS P-Value\n\n\n\n\nmrm2\n13.01\n12.998\n0.119\n0.905\n0.014\n0.905\n\n\nfreq\n8.04\n8.05\n-0.111\n0.912\n-0.012\n0.912\n\n\nyears\n6.08\n6.14\n-1.103\n0.270\n-0.058\n0.270\n\n\nfemale\n27.5%\n28.3%\n-1.758\n0.079\n-0.008\n0.079\n\n\nltmedmra\n49.7%\n48.8%\n1.910\n0.056\n0.009\n0.056\n\n\nmedian_hhincome\n$54,763\n$54,921\n-0.742\n0.458\n-157.93\n0.458\n\n\ncouple\n9.14%\n9.30%\n-0.584\n0.559\n-0.002\n0.559\n\n\n\nNone of the above variables show statistically significant differences (all p-values &gt; 0.05). This suggests that the treatment and control groups were balanced at baseline, and that any later difference in outcomes is likely attributable to the treatment itself. These findings mirror Table 1 in Karlan & List (2007)."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#experimental-results",
    "href": "blog/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nI begin by analyzing whether receiving a matching donation offer increases the likelihood of giving. As shown in the bar chart below, the response rate for the treatment group was 2.20%, while the control group had a response rate of only 1.79%.\n\n\n\n\nProportion of People Who Donated\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nresponse_rate = df.groupby('treatment')['gave'].mean().reset_index()\nresponse_rate['treatment'] = response_rate['treatment'].map({0: 'Control', 1: 'Treatment'})\n\nplt.figure(figsize=(6, 4))\nsns.barplot(data=response_rate, x='treatment', y='gave', palette=['#AFCBFF', '#FFD6A5'])\n\nplt.title('Proportion of People Who Donated')\nplt.xlabel('Group')\nplt.ylabel('Donation Rate')\nplt.ylim(0, 0.03)\nplt.grid(axis='y', linestyle='--')\nplt.tight_layout()\n\nplt.savefig('proportion_donated_by_group.png')\nplt.show()\n\n\n\n\nI conduct a two-sample t-test to compare the mean donation rate (gave) between the treatment and control groups:\n\n\n\n\n\n\nt-test code\n\n\n\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\ntreat_group = df[df['treatment'] == 1]['gave']\ncontrol_group = df[df['treatment'] == 0]['gave']\n\nt_stat, p_val = ttest_ind(treat_group, control_group)\nprint(f\"T-test: t = {t_stat:.3f}, p = {p_val:.4f}\")\n\n\n\n\nControl mean: ~1.79%\n\nTreatment mean: ~2.20%\n\nT-statistic: 3.10\n\np-value: 0.0019\n\nThis result indicates that the difference is statistically significant at the 1% level. In other words, people who received a matching donation offer were significantly more likely to donate.\n\nBivariate Linear Regression\nI also ran a linear regression model: gave ~ treatment:\nmodel = smf.ols('gave ~ treatment', data=df).fit()\nprint(model.summary())\n\nCoefficient on treatment: 0.00418\n\np-value: 0.0019\n\nThis suggests that assignment to treatment increases the donation probability by about 0.4 percentage points, which is a small but statistically meaningful effect, especially given the scale of the fundraising campaign. Together, the t-test and the regression confirm the same conclusion: the treatment group donated at a significantly higher rate than the control group.\n\n\n\nProbit Regression\nTo confirm the finding using a nonlinear model (as in the original paper), I also estimate a probit regression with the same dependent variable:\nprobit_model = smf.probit('gave ~ treatment', data=df).fit()\nprint(probit_model.summary())\n\nProbit coefficient on treatment: 0.087\np-value: 0.0019\n\nThis replicates the finding in Table 3, Column 1 of Karlan & List (2007), where the authors also find that the presence of a match significantly increases the probability of donation.\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\nt-test\nI conduct pairwise t-tests comparing donation rates across different match ratio groups within the treatment group:\ngave_1 = df[df['ratio'] == 1]['gave']\ngave_2 = df[df['ratio'] == 2]['gave']\ngave_3 = df[df['ratio'] == 3]['gave']\n\nprint(\"1:1 vs 2:1\", ttest_ind(gave_1, gave_2))\nprint(\"2:1 vs 3:1\", ttest_ind(gave_2, gave_3))\nprint(\"1:1 vs 3:1\", ttest_ind(gave_1, gave_3))\n\n$1:$1 vs $2:$1: t = -0.97, p = 0.335\n\n$2:$1 vs $3:$1: t = -0.05, p = 0.960\n\n$1:$1 vs $3:$1: t = -1.02, p = 0.310\n\nNone of these comparisons are statistically significant at the 5% level, which suggests that increasing the match ratio does not significantly increase the likelihood of giving—consistent with the authors’ conclusion on page 8 of the paper.\n\n\nRegression Analysis\nI also fit a linear regression model where the dependent variable is gave and the independent variables are ratio2 and ratio3, using $1:$1 match as the baseline:\nmatch_df = df[df['treatment'] == 1]\nmodel = smf.ols('gave ~ ratio2 + ratio3', data=match_df).fit()\nmodel.summary()\nRegression output summary:\n\nCoefficient on ratio2: 0.0019, p = 0.338\nCoefficient on ratio3: 0.0020, p = 0.313\n\nThe coefficients are small and statistically insignificant, confirming the same conclusion as the t-tests.\nDifference in Donation Rates\nWe also directly compute the difference in response rates:\nmatch_df = df[df['treatment'] == 1]\nmodel = smf.ols('gave ~ ratio2 + ratio3', data=match_df).fit()\ndiff_12 = gave_2.mean() - gave_1.mean()\ndiff_23 = gave_3.mean() - gave_2.mean()\ncoef_diff = model.params['ratio3'] - model.params['ratio2']\n\nFrom $1:$1 to $2:$1: +0.00188 (≈ 0.19 percentage points)\nFrom $2:$1 to $3:$1: +0.0001 (≈ 0.01 percentage points)\n\nFrom the regression coefficients:\n\nratio3 - ratio2 = +0.0001\n\nThese small and statistically insignificant changes indicate that donors do not respond more strongly to larger match ratios. Simply offering a match matters, but increasing the match ratio offers no additional benefit.\n\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nFull Sample Analysis (Including Non-Donors)\nFirst, I compare average donation amounts across all individuals, including those who gave $0. Using both a t-test and a linear regression:\nt_stat, p_val = ttest_ind(df[df['treatment'] == 1]['amount'], df[df['treatment'] == 0]['amount'])\nmodel = smf.ols('amount ~ treatment', data=df).fit()\n\nMean amount (control) = $0.813\n\nMean amount (treatment) = $0.967\n\nOLS coefficient on treatment = 0.1536\n\nt-statistic = 1.861\n\np-value = 0.063\n\nThe positive coefficient indicates that the treatment group gave slightly more on average. However, the p-value is just above the common 0.05 threshold, suggesting marginal significance. Most of the observed difference appears to be driven by the fact that more people gave in the treatment group, rather than those who gave giving significantly more.\n\n\nConditional on Donation\nI then limit the analysis to individuals who actually made a donation (gave == 1). I repeat the same steps:\ndonors_df = df[df['gave'] == 1]\nt2, p2 = ttest_ind(donors_df[donors_df['treatment'] == 1]['amount'],\n                   donors_df[donors_df['treatment'] == 0]['amount'])\nmodel2 = smf.ols('amount ~ treatment', data=donors_df).fit()\n\nMean amount (control) = $45.54\n\nMean amount (treatment) = $43.87\n\nOLS coefficient on treatment = -1.668\n\nt-statistic = -0.581\n\np-value = 0.561\n\nThese results suggest that conditional on donating, people in the treatment group did not give more, and actually gave slightly less on average (though not statistically significantly). The p-value of 0.561 indicates no meaningful difference.\nThis implies that the treatment’s impact was primarily at the extensive margin (increasing the number of people who gave), and not at the intensive margin (increasing donation amount conditional on giving). This result supports the original conclusion from Karlan & List (2007).\n\n\nHistograms of Donation Amounts\nBelow are two histograms showing the distribution of donation amounts among donors, separated by treatment group. The red dashed line represents the group mean.\n \n\n\n\n\n\n\nplot code\n\n\n\n\n\ntreatment_amount = donors_df[donors_df['treatment'] == 1]['amount']\ncontrol_amount = donors_df[donors_df['treatment'] == 0]['amount']\n\nplt.figure(figsize=(6,4))\nplt.hist(treatment_amount, bins=30, color='skyblue', edgecolor='black')\nplt.axvline(treatment_amount.mean(), color='red', linestyle='--', label=f'Mean = {treatment_amount.mean():.2f}')\nplt.title(\"Donation Amounts (Treatment Group)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"hist_treatment.png\")\nplt.show()\n\nplt.figure(figsize=(6,4))\nplt.hist(control_amount, bins=30, color='lightgreen', edgecolor='black')\nplt.axvline(control_amount.mean(), color='red', linestyle='--', label=f'Mean = {control_amount.mean():.2f}')\nplt.title(\"Donation Amounts (Control Group)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"hist_control.png\")\nplt.show()"
  },
  {
    "objectID": "blog/project1/hw1_questions.html#simulation-experiment",
    "href": "blog/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nTo demonstrate the Law of Large Numbers, I simulate two groups:\n\nA control group with a true probability of giving of p = 0.018\n\nA treatment group with a true probability of p = 0.022\n\nI draw 10,000 samples from each distribution and calculate the difference in donation outcome (1 or 0) for each pair. Then I compute the cumulative average of these 10,000 differences and plot the result below:\n\n\n\nLaw of Large Numbers Simulation\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\np_control = 0.018\np_treatment = 0.022\nn = 10000\nnp.random.seed(42)\n\ncontrol_sim = np.random.binomial(1, p_control, size=n)\ntreatment_sim = np.random.binomial(1, p_treatment, size=n)\ndiff_vector = treatment_sim - control_sim\ncumulative_avg = np.cumsum(diff_vector) / np.arange(1, n + 1)\n\nplt.figure(figsize=(8, 4))\nplt.plot(cumulative_avg, label='Cumulative Average Difference')\nplt.axhline(p_treatment - p_control, color='red', linestyle='--', label='True Difference (0.004)')\nplt.title('Law of Large Numbers Simulation')\nplt.xlabel('Sample Size')\nplt.ylabel('Cumulative Difference')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.savefig('law_of_large_numbers.png')\nplt.show()\n\n\n\n\n\nCentral Limit Theorem\n\nTo demonstrate the Central Limit Theorem (CLT), I simulate the difference in donation rates between the treatment (p = 0.022) and control (p = 0.018) groups at four different sample sizes: 50, 200, 500, and 1000. For each sample size, I repeat the experiment 1000 times, and in each trial I compute the average difference in donation rate:\n\nAt n = 50, the distribution is wide and irregular. The mean is slightly right of zero, but there’s a lot of noise. Zero is near the center.\nAt n = 200, the distribution starts to resemble a bell shape, but still has considerable spread.\nAt n = 500, the distribution becomes noticeably more symmetric, and the mean difference starts to stand out from zero.\nAt n = 1000, the distribution is tightly centered around 0.004, and zero is clearly in the left tail, indicating a consistent positive treatment effect.\n\n\n\n\nCLT Simulation\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\nnp.random.seed(42)\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\naxs = axs.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    differences = []\n    for _ in range(n_simulations):\n        control = np.random.binomial(1, p_control, n)\n        treatment = np.random.binomial(1, p_treatment, n)\n        diff = treatment.mean() - control.mean()\n        differences.append(diff)\n    \n    mean_diff = np.mean(differences)\n\n    axs[i].hist(differences, bins=30, color='lightblue', edgecolor='black')\n    axs[i].axvline(0, color='red', linestyle='--', label='Zero')\n    axs[i].axvline(mean_diff, color='green', linestyle='-', label=f'Mean = {mean_diff:.4f}')\n    axs[i].set_title(f'Sample Size = {n}')\n    axs[i].set_xlabel('Difference in Donation Rate')\n    axs[i].set_ylabel('Frequency')\n    axs[i].legend()\n\nplt.suptitle('CLT Simulation: Sampling Distribution of Differences', fontsize=14)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.savefig('clt_histograms_labeled.png')\nplt.show()\n\n\n\nI conclude that as sample size increases, the distribution of the average difference between treatment and control becomes more normal and more centered around the true mean—exactly as predicted by the Central Limit Theorem."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#conclusion",
    "href": "blog/project1/hw1_questions.html#conclusion",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conclusion",
    "text": "Conclusion\nThis project gave me a chance to explore how matching donations affect charitable giving by replicating the results from Karlan & List (2007). After digging into the data, I found that people who received a matching offer were definitely more likely to donate—just like the original paper said. Even though the increase was small in percentage terms, it’s meaningful when you’re dealing with tens of thousands of people.\nOne thing that stood out to me was that higher match ratios (like 2:1 or 3:1) didn’t really help any more than the basic 1:1 offer. So the key seems to be just having a match at all—not how big the match is.\nWhen I looked at how much people gave, it turned out that the treatment group gave slightly more overall, but not because they gave more money individually. Instead, the bump came from more people deciding to give, not from people giving larger amounts.\nOverall, this was a great example of how subtle changes in message framing (like offering a match) can change real-world behavior. And it showed me how field experiments and behavioral economics can go hand-in-hand."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Junye's Website",
    "section": "",
    "text": "Hi, my name is Junye Fan. With a bachelor degree of Economics, I am currently pursuing my Master’s degree in Business Analytics at the University of California, San Diego."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Junye's Website",
    "section": "",
    "text": "Hi, my name is Junye Fan. With a bachelor degree of Economics, I am currently pursuing my Master’s degree in Business Analytics at the University of California, San Diego."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Junye's Website",
    "section": "🎓 Education",
    "text": "🎓 Education\n\nUniversity of California, San Diego\nMaster of Science in Business Analytics\nShanghai Normal University\nBachelor of Economics in Economics"
  },
  {
    "objectID": "index.html#my-story",
    "href": "index.html#my-story",
    "title": "Junye's Website",
    "section": "🎥 My Story",
    "text": "🎥 My Story\nMy favorite part of my academic and professional journey has been combining my business knowledge with data analytics to solve real-world problems. During my undergraduate studies in economics, I developed a strong foundation in data analysis techniques, learning key skills in SQL, business statistics and econometrics. However, I realized that hands-on experience was essential to applying these skills effectively, so I tried opportunities to intern with top organizations.\nIn my recent internship at Hellobike, one of China’s largest mobility platforms, I utilized SQL and Tableau to create dashboards that made complex data accessible and actionable for stakeholders. By conducting A/B testing and statistical analysis, I optimized user interface design, resulting in an 8.68% increase in click-through rates.\nThis experience underscored the importance of understanding user needs and translating data insights into clear, data-driven recommendations. Collaborating closely with cross-functional teams, I honed my ability to communicate complex data insights in an easily understandable way, further aligning our strategies with business goals.\nNow, as a Master’s student in Business Analytics, I am deepening my technical expertise and learning to leverage big data tools like Python, SQL, and cloud-hosted databases. I am excited about the opportunity to bring my skills in data analytics and my experience in client-focused solutions to industries and empower consumers and businesses to make informed decisions and thrive."
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Junye's Website",
    "section": "🛠 Skills",
    "text": "🛠 Skills\n\nQuery Languages: SQL\n\nAnalytics & Modeling: Python, R, Alteryx, MATLAB, STATA\n\nFront-End: JavaScript, HTML\n\nData Visualization: Tableau\n\nOthers: A/B Testing, Industry Research"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Junye's Website",
    "section": "📬 Contact",
    "text": "📬 Contact\n📧 E-mail 🔗 LinkedIn"
  },
  {
    "objectID": "blog/project2/hw2_questions.html",
    "href": "blog/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nThis dataset contains information on 1,500 mature engineering firms and includes variables on patenting activity, geographic location, firm age, and Blueprinty software usage. The primary goal is to assess whether firms using Blueprinty’s software are more successful in obtaining patents.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\npatents\nNumber of patents awarded to the firm over the past 5 years\n\n\nregion\nRegion where the firm is located (e.g., Northeast, Midwest, etc.)\n\n\nage\nNumber of years since the firm’s incorporation\n\n\niscustomer\nBlueprinty customer indicator (1 = firm uses Blueprinty, 0 = not)\n\n\n\n\n\n\n\nBased on the bar chart comparing the average number of patents, we observe that Blueprinty customers have a higher average number of patents awarded over the past five years compared to non-customers. Specifically, customers average 4.13 patents, while non-customers average closer to 3.5 (3.47). This suggests that firms using Blueprinty’s software may be more successful in obtaining patents.\nThis conclusion is further supported by the histogram comparing the full distribution of patent counts across customer groups. The distribution for Blueprinty customers is visibly shifted to the right, indicating a greater concentration of firms with 4 or more patents. In contrast, non-customers are more heavily represented in the 0 to 2 patent range, with relatively fewer firms reaching the higher patent counts observed among customers. The histogram also shows a longer right tail for customers, with more firms reaching double-digit patent counts.\nTaken together, both the difference in means and the shape of the distribution suggest that Blueprinty customers, on average, have higher patenting activity. However, it is important to note that this is a descriptive comparison. These differences may be influenced by other factors, such as firm age or geographic region, which are not yet accounted for in this analysis.\n\n\n\nPatent Count Distribution by Customer Status\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"blueprinty.csv\")\nsns.set_style(\"white\")\n\nplt.figure(figsize=(10, 6))\nax = sns.histplot(\n    data=df,\n    x='patents',\n    hue='iscustomer',\n    bins=30,\n    palette='Set2',\n    multiple='dodge'\n)\n\nfor bar in ax.patches:\n    bar.set_edgecolor(\"black\")\n    bar.set_linewidth(1)\n\nplt.title(\"Patent Count Distribution by Customer Status\")\nplt.xlabel(\"Number of Patents (past 5 years)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\nplt.tight_layout()\nplt.show()\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nWhen comparing company age by customer status, we observe that Blueprinty customers are slightly older than non-customers. On average, customer firms have been incorporated for about 26.9 years, while non-customers average 26.1 years. Although the difference is small, it may suggest that Blueprinty customers are marginally more established or experienced.\n\n\n\nCompany Age Distribution by Customer Status\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nage_means = df.groupby(\"iscustomer\")[\"age\"].mean().rename(index={0: \"Non-Customer\", 1: \"Customer\"})\nprint(\"Mean Company Age:\\n\", age_means)\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x='age', hue='iscustomer', bins=30, palette='Set2', multiple='dodge')\nplt.title(\"Company Age Distribution by Customer Status\")\nplt.xlabel(\"Company Age (Years)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\nplt.tight_layout()\nplt.show()\n\n\n\nRegional differences, however, are much more pronounced. In the Northeast, more than 54% of firms are Blueprinty customers, making it the only region where customers outnumber non-customers. In all other regions—such as the Midwest, South, Southwest, and Northwest—Blueprinty customers represent less than 20% of firms. This shows that Blueprinty has a particularly strong presence in the Northeast, while adoption is much lower in other parts of the country.\n\n\n\nDistribution of Firms by Region and Customer Status\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nsns.histplot(\n    data=df,\n    x='region',\n    hue='iscustomer',\n    multiple='dodge',\n    shrink=0.8,\n    palette='Set2',\n    stat='count',\n    edgecolor='black'\n)\n\nplt.title(\"Distribution of Firms by Region and Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\n\nplt.grid(False)\n\nplt.tight_layout()\nplt.show()\n\n\n\nThese observations highlight important systematic differences between customers and non-customers. Since customer status is not randomly assigned, it’s essential to account for age and regional factors when evaluating the effect of Blueprinty software on patent outcomes.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nGiven that \\(Y \\sim \\text{Poisson}(\\lambda)\\), the likelihood function is:\n\\[\nL(\\lambda \\mid Y) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking logs, the log-likelihood function becomes:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln  # log(y!) for numerical stability\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf  # log likelihood is undefined for λ &lt;= 0\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='darkblue')\nplt.title(\"Log-Likelihood of Poisson Model\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.axvline(np.mean(Y), color='red', linestyle='--', label='Mean of Y')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\nLog-Likelihood of Poisson Model\n\n\n\nLet’s consider the log-likelihood of a Poisson model where \\(Y_i \\sim \\text{Poisson}(\\lambda)\\):\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n\\]\nTaking the first derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell(\\lambda)}{d\\lambda} = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSetting the derivative equal to zero to find the maximum likelihood estimate (MLE):\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\]\nSolve for \\(\\lambda\\):\n\\[\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThus, the MLE for \\(\\lambda\\) is simply the sample mean of \\(Y\\), which intuitively makes sense since the Poisson distribution has its mean equal to \\(\\lambda\\).\n\nWe used numerical optimization to estimate the maximum likelihood value of λ in the Poisson model. The result, λ̂ = 3.6847, matches exactly with the sample mean of the observed data. This confirms our analytical result that the MLE of λ is simply the average of Y in a Poisson setting.\n\n\n\n\n\n\ncode\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\nneg_loglik = lambda lmbda: -poisson_loglikelihood(lmbda, Y)\n\nresult = minimize_scalar(neg_loglik, bounds=(0.01, 20), method='bounded')\n\nlambda_mle = result.x\nprint(f\"MLE of λ (via optimization): {lambda_mle:.4f}\")\nprint(f\"Sample mean of Y (baseline): {np.mean(Y):.4f}\")\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\text{where} \\quad \\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nThis ensures that \\(\\lambda_i &gt; 0\\) for all \\(i\\). The log-likelihood function for this model is:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\left( -\\lambda_i + Y_i \\log(\\lambda_i) - \\log(Y_i!) \\right)\n= \\sum_{i=1}^n \\left( -\\exp(X_i^\\top \\beta) + Y_i X_i^\\top \\beta - \\log(Y_i!) \\right)\n\\]\nWe now implement this in Python.\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\n\n# Feature engineering\ndf[\"age_scaled\"] = df[\"age\"] / 10\ndf[\"age_sq_scaled\"] = (df[\"age\"] ** 2) / 100\n\n# Construct design matrix X\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age_scaled\", \"age_sq_scaled\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\nX_matrix = X.values\nY = df[\"patents\"].values\n\n\nI estimated a Poisson regression model where the number of patents is modeled as a function of firm age (scaled), age squared, customer status, and regional dummy variables. The fitted coefficients and their standard errors are shown in the table below:\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\nIntercept\n-0.509\n0.183\n\n\nAge (scaled)\n1.486\n0.139\n\n\nAge² (scaled)\n-0.297\n0.026\n\n\nIsCustomer\n0.208\n0.031\n\n\nNortheast\n0.029\n0.044\n\n\nNorthwest\n-0.018\n0.054\n\n\nSouth\n0.057\n0.053\n\n\nSouthwest\n0.051\n0.047\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n# Feature scaling\ndf[\"age_scaled\"] = df[\"age\"] / 10\ndf[\"age_sq_scaled\"] = (df[\"age\"] ** 2) / 100\n\n# Construct design matrix\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX = pd.concat([\n    pd.Series(1.0, index=df.index, name=\"intercept\"),\n    df[[\"age_scaled\", \"age_sq_scaled\", \"iscustomer\"]],\n    region_dummies\n], axis=1).astype(float)\nX_matrix = X.values\nY = df[\"patents\"].values\n\n# Define Poisson log-likelihood\ndef poisson_loglikelihood_beta(beta, Y, X):\n    XB = X @ beta\n    lambdas = np.exp(XB)\n    if np.any(np.isnan(lambdas)) or np.any(np.isinf(lambdas)):\n        return np.inf\n    loglik = np.sum(-lambdas + Y * XB - gammaln(Y + 1))\n    return -loglik  # minimize negative log-likelihood\n\n# Optimize\nbeta_start = np.zeros(X.shape[1])\nres = minimize(poisson_loglikelihood_beta, beta_start, args=(Y, X_matrix), method=\"BFGS\")\n\n# Extract coefficient estimates and standard errors\nbeta_hat = res.x\nhessian_inv = res.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Create result table\nresults = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X.columns)\n\nprint(results)\n\n\n\n\n\n\nTo validate our MLE results, I also fit the same Poisson regression model using Python’s statsmodels.api.GLM() function. The results closely match the custom optimization estimates, confirming the consistency and correctness of the likelihood-based approach.\nThe iscustomer coefficient is positive and statistically significant. Since the model uses a log link, we interpret the coefficient of 0.208 as follows:\n\nFirms using Blueprinty are expected to have approximately 23% more patents, all else equal, since \\(e^{0.208} \\approx 1.231\\).\n\nThis suggests that using Blueprinty’s software is associated with increased patent success.\nIn addition, the model suggests that company age has a positive effect on patenting up to a point (since the coefficient for age_scaled is positive), but the negative coefficient on age_sq_scaled indicates diminishing returns as firms get older. Regional effects appear small and are not statistically significant.\nOverall, the model supports the hypothesis that using Blueprinty’s software is associated with higher patenting activity, even after controlling for age and region.\n\n\n\nTo better interpret the practical effect of Blueprinty’s software on patenting outcomes, I simulate a counterfactual scenario. We use our estimated Poisson regression model to compare predicted outcomes for each firm under two conditions:\n\nScenario 1 (X_0): All firms are treated as non-customers (iscustomer = 0)\nScenario 2 (X_1): All firms are treated as Blueprinty customers (iscustomer = 1)\n\nI then compute the expected number of patents under each condition and take the average difference.\n\n\n\n\n\n\ncode\n\n\n\n\n\n# Create counterfactual design matrices\nX_0 = X.copy()\nX_1 = X.copy()\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected number of patents under each scenario\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\n# Calculate average effect\neffect_vector = y_pred_1 - y_pred_0\naverage_effect = np.mean(effect_vector)\nprint(f\"Average predicted increase in patents due to Blueprinty: {average_effect:.4f}\")\n\n\n\n\nBased on our counterfactual simulation using the estimated Poisson regression model, firms that use Blueprinty are predicted to receive 0.79 more patents, on average, over a five-year period compared to if they did not use the software.\n\n\n\nPredicted Patents under Counterfactual Scenarios\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\navg_y_0 = np.mean(y_pred_0)\navg_y_1 = np.mean(y_pred_1)\n\n\ncomparison_df = pd.DataFrame({\n    \"Scenario\": [\"All Non-Customers\", \"All Customers\"],\n    \"Average Predicted Patents\": [avg_y_0, avg_y_1]\n})\n\n\ncolors = sns.color_palette(\"Set2\")\n\nplt.figure(figsize=(6, 5))\nbars = plt.bar(\n    comparison_df[\"Scenario\"],\n    comparison_df[\"Average Predicted Patents\"],\n    color=[colors[0], colors[1]],\n    edgecolor=\"black\"\n)\n\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(\n        bar.get_x() + bar.get_width() / 2,\n        height + 0.05,\n        f\"{height:.2f}\",\n        ha='center',\n        va='bottom',\n        fontsize=11\n    )\n\nplt.title(\"Predicted Average Number of Patents\\nUnder Counterfactual Scenarios\")\nplt.ylabel(\"Average Predicted Patents\")\nplt.ylim(0, max(avg_y_0, avg_y_1) + 1)\nplt.tight_layout()\nplt.show()\n\n\n\nThis effect holds after controlling for firm age and regional differences, suggesting that Blueprinty’s software is associated with a meaningful increase in patenting success."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nThis dataset contains information on 1,500 mature engineering firms and includes variables on patenting activity, geographic location, firm age, and Blueprinty software usage. The primary goal is to assess whether firms using Blueprinty’s software are more successful in obtaining patents.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\npatents\nNumber of patents awarded to the firm over the past 5 years\n\n\nregion\nRegion where the firm is located (e.g., Northeast, Midwest, etc.)\n\n\nage\nNumber of years since the firm’s incorporation\n\n\niscustomer\nBlueprinty customer indicator (1 = firm uses Blueprinty, 0 = not)\n\n\n\n\n\n\n\nBased on the bar chart comparing the average number of patents, we observe that Blueprinty customers have a higher average number of patents awarded over the past five years compared to non-customers. Specifically, customers average 4.13 patents, while non-customers average closer to 3.5 (3.47). This suggests that firms using Blueprinty’s software may be more successful in obtaining patents.\nThis conclusion is further supported by the histogram comparing the full distribution of patent counts across customer groups. The distribution for Blueprinty customers is visibly shifted to the right, indicating a greater concentration of firms with 4 or more patents. In contrast, non-customers are more heavily represented in the 0 to 2 patent range, with relatively fewer firms reaching the higher patent counts observed among customers. The histogram also shows a longer right tail for customers, with more firms reaching double-digit patent counts.\nTaken together, both the difference in means and the shape of the distribution suggest that Blueprinty customers, on average, have higher patenting activity. However, it is important to note that this is a descriptive comparison. These differences may be influenced by other factors, such as firm age or geographic region, which are not yet accounted for in this analysis.\n\n\n\nPatent Count Distribution by Customer Status\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"blueprinty.csv\")\nsns.set_style(\"white\")\n\nplt.figure(figsize=(10, 6))\nax = sns.histplot(\n    data=df,\n    x='patents',\n    hue='iscustomer',\n    bins=30,\n    palette='Set2',\n    multiple='dodge'\n)\n\nfor bar in ax.patches:\n    bar.set_edgecolor(\"black\")\n    bar.set_linewidth(1)\n\nplt.title(\"Patent Count Distribution by Customer Status\")\nplt.xlabel(\"Number of Patents (past 5 years)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\nplt.tight_layout()\nplt.show()\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nWhen comparing company age by customer status, we observe that Blueprinty customers are slightly older than non-customers. On average, customer firms have been incorporated for about 26.9 years, while non-customers average 26.1 years. Although the difference is small, it may suggest that Blueprinty customers are marginally more established or experienced.\n\n\n\nCompany Age Distribution by Customer Status\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nage_means = df.groupby(\"iscustomer\")[\"age\"].mean().rename(index={0: \"Non-Customer\", 1: \"Customer\"})\nprint(\"Mean Company Age:\\n\", age_means)\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x='age', hue='iscustomer', bins=30, palette='Set2', multiple='dodge')\nplt.title(\"Company Age Distribution by Customer Status\")\nplt.xlabel(\"Company Age (Years)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\nplt.tight_layout()\nplt.show()\n\n\n\nRegional differences, however, are much more pronounced. In the Northeast, more than 54% of firms are Blueprinty customers, making it the only region where customers outnumber non-customers. In all other regions—such as the Midwest, South, Southwest, and Northwest—Blueprinty customers represent less than 20% of firms. This shows that Blueprinty has a particularly strong presence in the Northeast, while adoption is much lower in other parts of the country.\n\n\n\nDistribution of Firms by Region and Customer Status\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nsns.histplot(\n    data=df,\n    x='region',\n    hue='iscustomer',\n    multiple='dodge',\n    shrink=0.8,\n    palette='Set2',\n    stat='count',\n    edgecolor='black'\n)\n\nplt.title(\"Distribution of Firms by Region and Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\n\nplt.grid(False)\n\nplt.tight_layout()\nplt.show()\n\n\n\nThese observations highlight important systematic differences between customers and non-customers. Since customer status is not randomly assigned, it’s essential to account for age and regional factors when evaluating the effect of Blueprinty software on patent outcomes.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nGiven that \\(Y \\sim \\text{Poisson}(\\lambda)\\), the likelihood function is:\n\\[\nL(\\lambda \\mid Y) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking logs, the log-likelihood function becomes:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln  # log(y!) for numerical stability\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf  # log likelihood is undefined for λ &lt;= 0\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='darkblue')\nplt.title(\"Log-Likelihood of Poisson Model\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.axvline(np.mean(Y), color='red', linestyle='--', label='Mean of Y')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\nLog-Likelihood of Poisson Model\n\n\n\nLet’s consider the log-likelihood of a Poisson model where \\(Y_i \\sim \\text{Poisson}(\\lambda)\\):\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n\\]\nTaking the first derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell(\\lambda)}{d\\lambda} = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSetting the derivative equal to zero to find the maximum likelihood estimate (MLE):\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\]\nSolve for \\(\\lambda\\):\n\\[\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThus, the MLE for \\(\\lambda\\) is simply the sample mean of \\(Y\\), which intuitively makes sense since the Poisson distribution has its mean equal to \\(\\lambda\\).\n\nWe used numerical optimization to estimate the maximum likelihood value of λ in the Poisson model. The result, λ̂ = 3.6847, matches exactly with the sample mean of the observed data. This confirms our analytical result that the MLE of λ is simply the average of Y in a Poisson setting.\n\n\n\n\n\n\ncode\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\nneg_loglik = lambda lmbda: -poisson_loglikelihood(lmbda, Y)\n\nresult = minimize_scalar(neg_loglik, bounds=(0.01, 20), method='bounded')\n\nlambda_mle = result.x\nprint(f\"MLE of λ (via optimization): {lambda_mle:.4f}\")\nprint(f\"Sample mean of Y (baseline): {np.mean(Y):.4f}\")\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\text{where} \\quad \\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nThis ensures that \\(\\lambda_i &gt; 0\\) for all \\(i\\). The log-likelihood function for this model is:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\left( -\\lambda_i + Y_i \\log(\\lambda_i) - \\log(Y_i!) \\right)\n= \\sum_{i=1}^n \\left( -\\exp(X_i^\\top \\beta) + Y_i X_i^\\top \\beta - \\log(Y_i!) \\right)\n\\]\nWe now implement this in Python.\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\n\n# Feature engineering\ndf[\"age_scaled\"] = df[\"age\"] / 10\ndf[\"age_sq_scaled\"] = (df[\"age\"] ** 2) / 100\n\n# Construct design matrix X\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age_scaled\", \"age_sq_scaled\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\nX_matrix = X.values\nY = df[\"patents\"].values\n\n\nI estimated a Poisson regression model where the number of patents is modeled as a function of firm age (scaled), age squared, customer status, and regional dummy variables. The fitted coefficients and their standard errors are shown in the table below:\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\nIntercept\n-0.509\n0.183\n\n\nAge (scaled)\n1.486\n0.139\n\n\nAge² (scaled)\n-0.297\n0.026\n\n\nIsCustomer\n0.208\n0.031\n\n\nNortheast\n0.029\n0.044\n\n\nNorthwest\n-0.018\n0.054\n\n\nSouth\n0.057\n0.053\n\n\nSouthwest\n0.051\n0.047\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n# Feature scaling\ndf[\"age_scaled\"] = df[\"age\"] / 10\ndf[\"age_sq_scaled\"] = (df[\"age\"] ** 2) / 100\n\n# Construct design matrix\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX = pd.concat([\n    pd.Series(1.0, index=df.index, name=\"intercept\"),\n    df[[\"age_scaled\", \"age_sq_scaled\", \"iscustomer\"]],\n    region_dummies\n], axis=1).astype(float)\nX_matrix = X.values\nY = df[\"patents\"].values\n\n# Define Poisson log-likelihood\ndef poisson_loglikelihood_beta(beta, Y, X):\n    XB = X @ beta\n    lambdas = np.exp(XB)\n    if np.any(np.isnan(lambdas)) or np.any(np.isinf(lambdas)):\n        return np.inf\n    loglik = np.sum(-lambdas + Y * XB - gammaln(Y + 1))\n    return -loglik  # minimize negative log-likelihood\n\n# Optimize\nbeta_start = np.zeros(X.shape[1])\nres = minimize(poisson_loglikelihood_beta, beta_start, args=(Y, X_matrix), method=\"BFGS\")\n\n# Extract coefficient estimates and standard errors\nbeta_hat = res.x\nhessian_inv = res.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Create result table\nresults = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X.columns)\n\nprint(results)\n\n\n\n\n\n\nTo validate our MLE results, I also fit the same Poisson regression model using Python’s statsmodels.api.GLM() function. The results closely match the custom optimization estimates, confirming the consistency and correctness of the likelihood-based approach.\nThe iscustomer coefficient is positive and statistically significant. Since the model uses a log link, we interpret the coefficient of 0.208 as follows:\n\nFirms using Blueprinty are expected to have approximately 23% more patents, all else equal, since \\(e^{0.208} \\approx 1.231\\).\n\nThis suggests that using Blueprinty’s software is associated with increased patent success.\nIn addition, the model suggests that company age has a positive effect on patenting up to a point (since the coefficient for age_scaled is positive), but the negative coefficient on age_sq_scaled indicates diminishing returns as firms get older. Regional effects appear small and are not statistically significant.\nOverall, the model supports the hypothesis that using Blueprinty’s software is associated with higher patenting activity, even after controlling for age and region.\n\n\n\nTo better interpret the practical effect of Blueprinty’s software on patenting outcomes, I simulate a counterfactual scenario. We use our estimated Poisson regression model to compare predicted outcomes for each firm under two conditions:\n\nScenario 1 (X_0): All firms are treated as non-customers (iscustomer = 0)\nScenario 2 (X_1): All firms are treated as Blueprinty customers (iscustomer = 1)\n\nI then compute the expected number of patents under each condition and take the average difference.\n\n\n\n\n\n\ncode\n\n\n\n\n\n# Create counterfactual design matrices\nX_0 = X.copy()\nX_1 = X.copy()\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected number of patents under each scenario\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\n# Calculate average effect\neffect_vector = y_pred_1 - y_pred_0\naverage_effect = np.mean(effect_vector)\nprint(f\"Average predicted increase in patents due to Blueprinty: {average_effect:.4f}\")\n\n\n\n\nBased on our counterfactual simulation using the estimated Poisson regression model, firms that use Blueprinty are predicted to receive 0.79 more patents, on average, over a five-year period compared to if they did not use the software.\n\n\n\nPredicted Patents under Counterfactual Scenarios\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\navg_y_0 = np.mean(y_pred_0)\navg_y_1 = np.mean(y_pred_1)\n\n\ncomparison_df = pd.DataFrame({\n    \"Scenario\": [\"All Non-Customers\", \"All Customers\"],\n    \"Average Predicted Patents\": [avg_y_0, avg_y_1]\n})\n\n\ncolors = sns.color_palette(\"Set2\")\n\nplt.figure(figsize=(6, 5))\nbars = plt.bar(\n    comparison_df[\"Scenario\"],\n    comparison_df[\"Average Predicted Patents\"],\n    color=[colors[0], colors[1]],\n    edgecolor=\"black\"\n)\n\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(\n        bar.get_x() + bar.get_width() / 2,\n        height + 0.05,\n        f\"{height:.2f}\",\n        ha='center',\n        va='bottom',\n        fontsize=11\n    )\n\nplt.title(\"Predicted Average Number of Patents\\nUnder Counterfactual Scenarios\")\nplt.ylabel(\"Average Predicted Patents\")\nplt.ylim(0, max(avg_y_0, avg_y_1) + 1)\nplt.tight_layout()\nplt.show()\n\n\n\nThis effect holds after controlling for firm age and regional differences, suggesting that Blueprinty’s software is associated with a meaningful increase in patenting success."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#airbnb-case-study",
    "href": "blog/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\n\nExploratory Data Analysis\n\nThe distribution of the number of reviews is highly right-skewed, with most listings receiving fewer than 20 reviews.\nListings that are instant bookable appear to have more reviews, based on a comparison of medians in the boxplot.\n\n\n\n\nHistogram of Reviews\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nvars_used = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\ndf = df[vars_used]\n\n# Drop rows with missing values\ndf_clean = df.dropna()\n\n# Convert variables\ndf_clean[\"instant_bookable\"] = (df_clean[\"instant_bookable\"] == \"t\").astype(int)\ndf_dummies = pd.get_dummies(df_clean, columns=[\"room_type\"], drop_first=True)\ndf_dummies.columns = df_dummies.columns.str.replace(\" \", \"_\")\n\n# Poisson regression\nformula = (\n    \"number_of_reviews ~ bathrooms + bedrooms + price + \"\n    \"review_scores_cleanliness + review_scores_location + \"\n    \"review_scores_value + instant_bookable + \"\n    \"room_type_Private_room + room_type_Shared_room\"\n)\nmodel = glm(formula=formula, data=df_dummies, family=sm.families.Poisson()).fit()\nresults_df = pd.DataFrame({\n    \"Coefficient\": model.params,\n    \"Std. Error\": model.bse\n})\nprint(results_df)\n\nreviews = df_clean[\"number_of_reviews\"]\n\nplt.figure(figsize=(8, 5))\nsns.histplot(reviews, bins=50, kde=True)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Frequency\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nBoxplot by Instant Bookable\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(x=\"instant_bookable\", y=\"number_of_reviews\", data=df_clean)\nplt.title(\"Number of Reviews by Instant Bookable\")\nplt.xlabel(\"Instant Bookable (0 = No, 1 = Yes)\")\nplt.ylabel(\"Number of Reviews\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nData Cleaning\n\nDropped listings with missing values in any relevant modeling variable.\nConverted instant_bookable to a binary variable (1 for ‘t’, 0 for ‘f’).\nEncoded room_type as dummy variables, using “Entire home/apt” as the reference category.\n\n\n\nPoisson Regression Model\nI used the following covariates to explain variation in the number of reviews: - bathrooms, bedrooms, price - review_scores_cleanliness, review_scores_location, review_scores_value - instant_bookable (binary) - room_type_Private_room, room_type_Shared_room (dummies)\n\n\n\n\n\n\ncode\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import glm\n\ndf = pd.read_csv(\"airbnb.csv\")\nvars_used = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\ndf = df[vars_used]\n\n# Drop rows with missing values\ndf_clean = df.dropna()\n\n# Convert variables\ndf_clean[\"instant_bookable\"] = (df_clean[\"instant_bookable\"] == \"t\").astype(int)\ndf_dummies = pd.get_dummies(df_clean, columns=[\"room_type\"], drop_first=True)\ndf_dummies.columns = df_dummies.columns.str.replace(\" \", \"_\")\n\n# Poisson regression\nformula = (\n    \"number_of_reviews ~ bathrooms + bedrooms + price + \"\n    \"review_scores_cleanliness + review_scores_location + \"\n    \"review_scores_value + instant_bookable + \"\n    \"room_type_Private_room + room_type_Shared_room\"\n)\nmodel = glm(formula=formula, data=df_dummies, family=sm.families.Poisson()).fit()\nresults_df = pd.DataFrame({\n    \"Coefficient\": model.params,\n    \"Std. Error\": model.bse\n})\nprint(results_df)\n\n\n\n\n\nVariable Interpretations\nThe estimated model is:\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nInterpretation\n\n\n\n\nIntercept\n3.572\nThe baseline log expected number of reviews for a reference listing\n\n\nbathrooms\n-0.124\nEach additional bathroom is associated with a ~11.6% decrease in expected reviews: \\(e^{-0.124} \\approx 0.883\\)\n\n\nbedrooms\n+0.075\nEach additional bedroom is associated with ~7.8% increase in reviews: \\(e^{0.075} \\approx 1.078\\)\n\n\nprice\n-0.000014\nThe effect is small and negative; more expensive listings receive slightly fewer reviews\n\n\nreview_scores_cleanliness\n+0.113\nA one-point increase in cleanliness score leads to ~12% more reviews: \\(e^{0.113} \\approx 1.12\\)\n\n\nreview_scores_location\n-0.077\nA higher location score is associated with slightly fewer reviews; possible saturation in popular areas\n\n\nreview_scores_value\n-0.092\nHigher value scores correspond to fewer reviews; possibly reflects different guest expectations\n\n\ninstant_bookable\n+0.334\nInstant booking listings receive ~40% more reviews: \\(e^{0.334} \\approx 1.40\\)\n\n\nroom_type_Private_room\n-0.015\nLittle difference from entire homes; slightly fewer reviews\n\n\nroom_type_Shared_room\n-0.252\nShared rooms get ~22% fewer reviews: \\(e^{-0.252} \\approx 0.78\\)\n\n\n\n\n\nSummary\n\nThe strongest positive driver of review volume is instant booking, which increases expected review counts by roughly 40%.\nCleanliness and number of bedrooms are also positively associated with reviews.\nShared rooms significantly underperform compared to entire homes.\nPrice, location, and value scores show weak or negative associations, possibly due to nonlinear effects or omitted variables.\n\n\nBecause the model uses a log link, the effect of each variable is multiplicative. A coefficient \\(\\beta\\) implies that the number of reviews changes by a factor of \\(e^\\beta\\) for a one-unit increase in that variable, holding all others constant."
  },
  {
    "objectID": "HW1/project1/hw1_questions.html",
    "href": "HW1/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was designed to test how different types of matching grants—offers to match the donor’s contribution at different rates—would influence both the likelihood of giving and the amount donated. The treatments varied in three main dimensions:\n\nMatch ratio: The donor’s contribution was matched by a leadership donor at a ratio of $1:$1, $2:$1, or $3:$1.\nMaximum match amount: The cap on the matching gift was randomly set at $25,000, $50,000, $100,000, or left unstated.\nSuggested donation amount: Based on the recipient’s highest previous contribution (HPC), the letters included one of three suggestions: HPC × 1.00, HPC × 1.25, or HPC × 1.50.\n\nThese variations were fully randomized, making this a natural field experiment that allows for strong causal inference.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "HW1/project1/hw1_questions.html#introduction",
    "href": "HW1/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was designed to test how different types of matching grants—offers to match the donor’s contribution at different rates—would influence both the likelihood of giving and the amount donated. The treatments varied in three main dimensions:\n\nMatch ratio: The donor’s contribution was matched by a leadership donor at a ratio of $1:$1, $2:$1, or $3:$1.\nMaximum match amount: The cap on the matching gift was randomly set at $25,000, $50,000, $100,000, or left unstated.\nSuggested donation amount: Based on the recipient’s highest previous contribution (HPC), the letters included one of three suggestions: HPC × 1.00, HPC × 1.25, or HPC × 1.50.\n\nThese variations were fully randomized, making this a natural field experiment that allows for strong causal inference.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "HW1/project1/hw1_questions.html#data",
    "href": "HW1/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset provided by Karlan and List (2007) is in .dta (Stata) format and includes over 50,000 observations, one for each individual who received a fundraising letter. Each row represents a donor and contains information about the treatment they were assigned to, their prior donation history, demographic characteristics, and whether they donated after receiving the letter.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nTo assess whether the randomization created comparable groups, I performed balance checks on several background variables. These include donation history (mrm2, freq, years), demographics (female, couple), and income indicators (ltmedmra, median_hhincome).\nFor each variable, I compared:\n\nthe mean in the treatment vs control group,\na t-test of the difference in means,\nand a regression coefficient from variable ~ treatment.\n\n\n\n\n\n\n\nbasic code\n\n\n\n\n\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n\n\nvars_to_test = ['mrm2', 'freq', 'years', 'female', 'ltmedmra', 'median_hhincome', 'couple']\n\nresults = []\n\nfor var in vars_to_test:\n    treat = df[df['treatment'] == 1][var]\n    control = df[df['treatment'] == 0][var]\n    t_stat, p_val = ttest_ind(treat, control, nan_policy='omit')\n    \n    model = smf.ols(f'{var} ~ treatment', data=df).fit()\n    coef = model.params['treatment']\n    reg_p = model.pvalues['treatment']\n    \n    results.append({\n        \"Variable\": var,\n        \"Treatment Mean\": round(treat.mean(), 3),\n        \"Control Mean\": round(control.mean(), 3),\n        \"T-Statistic\": round(t_stat, 3),\n        \"P-Value\": round(p_val, 3),\n        \"OLS Coefficient\": round(coef, 3),\n        \"OLS P-Value\": round(reg_p, 3)\n    })\n\nbalance_summary = pd.DataFrame(results)\n\nbalance_summary.loc[balance_summary['Variable'] == 'female', ['Treatment Mean', 'Control Mean']] *= 100\nbalance_summary.loc[balance_summary['Variable'] == 'couple', ['Treatment Mean', 'Control Mean']] *= 100\nbalance_summary.loc[balance_summary['Variable'] == 'ltmedmra', ['Treatment Mean', 'Control Mean']] *= 100\n\nbalance_summary.loc[balance_summary['Variable'] == 'median_hhincome', ['Treatment Mean', 'Control Mean']] = \\\n    balance_summary.loc[balance_summary['Variable'] == 'median_hhincome', ['Treatment Mean', 'Control Mean']].applymap(lambda x: f\"${x:,.0f}\")\n\nbalance_summary\nThe table below summarizes these tests:\n\n\nBalance Test Summaries\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nTreatment Mean\nControl Mean\nT-Statistic\nP-Value\nOLS Coefficient\nOLS P-Value\n\n\n\n\nmrm2\n13.01\n12.998\n0.119\n0.905\n0.014\n0.905\n\n\nfreq\n8.04\n8.05\n-0.111\n0.912\n-0.012\n0.912\n\n\nyears\n6.08\n6.14\n-1.103\n0.270\n-0.058\n0.270\n\n\nfemale\n27.5%\n28.3%\n-1.758\n0.079\n-0.008\n0.079\n\n\nltmedmra\n49.7%\n48.8%\n1.910\n0.056\n0.009\n0.056\n\n\nmedian_hhincome\n$54,763\n$54,921\n-0.742\n0.458\n-157.93\n0.458\n\n\ncouple\n9.14%\n9.30%\n-0.584\n0.559\n-0.002\n0.559\n\n\n\nNone of the above variables show statistically significant differences (all p-values &gt; 0.05). This suggests that the treatment and control groups were balanced at baseline, and that any later difference in outcomes is likely attributable to the treatment itself. These findings mirror Table 1 in Karlan & List (2007)."
  },
  {
    "objectID": "HW1/project1/hw1_questions.html#experimental-results",
    "href": "HW1/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nI begin by analyzing whether receiving a matching donation offer increases the likelihood of giving. As shown in the bar chart below, the response rate for the treatment group was 2.20%, while the control group had a response rate of only 1.79%.\n\n\n\n\nProportion of People Who Donated\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nresponse_rate = df.groupby('treatment')['gave'].mean().reset_index()\nresponse_rate['treatment'] = response_rate['treatment'].map({0: 'Control', 1: 'Treatment'})\n\nplt.figure(figsize=(6, 4))\nsns.barplot(data=response_rate, x='treatment', y='gave', palette=['#AFCBFF', '#FFD6A5'])\n\nplt.title('Proportion of People Who Donated')\nplt.xlabel('Group')\nplt.ylabel('Donation Rate')\nplt.ylim(0, 0.03)\nplt.grid(axis='y', linestyle='--')\nplt.tight_layout()\n\nplt.savefig('proportion_donated_by_group.png')\nplt.show()\n\n\n\n\nI conduct a two-sample t-test to compare the mean donation rate (gave) between the treatment and control groups:\n\n\n\n\n\n\nt-test code\n\n\n\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\ntreat_group = df[df['treatment'] == 1]['gave']\ncontrol_group = df[df['treatment'] == 0]['gave']\n\nt_stat, p_val = ttest_ind(treat_group, control_group)\nprint(f\"T-test: t = {t_stat:.3f}, p = {p_val:.4f}\")\n\n\n\n\nControl mean: ~1.79%\n\nTreatment mean: ~2.20%\n\nT-statistic: 3.10\n\np-value: 0.0019\n\nThis result indicates that the difference is statistically significant at the 1% level. In other words, people who received a matching donation offer were significantly more likely to donate.\n\nBivariate Linear Regression\nI also ran a linear regression model: gave ~ treatment:\nmodel = smf.ols('gave ~ treatment', data=df).fit()\nprint(model.summary())\n\nCoefficient on treatment: 0.00418\n\np-value: 0.0019\n\nThis suggests that assignment to treatment increases the donation probability by about 0.4 percentage points, which is a small but statistically meaningful effect, especially given the scale of the fundraising campaign. Together, the t-test and the regression confirm the same conclusion: the treatment group donated at a significantly higher rate than the control group.\n\n\n\nProbit Regression\nTo confirm the finding using a nonlinear model (as in the original paper), I also estimate a probit regression with the same dependent variable:\nprobit_model = smf.probit('gave ~ treatment', data=df).fit()\nprint(probit_model.summary())\n\nProbit coefficient on treatment: 0.087\np-value: 0.0019\n\nThis replicates the finding in Table 3, Column 1 of Karlan & List (2007), where the authors also find that the presence of a match significantly increases the probability of donation.\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\nt-test\nI conduct pairwise t-tests comparing donation rates across different match ratio groups within the treatment group:\ngave_1 = df[df['ratio'] == 1]['gave']\ngave_2 = df[df['ratio'] == 2]['gave']\ngave_3 = df[df['ratio'] == 3]['gave']\n\nprint(\"1:1 vs 2:1\", ttest_ind(gave_1, gave_2))\nprint(\"2:1 vs 3:1\", ttest_ind(gave_2, gave_3))\nprint(\"1:1 vs 3:1\", ttest_ind(gave_1, gave_3))\n\n$1:$1 vs $2:$1: t = -0.97, p = 0.335\n\n$2:$1 vs $3:$1: t = -0.05, p = 0.960\n\n$1:$1 vs $3:$1: t = -1.02, p = 0.310\n\nNone of these comparisons are statistically significant at the 5% level, which suggests that increasing the match ratio does not significantly increase the likelihood of giving—consistent with the authors’ conclusion on page 8 of the paper.\n\n\nRegression Analysis\nI also fit a linear regression model where the dependent variable is gave and the independent variables are ratio2 and ratio3, using $1:$1 match as the baseline:\nmatch_df = df[df['treatment'] == 1]\nmodel = smf.ols('gave ~ ratio2 + ratio3', data=match_df).fit()\nmodel.summary()\nRegression output summary:\n\nCoefficient on ratio2: 0.0019, p = 0.338\nCoefficient on ratio3: 0.0020, p = 0.313\n\nThe coefficients are small and statistically insignificant, confirming the same conclusion as the t-tests.\nDifference in Donation Rates\nWe also directly compute the difference in response rates:\nmatch_df = df[df['treatment'] == 1]\nmodel = smf.ols('gave ~ ratio2 + ratio3', data=match_df).fit()\ndiff_12 = gave_2.mean() - gave_1.mean()\ndiff_23 = gave_3.mean() - gave_2.mean()\ncoef_diff = model.params['ratio3'] - model.params['ratio2']\n\nFrom $1:$1 to $2:$1: +0.00188 (≈ 0.19 percentage points)\nFrom $2:$1 to $3:$1: +0.0001 (≈ 0.01 percentage points)\n\nFrom the regression coefficients:\n\nratio3 - ratio2 = +0.0001\n\nThese small and statistically insignificant changes indicate that donors do not respond more strongly to larger match ratios. Simply offering a match matters, but increasing the match ratio offers no additional benefit.\n\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nFull Sample Analysis (Including Non-Donors)\nFirst, I compare average donation amounts across all individuals, including those who gave $0. Using both a t-test and a linear regression:\nt_stat, p_val = ttest_ind(df[df['treatment'] == 1]['amount'], df[df['treatment'] == 0]['amount'])\nmodel = smf.ols('amount ~ treatment', data=df).fit()\n\nMean amount (control) = $0.813\n\nMean amount (treatment) = $0.967\n\nOLS coefficient on treatment = 0.1536\n\nt-statistic = 1.861\n\np-value = 0.063\n\nThe positive coefficient indicates that the treatment group gave slightly more on average. However, the p-value is just above the common 0.05 threshold, suggesting marginal significance. Most of the observed difference appears to be driven by the fact that more people gave in the treatment group, rather than those who gave giving significantly more.\n\n\nConditional on Donation\nI then limit the analysis to individuals who actually made a donation (gave == 1). I repeat the same steps:\ndonors_df = df[df['gave'] == 1]\nt2, p2 = ttest_ind(donors_df[donors_df['treatment'] == 1]['amount'],\n                   donors_df[donors_df['treatment'] == 0]['amount'])\nmodel2 = smf.ols('amount ~ treatment', data=donors_df).fit()\n\nMean amount (control) = $45.54\n\nMean amount (treatment) = $43.87\n\nOLS coefficient on treatment = -1.668\n\nt-statistic = -0.581\n\np-value = 0.561\n\nThese results suggest that conditional on donating, people in the treatment group did not give more, and actually gave slightly less on average (though not statistically significantly). The p-value of 0.561 indicates no meaningful difference.\nThis implies that the treatment’s impact was primarily at the extensive margin (increasing the number of people who gave), and not at the intensive margin (increasing donation amount conditional on giving). This result supports the original conclusion from Karlan & List (2007).\n\n\nHistograms of Donation Amounts\nBelow are two histograms showing the distribution of donation amounts among donors, separated by treatment group. The red dashed line represents the group mean.\n \n\n\n\n\n\n\nplot code\n\n\n\n\n\ntreatment_amount = donors_df[donors_df['treatment'] == 1]['amount']\ncontrol_amount = donors_df[donors_df['treatment'] == 0]['amount']\n\nplt.figure(figsize=(6,4))\nplt.hist(treatment_amount, bins=30, color='skyblue', edgecolor='black')\nplt.axvline(treatment_amount.mean(), color='red', linestyle='--', label=f'Mean = {treatment_amount.mean():.2f}')\nplt.title(\"Donation Amounts (Treatment Group)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"hist_treatment.png\")\nplt.show()\n\nplt.figure(figsize=(6,4))\nplt.hist(control_amount, bins=30, color='lightgreen', edgecolor='black')\nplt.axvline(control_amount.mean(), color='red', linestyle='--', label=f'Mean = {control_amount.mean():.2f}')\nplt.title(\"Donation Amounts (Control Group)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"hist_control.png\")\nplt.show()"
  },
  {
    "objectID": "HW1/project1/hw1_questions.html#simulation-experiment",
    "href": "HW1/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nTo demonstrate the Law of Large Numbers, I simulate two groups:\n\nA control group with a true probability of giving of p = 0.018\n\nA treatment group with a true probability of p = 0.022\n\nI draw 10,000 samples from each distribution and calculate the difference in donation outcome (1 or 0) for each pair. Then I compute the cumulative average of these 10,000 differences and plot the result below:\n\n\n\nLaw of Large Numbers Simulation\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\np_control = 0.018\np_treatment = 0.022\nn = 10000\nnp.random.seed(42)\n\ncontrol_sim = np.random.binomial(1, p_control, size=n)\ntreatment_sim = np.random.binomial(1, p_treatment, size=n)\ndiff_vector = treatment_sim - control_sim\ncumulative_avg = np.cumsum(diff_vector) / np.arange(1, n + 1)\n\nplt.figure(figsize=(8, 4))\nplt.plot(cumulative_avg, label='Cumulative Average Difference')\nplt.axhline(p_treatment - p_control, color='red', linestyle='--', label='True Difference (0.004)')\nplt.title('Law of Large Numbers Simulation')\nplt.xlabel('Sample Size')\nplt.ylabel('Cumulative Difference')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.savefig('law_of_large_numbers.png')\nplt.show()\n\n\n\n\n\nCentral Limit Theorem\n\nTo demonstrate the Central Limit Theorem (CLT), I simulate the difference in donation rates between the treatment (p = 0.022) and control (p = 0.018) groups at four different sample sizes: 50, 200, 500, and 1000. For each sample size, I repeat the experiment 1000 times, and in each trial I compute the average difference in donation rate:\n\nAt n = 50, the distribution is wide and irregular. The mean is slightly right of zero, but there’s a lot of noise. Zero is near the center.\nAt n = 200, the distribution starts to resemble a bell shape, but still has considerable spread.\nAt n = 500, the distribution becomes noticeably more symmetric, and the mean difference starts to stand out from zero.\nAt n = 1000, the distribution is tightly centered around 0.004, and zero is clearly in the left tail, indicating a consistent positive treatment effect.\n\n\n\n\nCLT Simulation\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\nnp.random.seed(42)\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\naxs = axs.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    differences = []\n    for _ in range(n_simulations):\n        control = np.random.binomial(1, p_control, n)\n        treatment = np.random.binomial(1, p_treatment, n)\n        diff = treatment.mean() - control.mean()\n        differences.append(diff)\n    \n    mean_diff = np.mean(differences)\n\n    axs[i].hist(differences, bins=30, color='lightblue', edgecolor='black')\n    axs[i].axvline(0, color='red', linestyle='--', label='Zero')\n    axs[i].axvline(mean_diff, color='green', linestyle='-', label=f'Mean = {mean_diff:.4f}')\n    axs[i].set_title(f'Sample Size = {n}')\n    axs[i].set_xlabel('Difference in Donation Rate')\n    axs[i].set_ylabel('Frequency')\n    axs[i].legend()\n\nplt.suptitle('CLT Simulation: Sampling Distribution of Differences', fontsize=14)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.savefig('clt_histograms_labeled.png')\nplt.show()\n\n\n\nI conclude that as sample size increases, the distribution of the average difference between treatment and control becomes more normal and more centered around the true mean—exactly as predicted by the Central Limit Theorem."
  },
  {
    "objectID": "HW1/project1/hw1_questions.html#conclusion",
    "href": "HW1/project1/hw1_questions.html#conclusion",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conclusion",
    "text": "Conclusion\nThis project gave me a chance to explore how matching donations affect charitable giving by replicating the results from Karlan & List (2007). After digging into the data, I found that people who received a matching offer were definitely more likely to donate—just like the original paper said. Even though the increase was small in percentage terms, it’s meaningful when you’re dealing with tens of thousands of people.\nOne thing that stood out to me was that higher match ratios (like 2:1 or 3:1) didn’t really help any more than the basic 1:1 offer. So the key seems to be just having a match at all—not how big the match is.\nWhen I looked at how much people gave, it turned out that the treatment group gave slightly more overall, but not because they gave more money individually. Instead, the bump came from more people deciding to give, not from people giving larger amounts.\nOverall, this was a great example of how subtle changes in message framing (like offering a match) can change real-world behavior. And it showed me how field experiments and behavioral economics can go hand-in-hand."
  }
]