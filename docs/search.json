[
  {
    "objectID": "HW1/project2/hw2.html",
    "href": "HW1/project2/hw2.html",
    "title": "Junye's Website",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"blueprinty.csv\")\n\nage_means = df.groupby(\"iscustomer\")[\"age\"].mean().rename(index={0: \"Non-Customer\", 1: \"Customer\"})\nprint(\"Mean Company Age:\\n\", age_means)\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x='age', hue='iscustomer', bins=30, palette='Set2', multiple='dodge')\nplt.title(\"Company Age Distribution by Customer Status\")\nplt.xlabel(\"Company Age (Years)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\nplt.tight_layout()\nplt.show()\n\nregion_dist = pd.crosstab(df[\"region\"], df[\"iscustomer\"], normalize=\"index\")\nregion_dist.columns = [\"Non-Customer\", \"Customer\"]\nprint(\"\\nRegion Distribution by Customer Status:\\n\")\nprint(region_dist)\n\nMean Company Age:\n iscustomer\nNon-Customer    26.101570\nCustomer        26.900208\nName: age, dtype: float64\n\n\n\n\n\n\n\n\n\n\nRegion Distribution by Customer Status:\n\n           Non-Customer  Customer\nregion                           \nMidwest        0.834821  0.165179\nNortheast      0.454243  0.545757\nNorthwest      0.844920  0.155080\nSouth          0.816754  0.183246\nSouthwest      0.824916  0.175084\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 读取数据\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# 设置无网格样式\nsns.set_style(\"white\")  # 或者使用 \"ticks\" 也可以\n\n# 绘图：地区直方图（按客户状态）\nplt.figure(figsize=(10, 6))\nsns.histplot(\n    data=df,\n    x='region',\n    hue='iscustomer',\n    multiple='dodge',\n    shrink=0.8,\n    palette='Set2',\n    stat='count',\n    edgecolor='black'\n)\n\nplt.title(\"Distribution of Firms by Region and Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\n\n# 去掉网格线\nplt.grid(False)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln  # log(y!) for numerical stability\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf  # log likelihood is undefined for λ &lt;= 0\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\nimport pandas as pd\ndf = pd.read_csv(\"blueprinty.csv\")\nY = df['patents'].values\n\n# Plot\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='darkblue')\nplt.title(\"Log-Likelihood of Poisson Model\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.axvline(np.mean(Y), color='red', linestyle='--', label='Mean of Y')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"blueprinty.csv\")\nsns.set_style(\"white\")\n\nplt.figure(figsize=(10, 6))\nax = sns.histplot(\n    data=df,\n    x='patents',\n    hue='iscustomer',\n    bins=30,\n    palette='Set2',\n    multiple='dodge'\n)\n\nfor bar in ax.patches:\n    bar.set_edgecolor(\"black\")\n    bar.set_linewidth(1)\n\nplt.title(\"Patent Count Distribution by Customer Status\")\nplt.xlabel(\"Number of Patents (past 5 years)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Load data\ndf = pd.read_csv(\"blueprinty.csv\")\nY = df[\"patents\"].values\n\n# Log-likelihood function\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\n# Plot log-likelihood\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='darkblue')\nplt.axvline(np.mean(Y), color='red', linestyle='--', label='Sample Mean')\nplt.title(\"Poisson Log-Likelihood Curve\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf  # log likelihood undefined for λ ≤ 0\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\nneg_loglik = lambda lmbda: -poisson_loglikelihood(lmbda, Y)\n\nresult = minimize_scalar(neg_loglik, bounds=(0.01, 20), method='bounded')\n\nlambda_mle = result.x\nprint(f\"MLE of λ (via optimization): {lambda_mle:.4f}\")\nprint(f\"Sample mean of Y (baseline): {np.mean(Y):.4f}\")\n\nMLE of λ (via optimization): 3.6847\nSample mean of Y (baseline): 3.6847\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\n\n# Load the data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Feature engineering\ndf[\"age_scaled\"] = df[\"age\"] / 10\ndf[\"age_sq_scaled\"] = (df[\"age\"] ** 2) / 100\n\n# Construct design matrix X\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age_scaled\", \"age_sq_scaled\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\nX_matrix = X.values\nY = df[\"patents\"].values\n\ndef poisson_loglikelihood_beta(beta, Y, X):\n    XB = X @ beta\n    lambdas = np.exp(XB)\n    # handle overflow safely\n    if np.any(np.isnan(lambdas)) or np.any(np.isinf(lambdas)):\n        return np.inf\n    loglik = np.sum(-lambdas + Y * XB - gammaln(Y + 1))\n    return -loglik  # negative for use with minimizer\n\nbeta_test = np.zeros(X.shape[1])\nloglik_value = -poisson_loglikelihood_beta(beta_test, Y, X_matrix)\nloglik_value\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nAttributeError: 'float' object has no attribute 'exp'\n\nThe above exception was the direct cause of the following exception:\n\nTypeError                                 Traceback (most recent call last)\nCell In[16], line 32\n     29     return -loglik  # negative for use with minimizer\n     31 beta_test = np.zeros(X.shape[1])\n---&gt; 32 loglik_value = -poisson_loglikelihood_beta(beta_test, Y, X_matrix)\n     33 loglik_value\n\nCell In[16], line 24, in poisson_loglikelihood_beta(beta, Y, X)\n     22 def poisson_loglikelihood_beta(beta, Y, X):\n     23     XB = X @ beta\n---&gt; 24     lambdas = np.exp(XB)\n     25     # handle overflow safely\n     26     if np.any(np.isnan(lambdas)) or np.any(np.isinf(lambdas)):\n\nTypeError: loop of ufunc does not support argument 0 of type float which has no callable exp method\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\n# 读取数据\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# 特征构建：标准化 age，防止数值过大导致优化不稳定\ndf[\"age_scaled\"] = df[\"age\"] / 10\ndf[\"age_sq_scaled\"] = (df[\"age\"] ** 2) / 100\n\n# 构建 X 设计矩阵（带截距 + age + age² + region dummies + iscustomer）\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age_scaled\", \"age_sq_scaled\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\nX = X.astype(float)\nY = df[\"patents\"]\n\n# 使用 GLM (Poisson) 拟合\nmodel = sm.GLM(Y, X, family=sm.families.Poisson())\nresults = model.fit()\n\n# 输出结果表（系数 + 标准误）\nsummary_df = pd.DataFrame({\n    \"Coefficient\": results.params,\n    \"Std. Error\": results.bse\n})\nprint(summary_df)\n\n               Coefficient  Std. Error\nintercept        -0.508920    0.183179\nage_scaled        1.486195    0.138686\nage_sq_scaled    -0.297047    0.025801\niscustomer        0.207591    0.030895\nNortheast         0.029170    0.043625\nNorthwest        -0.017575    0.053781\nSouth             0.056561    0.052662\nSouthwest         0.050576    0.047198\n\n\n\nX\n\n\n\n\n\n\n\n\nintercept\nage_scaled\nage_sq_scaled\niscustomer\nNortheast\nNorthwest\nSouth\nSouthwest\n\n\n\n\n0\n1.0\n3.25\n10.5625\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n1.0\n3.75\n14.0625\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n2\n1.0\n2.70\n7.2900\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n3\n1.0\n2.45\n6.0025\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n4\n1.0\n3.70\n13.6900\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1495\n1.0\n1.85\n3.4225\n1.0\n1.0\n0.0\n0.0\n0.0\n\n\n1496\n1.0\n2.25\n5.0625\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1497\n1.0\n1.70\n2.8900\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n1498\n1.0\n2.90\n8.4100\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n1499\n1.0\n3.90\n15.2100\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n\n\n1500 rows × 8 columns\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\n# Load data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Feature scaling\ndf[\"age_scaled\"] = df[\"age\"] / 10\ndf[\"age_sq_scaled\"] = (df[\"age\"] ** 2) / 100\n\n# Construct design matrix\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX = pd.concat([\n    pd.Series(1.0, index=df.index, name=\"intercept\"),\n    df[[\"age_scaled\", \"age_sq_scaled\", \"iscustomer\"]],\n    region_dummies\n], axis=1).astype(float)\nX_matrix = X.values\nY = df[\"patents\"].values\n\n# Define Poisson log-likelihood\ndef poisson_loglikelihood_beta(beta, Y, X):\n    XB = X @ beta\n    lambdas = np.exp(XB)\n    if np.any(np.isnan(lambdas)) or np.any(np.isinf(lambdas)):\n        return np.inf\n    loglik = np.sum(-lambdas + Y * XB - gammaln(Y + 1))\n    return -loglik  # minimize negative log-likelihood\n\n# Optimize\nbeta_start = np.zeros(X.shape[1])\nres = minimize(poisson_loglikelihood_beta, beta_start, args=(Y, X_matrix), method=\"BFGS\")\n\n# Extract coefficient estimates and standard errors\nbeta_hat = res.x\nhessian_inv = res.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Create result table\nresults = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X.columns)\n\nprint(results)\n\n               Coefficient  Std. Error\nintercept        -0.508925    0.409336\nage_scaled        1.486199    0.296445\nage_sq_scaled    -0.297048    0.052228\niscustomer        0.207591    0.031828\nNortheast         0.029170    0.024360\nNorthwest        -0.017575    0.045547\nSouth             0.056561    0.045048\nSouthwest         0.050576    0.024359\n\n\n\n# 假设你已有：\n# - beta_hat（MLE β 向量）\n# - X（原始设计矩阵，DataFrame 格式）\n\n# 创建 X_0 和 X_1\nX_0 = X.copy()\nX_1 = X.copy()\n\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# 预测两组的 λ（预期专利数）\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\n# 差值并求平均\neffect_vector = y_pred_1 - y_pred_0\naverage_effect = np.mean(effect_vector)\n\nprint(f\"Average predicted increase in patents due to Blueprinty: {average_effect:.4f}\")\n\nAverage predicted increase in patents due to Blueprinty: 0.7928\n\n\n\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\navg_y_0 = np.mean(y_pred_0)\navg_y_1 = np.mean(y_pred_1)\n\n\ncomparison_df = pd.DataFrame({\n    \"Scenario\": [\"All Non-Customers\", \"All Customers\"],\n    \"Average Predicted Patents\": [avg_y_0, avg_y_1]\n})\n\n\ncolors = sns.color_palette(\"Set2\")\n\nplt.figure(figsize=(6, 5))\nbars = plt.bar(\n    comparison_df[\"Scenario\"],\n    comparison_df[\"Average Predicted Patents\"],\n    color=[colors[0], colors[1]],\n    edgecolor=\"black\"\n)\n\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(\n        bar.get_x() + bar.get_width() / 2,\n        height + 0.05,\n        f\"{height:.2f}\",\n        ha='center',\n        va='bottom',\n        fontsize=11\n    )\n\nplt.title(\"Predicted Average Number of Patents\\nUnder Counterfactual Scenarios\")\nplt.ylabel(\"Average Predicted Patents\")\nplt.ylim(0, max(avg_y_0, avg_y_1) + 1)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load data\ndf = pd.read_csv(\"/Users/junyefan/Desktop/Graduate/Course/2025_Spring/MGTA_495_Marketing_Analytics/Week1/quartosite/HW1/project2/airbnb.csv\")\n\n# Select relevant variables\nvars_used = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\ndf = df[vars_used]\n\n# Drop rows with missing values\ndf_clean = df.dropna()\n\n# Convert instant_bookable to binary\ndf_clean[\"instant_bookable\"] = (df_clean[\"instant_bookable\"] == \"t\").astype(int)\n\n# Convert room_type to dummies\ndf_dummies = pd.get_dummies(df_clean, columns=[\"room_type\"], drop_first=True)\n\n# Poisson Regression\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import glm\n\n# Build formula (log link implied by default Poisson)\nformula = \"number_of_reviews ~ bathrooms + bedrooms + price + review_scores_cleanliness + review_scores_location + review_scores_value + instant_bookable + room_type_Private room + room_type_Shared room\"\n\nmodel = glm(formula=formula, data=df_dummies, family=sm.families.Poisson()).fit()\nprint(model.summary())\n\n/var/folders/31/d8_mdwcs6k53znfr05m5f2mw0000gn/T/ipykernel_50016/1658976266.py:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_clean[\"instant_bookable\"] = (df_clean[\"instant_bookable\"] == \"t\").astype(int)\n\n\n\nTraceback (most recent call last):\n\n  File ~/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3577 in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\n  Cell In[28], line 32\n    model = glm(formula=formula, data=df_dummies, family=sm.families.Poisson()).fit()\n\n  File ~/.local/lib/python3.12/site-packages/statsmodels/base/model.py:203 in from_formula\n    tmp = handle_formula_data(data, None, formula, depth=eval_env,\n\n  File ~/.local/lib/python3.12/site-packages/statsmodels/formula/formulatools.py:63 in handle_formula_data\n    result = dmatrices(formula, Y, depth, return_type='dataframe',\n\n  File ~/.local/lib/python3.12/site-packages/patsy/highlevel.py:309 in dmatrices\n    (lhs, rhs) = _do_highlevel_design(formula_like, data, eval_env,\n\n  File ~/.local/lib/python3.12/site-packages/patsy/highlevel.py:164 in _do_highlevel_design\n    design_infos = _try_incr_builders(formula_like, data_iter_maker, eval_env,\n\n  File ~/.local/lib/python3.12/site-packages/patsy/highlevel.py:66 in _try_incr_builders\n    return design_matrix_builders([formula_like.lhs_termlist,\n\n  File ~/.local/lib/python3.12/site-packages/patsy/build.py:689 in design_matrix_builders\n    factor_states = _factors_memorize(all_factors, data_iter_maker, eval_env)\n\n  File ~/.local/lib/python3.12/site-packages/patsy/build.py:354 in _factors_memorize\n    which_pass = factor.memorize_passes_needed(state, eval_env)\n\n  File ~/.local/lib/python3.12/site-packages/patsy/eval.py:478 in memorize_passes_needed\n    subset_names = [name for name in ast_names(self.code)\n\n  File ~/.local/lib/python3.12/site-packages/patsy/eval.py:109 in ast_names\n    for node in ast.walk(ast.parse(code)):\n\n  File /opt/miniconda3/lib/python3.12/ast.py:52 in parse\n    return compile(source, filename, mode, flags,\n\n  File &lt;unknown&gt;:1\n    room_type_Shared room\n                     ^\nSyntaxError: invalid syntax\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import glm\n\ndf = pd.read_csv(\"/Users/junyefan/Desktop/Graduate/Course/2025_Spring/MGTA_495_Marketing_Analytics/Week1/quartosite/HW1/project2/airbnb.csv\")\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nvars_used = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\ndf = df[vars_used]\n\n# Drop rows with missing values\ndf_clean = df.dropna()\n\n# Convert variables\ndf_clean[\"instant_bookable\"] = (df_clean[\"instant_bookable\"] == \"t\").astype(int)\ndf_dummies = pd.get_dummies(df_clean, columns=[\"room_type\"], drop_first=True)\ndf_dummies.columns = df_dummies.columns.str.replace(\" \", \"_\")\n\n# Poisson regression\nformula = (\n    \"number_of_reviews ~ bathrooms + bedrooms + price + \"\n    \"review_scores_cleanliness + review_scores_location + \"\n    \"review_scores_value + instant_bookable + \"\n    \"room_type_Private_room + room_type_Shared_room\"\n)\nmodel = glm(formula=formula, data=df_dummies, family=sm.families.Poisson()).fit()\nresults_df = pd.DataFrame({\n    \"Coefficient\": model.params,\n    \"Std. Error\": model.bse\n})\nprint(results_df)\n\n# 定义目标列\nreviews = df_clean[\"number_of_reviews\"]\n\n# 描述性统计\nprint(\"Summary Statistics:\")\nprint(reviews.describe())\nprint(\"Skewness:\", reviews.skew())\nprint(\"Kurtosis:\", reviews.kurtosis())\n\n# 分布图\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 5))\nsns.histplot(reviews, bins=50, kde=True)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Frequency\")\nplt.tight_layout()\nplt.show()\n\n/var/folders/31/d8_mdwcs6k53znfr05m5f2mw0000gn/T/ipykernel_50016/2796586652.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_clean[\"instant_bookable\"] = (df_clean[\"instant_bookable\"] == \"t\").astype(int)\n\n\n                                Coefficient  Std. Error\nIntercept                          3.572486    0.016005\nroom_type_Private_room[T.True]    -0.014535    0.002737\nroom_type_Shared_room[T.True]     -0.251896    0.008618\nbathrooms                         -0.123999    0.003747\nbedrooms                           0.074941    0.001988\nprice                             -0.000014    0.000008\nreview_scores_cleanliness          0.113187    0.001493\nreview_scores_location            -0.076795    0.001607\nreview_scores_value               -0.091529    0.001798\ninstant_bookable                   0.334397    0.002889\nSummary Statistics:\ncount    30160.000000\nmean        21.170889\nstd         32.007541\nmin          1.000000\n25%          3.000000\n50%          8.000000\n75%         26.000000\nmax        421.000000\nName: number_of_reviews, dtype: float64\nSkewness: 3.1723046468074183\nKurtosis: 14.564929111706064\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 确保数据中包含这两个变量\n# 如果你之前已经做了 df_clean[\"instant_bookable\"] = ...，你可以这样画：\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(x=\"instant_bookable\", y=\"number_of_reviews\", data=df_clean)\nplt.title(\"Number of Reviews by Instant Bookable\")\nplt.xlabel(\"Instant Bookable (0 = No, 1 = Yes)\")\nplt.ylabel(\"Number of Reviews\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "HW1/project3/hw3_questions.html",
    "href": "HW1/project3/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "HW1/project3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "HW1/project3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "HW1/project3/hw3_questions.html#simulate-conjoint-data",
    "href": "HW1/project3/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\ncode\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\n\n# Set random seed\nnp.random.seed(123)\n\n# Define attributes\nbrands = ['N', 'P', 'H']  # Netflix, Prime, Hulu\nads = ['Yes', 'No']\nprices = np.arange(8, 33, 4)  # 8 to 32 by 4\n\n# Generate all possible profiles\nimport itertools\nprofiles = pd.DataFrame(list(itertools.product(brands, ads, prices)), columns=['brand', 'ad', 'price'])\nm = len(profiles)\n\n# Part-worth utilities (true parameters)\nb_util = {'N': 1.0, 'P': 0.5, 'H': 0.0}\na_util = {'Yes': -0.8, 'No': 0.0}\ndef p_util(p): return -0.1 * p\n\n# Simulation parameters\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Function to simulate one respondent\ndef sim_one(id):\n    datlist = []\n    for t in range(1, n_tasks + 1):\n        sampled_profiles = profiles.sample(n=n_alts).copy()\n        sampled_profiles['resp'] = id\n        sampled_profiles['task'] = t\n\n        # Calculate deterministic utility\n        sampled_profiles['v'] = sampled_profiles.apply(\n            lambda row: b_util[row['brand']] + a_util[row['ad']] + p_util(row['price']),\n            axis=1\n        )\n\n        # Add Gumbel (Type I EV) noise\n        sampled_profiles['e'] = -np.log(-np.log(np.random.uniform(size=n_alts)))\n        sampled_profiles['u'] = sampled_profiles['v'] + sampled_profiles['e']\n\n        # Identify chosen alternative\n        sampled_profiles['choice'] = (sampled_profiles['u'] == sampled_profiles['u'].max()).astype(int)\n\n        datlist.append(sampled_profiles[['resp', 'task', 'brand', 'ad', 'price', 'choice']])\n    \n    return pd.concat(datlist, ignore_index=True)\n\n# Simulate data for all respondents\nconjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)"
  },
  {
    "objectID": "HW1/project3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "HW1/project3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\nBefore estimating the model, I prepare the data so that the utility function can be computed as a linear combination of covariates. This requires converting the categorical variables into dummy variables.\nSpecifically, I treat Hulu as the reference level for brand, and ad-free (No) as the reference for the ad variable. I create binary indicators for:\n\nNetflix (\\(\\beta_{\\text{netflix}}\\)),\nAmazon Prime (\\(\\beta_{\\text{prime}}\\)),\nAd presence (\\(\\beta_{\\text{ads}}\\)),\nAnd include price as a numeric variable (\\(\\beta_{\\text{price}}\\)).\n\nEach row in the dataset corresponds to an alternative presented in a choice task.\n\n\n\n\n\n\ncode\n\n\n\n\n\n# Convert categorical variables into dummy variables\ndf['brand'] = df['brand'].astype(str)\ndf['ad'] = df['ad'].astype(str)\n\n# Create dummies for brand (Hulu as reference)\nbrand_dummies = pd.get_dummies(df['brand'], prefix='brand')\nbrand_dummies.drop(columns=['brand_H'], inplace=True)\n\n# Convert ad to binary: Yes = 1, No = 0\ndf['ad_binary'] = df['ad'].map({'Yes': 1, 'No': 0})\n\n# Combine into feature matrix\nX = pd.concat([\n    df[['resp', 'task', 'choice', 'price']],\n    brand_dummies,\n    df['ad_binary']\n], axis=1)\n\n# Rename for clarity\nX.rename(columns={\n    'brand_N': 'brand_netflix',\n    'brand_P': 'brand_prime'\n}, inplace=True)\n\n# Final structure\nX = X[['resp', 'task', 'choice', 'brand_netflix', 'brand_prime', 'ad_binary', 'price']]\n\n\n\n\nReshaped Variable Definitions\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nresp\nRespondent ID\n\n\ntask\nChoice task number (each respondent has 10 tasks)\n\n\nchoice\nBinary indicator for whether this alternative was chosen (1 = chosen)\n\n\nbrand_netflix\n1 if the alternative is Netflix, 0 otherwise (Hulu is the reference level)\n\n\nbrand_prime\n1 if the alternative is Amazon Prime, 0 otherwise\n\n\nad_binary\n1 if the alternative includes ads (“Yes”), 0 if ad-free (“No”)\n\n\nprice\nMonthly subscription price of the alternative (in dollars)"
  },
  {
    "objectID": "HW1/project3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "HW1/project3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\nThe log-likelihood function is derived from the softmax probability of choosing an alternative:\n\\[\n\\ell(\\beta) = \\sum_{i,j} \\delta_{ij} \\log \\left( \\frac{e^{x_{ij}^\\top \\beta}}{\\sum_{k \\in C_i} e^{x_{ik}^\\top \\beta}} \\right)\n\\]\nI implement this in Python by computing the utility for each alternative, applying the softmax transformation within each choice set, and summing the log-probabilities of the chosen alternatives.\nTo estimate the parameters of the multinomial logit model, I define a log-likelihood function that reflects the probability of observing the actual choices made by each respondent. The utility for each alternative is modeled as a linear combination of product attributes and estimated coefficients.\nI use scipy.optimize.minimize() to maximize the log-likelihood (by minimizing the negative log-likelihood). The four parameters to be estimated are: - \\(\\beta_\\text{netflix}\\) for the Netflix brand dummy, - \\(\\beta_\\text{prime}\\) for the Amazon Prime dummy, - \\(\\beta_\\text{ads}\\) for ad presence, - \\(\\beta_\\text{price}\\) for price.\nI also calculate standard errors based on the inverse Hessian matrix and construct 95% confidence intervals for each coefficient.\n\n\n\n\n\n\ncode\n\n\n\n\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Define input variables for estimation\nX_vars = ['brand_netflix', 'brand_prime', 'ad_binary', 'price']\nX_mat = X[X_vars].values\ny = X['choice'].values\n\n# Group each choice set (task within respondent) for probability denominator\ngroup_ids = X.groupby(['resp', 'task']).ngroup().values\n\n# Define the negative log-likelihood function\ndef neg_log_likelihood(beta):\n    utilities = X_mat @ beta\n    df_temp = pd.DataFrame({\n        'group': group_ids,\n        'utility': utilities,\n        'choice': y\n    })\n    df_temp['exp_u'] = np.exp(df_temp['utility'])\n\n    # Compute denominator of softmax per choice set\n    group_sums = df_temp.groupby('group')['exp_u'].transform('sum')\n    df_temp['log_prob'] = df_temp['utility'] - np.log(group_sums)\n\n    # Only keep log probabilities of chosen alternatives\n    chosen_log_probs = df_temp[df_temp['choice'] == 1]['log_prob']\n    return -np.sum(chosen_log_probs)\n\n# Initial parameter guess\nbeta_init = np.zeros(X_mat.shape[1])\n\n# Minimize the negative log-likelihood\nresult = minimize(neg_log_likelihood, beta_init, method='BFGS')\n\n# Extract results\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Construct 95% confidence intervals\nz = 1.96\nconf_int = np.vstack([beta_hat - z * standard_errors, beta_hat + z * standard_errors]).T\n\n# Combine into a summary table\nparam_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']\nmle_summary = pd.DataFrame({\n    'parameter': param_names,\n    'estimate': beta_hat,\n    'std_error': standard_errors,\n    'ci_lower': conf_int[:, 0],\n    'ci_upper': conf_int[:, 1]\n})\n\nmle_summary.round(4)\n\n\n\n\nThe table below shows the estimated coefficients along with their standard errors and 95% confidence intervals based on the inverse Hessian from the MLE procedure.\n\n\n\nParameter\nEstimate\nStd. Error\n95% CI\n\n\n\n\nbeta_netflix\n0.9412\n1.4043\n[-1.8112, 3.6936]\n\n\nbeta_prime\n0.5016\n1.6607\n[-2.7534, 3.7566]\n\n\nbeta_ads\n-0.7320\n0.3449\n[-1.4080, -0.0560]\n\n\nbeta_price\n-0.0995\n0.0084\n[-0.1160, -0.0830]"
  },
  {
    "objectID": "HW1/project3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "HW1/project3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\nI implement a Metropolis-Hastings MCMC sampler to approximate the posterior distribution of the four model parameters. I use a total of 11,000 iterations, discarding the first 1,000 as burn-in and retaining 10,000 posterior draws.\nFor the priors:\n\nI assume \\(N(0, 5^2)\\) for beta_netflix, beta_prime, and beta_ads,\nand \\(N(0, 1^2)\\) for beta_price.\n\nThe proposal distribution is a multivariate normal centered at the current state with a diagonal covariance matrix:\n\\(\\Sigma = \\text{diag}(0.05, 0.05, 0.05, 0.005)\\)\nEach parameter is updated jointly in the 4-dimensional space.\n\n\n\n\n\n\ncode\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Reuse log-likelihood function from earlier\ndef log_likelihood(beta):\n    utilities = X_mat @ beta\n    df_temp = pd.DataFrame({\n        'group': group_ids,\n        'utility': utilities,\n        'choice': y\n    })\n    df_temp['exp_u'] = np.exp(pd.to_numeric(df_temp['utility'], errors='coerce'))\n    group_sums = df_temp.groupby('group')['exp_u'].transform('sum')\n    df_temp['log_prob'] = df_temp['utility'] - np.log(group_sums)\n    return np.sum(df_temp[df_temp['choice'] == 1]['log_prob'])\n\n# Log-prior function\ndef log_prior(beta):\n    return (\n        -0.5 * (beta[0]**2 + beta[1]**2 + beta[2]**2) / 25  # N(0, 5^2)\n        -0.5 * (beta[3]**2)  # N(0, 1^2) for price\n    )\n\n# Log-posterior\ndef log_posterior(beta):\n    return log_likelihood(beta) + log_prior(beta)\n\n# Proposal step: N(0, diag(...))\nproposal_sd = np.array([0.05, 0.05, 0.05, 0.005])\n\n# MCMC settings\nn_steps = 11000\nbeta_draws = np.zeros((n_steps, 4))\nbeta_current = np.zeros(4)\nlog_post_current = log_posterior(beta_current)\n\n# Run Metropolis-Hastings\nfor t in range(1, n_steps):\n    beta_proposal = beta_current + np.random.normal(0, proposal_sd)\n    log_post_proposal = log_posterior(beta_proposal)\n    accept_ratio = np.exp(log_post_proposal - log_post_current)\n\n    if np.random.rand() &lt; accept_ratio:\n        beta_current = beta_proposal\n        log_post_current = log_post_proposal\n\n    beta_draws[t, :] = beta_current\n\n# Drop first 1000 draws (burn-in)\nposterior_samples = beta_draws[1000:, :]\n\n\n\n\n \n\n\n\n\n\n\nplot code\n\n\n\n\n\n\nparam_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']\n\n# Trace plot for beta_price\nplt.figure(figsize=(10, 4))\nplt.plot(posterior_samples[:, 3])\nplt.title(\"Trace Plot: beta_price\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Value\")\nplt.grid(True)\nplt.show()\n\n# Histogram for beta_price\nplt.hist(posterior_samples[:, 3], bins=30, edgecolor='k')\nplt.title(\"Posterior Distribution: beta_price\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\n# Summary statistics\nposterior_df = pd.DataFrame(posterior_samples, columns=param_names)\nposterior_summary = posterior_df.describe(percentiles=[0.025, 0.975]).T\nposterior_summary['mean'] = posterior_df.mean()\nposterior_summary['std'] = posterior_df.std()\nposterior_summary['2.5%'] = posterior_df.quantile(0.025)\nposterior_summary['97.5%'] = posterior_df.quantile(0.975)\n\nposterior_summary[['mean', 'std', '2.5%', '97.5%']].round(4)\n\n\n\n\nBased on the retained 10,000 posterior draws from the Metropolis-Hastings algorithm, I summarize the posterior distributions for the four coefficients below. For each parameter, I report the posterior mean, standard deviation, and 95% credible interval.\nThese results can be compared directly to the MLE estimates from the previous section.\n\n\n\n\n\n\n\n\n\nParameter\nPosterior Mean\nStd. Dev.\n95% CI\n\n\n\n\nbeta_netflix\n0.9427\n0.1096\n[0.7156, 1.1490]\n\n\nbeta_prime\n0.5026\n0.1060\n[0.2870, 0.7043]\n\n\nbeta_ads\n-0.7316\n0.0885\n[-0.9113, -0.5526]\n\n\nbeta_price\n-0.0998\n0.0064\n[-0.1127, -0.0879]"
  },
  {
    "objectID": "HW1/project3/hw3_questions.html#discussion",
    "href": "HW1/project3/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\n\nIf I did not simulate the data and instead treated this as real-world survey responses, the parameter estimates would suggest meaningful behavioral patterns.\nFirst, since \\(\\beta_{\\text{Netflix}} &gt; \\beta_{\\text{Prime}}\\), I would interpret this to mean that, all else equal, users have a stronger preference for Netflix over Amazon Prime. Specifically, Netflix contributes more to overall utility than Prime, indicating that it is perceived as more desirable by the average respondent.\nThe negative coefficient on price (\\(\\beta_{\\text{price}} &lt; 0\\)) is also intuitive: higher prices reduce utility, making a streaming offer less likely to be chosen. This aligns with standard consumer behavior — people prefer lower-cost options when all else is held constant.\nFinally, the negative and statistically significant coefficient on ads indicates that consumers generally dislike advertisements and are more likely to choose ad-free plans.\nOverall, these estimates would provide interpretable insights into user preferences if they came from actual choice experiment data.\n\nTo simulate data from a hierarchical or random-parameter logit model, I would need to introduce individual-level heterogeneity in the coefficients. Instead of assuming a single global \\(\\beta\\) vector shared by all respondents, I would assume that each respondent i has their own vector of part-worths _i, drawn from a population-level distribution.\nFor example, I might assume:\n\\[\n\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\n\\]\nThen for each respondent:\n\nI would draw a unique _i from this multivariate normal prior,\nCompute their choice probabilities using _i,\nAnd simulate choices accordingly.\n\nTo estimate such a model, I would need to switch from maximum likelihood to hierarchical Bayesian methods, such as:\n\nUsing Gibbs sampling or Hamiltonian Monte Carlo (HMC),\nOr leveraging probabilistic programming tools like Stan or PyMC.\n\nThis hierarchical model is more flexible than standard MNL, as it accounts for preference heterogeneity across users, which is often observed in real-world conjoint data.\nIn real-world conjoint studies, using a hierarchical model improves predictive accuracy and allows for more personalized insights at the individual respondent level."
  },
  {
    "objectID": "HW1/project1/hw1_questions.html",
    "href": "HW1/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was designed to test how different types of matching grants—offers to match the donor’s contribution at different rates—would influence both the likelihood of giving and the amount donated. The treatments varied in three main dimensions:\n\nMatch ratio: The donor’s contribution was matched by a leadership donor at a ratio of $1:$1, $2:$1, or $3:$1.\nMaximum match amount: The cap on the matching gift was randomly set at $25,000, $50,000, $100,000, or left unstated.\nSuggested donation amount: Based on the recipient’s highest previous contribution (HPC), the letters included one of three suggestions: HPC × 1.00, HPC × 1.25, or HPC × 1.50.\n\nThese variations were fully randomized, making this a natural field experiment that allows for strong causal inference.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "HW1/project1/hw1_questions.html#introduction",
    "href": "HW1/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThe experiment was designed to test how different types of matching grants—offers to match the donor’s contribution at different rates—would influence both the likelihood of giving and the amount donated. The treatments varied in three main dimensions:\n\nMatch ratio: The donor’s contribution was matched by a leadership donor at a ratio of $1:$1, $2:$1, or $3:$1.\nMaximum match amount: The cap on the matching gift was randomly set at $25,000, $50,000, $100,000, or left unstated.\nSuggested donation amount: Based on the recipient’s highest previous contribution (HPC), the letters included one of three suggestions: HPC × 1.00, HPC × 1.25, or HPC × 1.50.\n\nThese variations were fully randomized, making this a natural field experiment that allows for strong causal inference.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "HW1/project1/hw1_questions.html#data",
    "href": "HW1/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset provided by Karlan and List (2007) is in .dta (Stata) format and includes over 50,000 observations, one for each individual who received a fundraising letter. Each row represents a donor and contains information about the treatment they were assigned to, their prior donation history, demographic characteristics, and whether they donated after receiving the letter.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nTo assess whether the randomization created comparable groups, I performed balance checks on several background variables. These include donation history (mrm2, freq, years), demographics (female, couple), and income indicators (ltmedmra, median_hhincome).\nFor each variable, I compared:\n\nthe mean in the treatment vs control group,\na t-test of the difference in means,\nand a regression coefficient from variable ~ treatment.\n\n\n\n\n\n\n\nbasic code\n\n\n\n\n\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n\n\nvars_to_test = ['mrm2', 'freq', 'years', 'female', 'ltmedmra', 'median_hhincome', 'couple']\n\nresults = []\n\nfor var in vars_to_test:\n    treat = df[df['treatment'] == 1][var]\n    control = df[df['treatment'] == 0][var]\n    t_stat, p_val = ttest_ind(treat, control, nan_policy='omit')\n    \n    model = smf.ols(f'{var} ~ treatment', data=df).fit()\n    coef = model.params['treatment']\n    reg_p = model.pvalues['treatment']\n    \n    results.append({\n        \"Variable\": var,\n        \"Treatment Mean\": round(treat.mean(), 3),\n        \"Control Mean\": round(control.mean(), 3),\n        \"T-Statistic\": round(t_stat, 3),\n        \"P-Value\": round(p_val, 3),\n        \"OLS Coefficient\": round(coef, 3),\n        \"OLS P-Value\": round(reg_p, 3)\n    })\n\nbalance_summary = pd.DataFrame(results)\n\nbalance_summary.loc[balance_summary['Variable'] == 'female', ['Treatment Mean', 'Control Mean']] *= 100\nbalance_summary.loc[balance_summary['Variable'] == 'couple', ['Treatment Mean', 'Control Mean']] *= 100\nbalance_summary.loc[balance_summary['Variable'] == 'ltmedmra', ['Treatment Mean', 'Control Mean']] *= 100\n\nbalance_summary.loc[balance_summary['Variable'] == 'median_hhincome', ['Treatment Mean', 'Control Mean']] = \\\n    balance_summary.loc[balance_summary['Variable'] == 'median_hhincome', ['Treatment Mean', 'Control Mean']].applymap(lambda x: f\"${x:,.0f}\")\n\nbalance_summary\nThe table below summarizes these tests:\n\n\nBalance Test Summaries\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nTreatment Mean\nControl Mean\nT-Statistic\nP-Value\nOLS Coefficient\nOLS P-Value\n\n\n\n\nmrm2\n13.01\n12.998\n0.119\n0.905\n0.014\n0.905\n\n\nfreq\n8.04\n8.05\n-0.111\n0.912\n-0.012\n0.912\n\n\nyears\n6.08\n6.14\n-1.103\n0.270\n-0.058\n0.270\n\n\nfemale\n27.5%\n28.3%\n-1.758\n0.079\n-0.008\n0.079\n\n\nltmedmra\n49.7%\n48.8%\n1.910\n0.056\n0.009\n0.056\n\n\nmedian_hhincome\n$54,763\n$54,921\n-0.742\n0.458\n-157.93\n0.458\n\n\ncouple\n9.14%\n9.30%\n-0.584\n0.559\n-0.002\n0.559\n\n\n\nNone of the above variables show statistically significant differences (all p-values &gt; 0.05). This suggests that the treatment and control groups were balanced at baseline, and that any later difference in outcomes is likely attributable to the treatment itself. These findings mirror Table 1 in Karlan & List (2007)."
  },
  {
    "objectID": "HW1/project1/hw1_questions.html#experimental-results",
    "href": "HW1/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nI begin by analyzing whether receiving a matching donation offer increases the likelihood of giving. As shown in the bar chart below, the response rate for the treatment group was 2.20%, while the control group had a response rate of only 1.79%.\n\n\n\n\nProportion of People Who Donated\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nresponse_rate = df.groupby('treatment')['gave'].mean().reset_index()\nresponse_rate['treatment'] = response_rate['treatment'].map({0: 'Control', 1: 'Treatment'})\n\nplt.figure(figsize=(6, 4))\nsns.barplot(data=response_rate, x='treatment', y='gave', palette=['#AFCBFF', '#FFD6A5'])\n\nplt.title('Proportion of People Who Donated')\nplt.xlabel('Group')\nplt.ylabel('Donation Rate')\nplt.ylim(0, 0.03)\nplt.grid(axis='y', linestyle='--')\nplt.tight_layout()\n\nplt.savefig('proportion_donated_by_group.png')\nplt.show()\n\n\n\n\nI conduct a two-sample t-test to compare the mean donation rate (gave) between the treatment and control groups:\n\n\n\n\n\n\nt-test code\n\n\n\n\n\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\ntreat_group = df[df['treatment'] == 1]['gave']\ncontrol_group = df[df['treatment'] == 0]['gave']\n\nt_stat, p_val = ttest_ind(treat_group, control_group)\nprint(f\"T-test: t = {t_stat:.3f}, p = {p_val:.4f}\")\n\n\n\n\nControl mean: ~1.79%\n\nTreatment mean: ~2.20%\n\nT-statistic: 3.10\n\np-value: 0.0019\n\nThis result indicates that the difference is statistically significant at the 1% level. In other words, people who received a matching donation offer were significantly more likely to donate.\n\nBivariate Linear Regression\nI also ran a linear regression model: gave ~ treatment:\nmodel = smf.ols('gave ~ treatment', data=df).fit()\nprint(model.summary())\n\nCoefficient on treatment: 0.00418\n\np-value: 0.0019\n\nThis suggests that assignment to treatment increases the donation probability by about 0.4 percentage points, which is a small but statistically meaningful effect, especially given the scale of the fundraising campaign. Together, the t-test and the regression confirm the same conclusion: the treatment group donated at a significantly higher rate than the control group.\n\n\n\nProbit Regression\nTo confirm the finding using a nonlinear model (as in the original paper), I also estimate a probit regression with the same dependent variable:\nprobit_model = smf.probit('gave ~ treatment', data=df).fit()\nprint(probit_model.summary())\n\nProbit coefficient on treatment: 0.087\np-value: 0.0019\n\nThis replicates the finding in Table 3, Column 1 of Karlan & List (2007), where the authors also find that the presence of a match significantly increases the probability of donation.\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\nt-test\nI conduct pairwise t-tests comparing donation rates across different match ratio groups within the treatment group:\ngave_1 = df[df['ratio'] == 1]['gave']\ngave_2 = df[df['ratio'] == 2]['gave']\ngave_3 = df[df['ratio'] == 3]['gave']\n\nprint(\"1:1 vs 2:1\", ttest_ind(gave_1, gave_2))\nprint(\"2:1 vs 3:1\", ttest_ind(gave_2, gave_3))\nprint(\"1:1 vs 3:1\", ttest_ind(gave_1, gave_3))\n\n$1:$1 vs $2:$1: t = -0.97, p = 0.335\n\n$2:$1 vs $3:$1: t = -0.05, p = 0.960\n\n$1:$1 vs $3:$1: t = -1.02, p = 0.310\n\nNone of these comparisons are statistically significant at the 5% level, which suggests that increasing the match ratio does not significantly increase the likelihood of giving—consistent with the authors’ conclusion on page 8 of the paper.\n\n\nRegression Analysis\nI also fit a linear regression model where the dependent variable is gave and the independent variables are ratio2 and ratio3, using $1:$1 match as the baseline:\nmatch_df = df[df['treatment'] == 1]\nmodel = smf.ols('gave ~ ratio2 + ratio3', data=match_df).fit()\nmodel.summary()\nRegression output summary:\n\nCoefficient on ratio2: 0.0019, p = 0.338\nCoefficient on ratio3: 0.0020, p = 0.313\n\nThe coefficients are small and statistically insignificant, confirming the same conclusion as the t-tests.\nDifference in Donation Rates\nWe also directly compute the difference in response rates:\nmatch_df = df[df['treatment'] == 1]\nmodel = smf.ols('gave ~ ratio2 + ratio3', data=match_df).fit()\ndiff_12 = gave_2.mean() - gave_1.mean()\ndiff_23 = gave_3.mean() - gave_2.mean()\ncoef_diff = model.params['ratio3'] - model.params['ratio2']\n\nFrom $1:$1 to $2:$1: +0.00188 (≈ 0.19 percentage points)\nFrom $2:$1 to $3:$1: +0.0001 (≈ 0.01 percentage points)\n\nFrom the regression coefficients:\n\nratio3 - ratio2 = +0.0001\n\nThese small and statistically insignificant changes indicate that donors do not respond more strongly to larger match ratios. Simply offering a match matters, but increasing the match ratio offers no additional benefit.\n\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nFull Sample Analysis (Including Non-Donors)\nFirst, I compare average donation amounts across all individuals, including those who gave $0. Using both a t-test and a linear regression:\nt_stat, p_val = ttest_ind(df[df['treatment'] == 1]['amount'], df[df['treatment'] == 0]['amount'])\nmodel = smf.ols('amount ~ treatment', data=df).fit()\n\nMean amount (control) = $0.813\n\nMean amount (treatment) = $0.967\n\nOLS coefficient on treatment = 0.1536\n\nt-statistic = 1.861\n\np-value = 0.063\n\nThe positive coefficient indicates that the treatment group gave slightly more on average. However, the p-value is just above the common 0.05 threshold, suggesting marginal significance. Most of the observed difference appears to be driven by the fact that more people gave in the treatment group, rather than those who gave giving significantly more.\n\n\nConditional on Donation\nI then limit the analysis to individuals who actually made a donation (gave == 1). I repeat the same steps:\ndonors_df = df[df['gave'] == 1]\nt2, p2 = ttest_ind(donors_df[donors_df['treatment'] == 1]['amount'],\n                   donors_df[donors_df['treatment'] == 0]['amount'])\nmodel2 = smf.ols('amount ~ treatment', data=donors_df).fit()\n\nMean amount (control) = $45.54\n\nMean amount (treatment) = $43.87\n\nOLS coefficient on treatment = -1.668\n\nt-statistic = -0.581\n\np-value = 0.561\n\nThese results suggest that conditional on donating, people in the treatment group did not give more, and actually gave slightly less on average (though not statistically significantly). The p-value of 0.561 indicates no meaningful difference.\nThis implies that the treatment’s impact was primarily at the extensive margin (increasing the number of people who gave), and not at the intensive margin (increasing donation amount conditional on giving). This result supports the original conclusion from Karlan & List (2007).\n\n\nHistograms of Donation Amounts\nBelow are two histograms showing the distribution of donation amounts among donors, separated by treatment group. The red dashed line represents the group mean.\n \n\n\n\n\n\n\nplot code\n\n\n\n\n\ntreatment_amount = donors_df[donors_df['treatment'] == 1]['amount']\ncontrol_amount = donors_df[donors_df['treatment'] == 0]['amount']\n\nplt.figure(figsize=(6,4))\nplt.hist(treatment_amount, bins=30, color='skyblue', edgecolor='black')\nplt.axvline(treatment_amount.mean(), color='red', linestyle='--', label=f'Mean = {treatment_amount.mean():.2f}')\nplt.title(\"Donation Amounts (Treatment Group)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"hist_treatment.png\")\nplt.show()\n\nplt.figure(figsize=(6,4))\nplt.hist(control_amount, bins=30, color='lightgreen', edgecolor='black')\nplt.axvline(control_amount.mean(), color='red', linestyle='--', label=f'Mean = {control_amount.mean():.2f}')\nplt.title(\"Donation Amounts (Control Group)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(\"hist_control.png\")\nplt.show()"
  },
  {
    "objectID": "HW1/project1/hw1_questions.html#simulation-experiment",
    "href": "HW1/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nTo demonstrate the Law of Large Numbers, I simulate two groups:\n\nA control group with a true probability of giving of p = 0.018\n\nA treatment group with a true probability of p = 0.022\n\nI draw 10,000 samples from each distribution and calculate the difference in donation outcome (1 or 0) for each pair. Then I compute the cumulative average of these 10,000 differences and plot the result below:\n\n\n\nLaw of Large Numbers Simulation\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\np_control = 0.018\np_treatment = 0.022\nn = 10000\nnp.random.seed(42)\n\ncontrol_sim = np.random.binomial(1, p_control, size=n)\ntreatment_sim = np.random.binomial(1, p_treatment, size=n)\ndiff_vector = treatment_sim - control_sim\ncumulative_avg = np.cumsum(diff_vector) / np.arange(1, n + 1)\n\nplt.figure(figsize=(8, 4))\nplt.plot(cumulative_avg, label='Cumulative Average Difference')\nplt.axhline(p_treatment - p_control, color='red', linestyle='--', label='True Difference (0.004)')\nplt.title('Law of Large Numbers Simulation')\nplt.xlabel('Sample Size')\nplt.ylabel('Cumulative Difference')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.savefig('law_of_large_numbers.png')\nplt.show()\n\n\n\n\n\nCentral Limit Theorem\n\nTo demonstrate the Central Limit Theorem (CLT), I simulate the difference in donation rates between the treatment (p = 0.022) and control (p = 0.018) groups at four different sample sizes: 50, 200, 500, and 1000. For each sample size, I repeat the experiment 1000 times, and in each trial I compute the average difference in donation rate:\n\nAt n = 50, the distribution is wide and irregular. The mean is slightly right of zero, but there’s a lot of noise. Zero is near the center.\nAt n = 200, the distribution starts to resemble a bell shape, but still has considerable spread.\nAt n = 500, the distribution becomes noticeably more symmetric, and the mean difference starts to stand out from zero.\nAt n = 1000, the distribution is tightly centered around 0.004, and zero is clearly in the left tail, indicating a consistent positive treatment effect.\n\n\n\n\nCLT Simulation\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\nn_simulations = 1000\nnp.random.seed(42)\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\naxs = axs.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    differences = []\n    for _ in range(n_simulations):\n        control = np.random.binomial(1, p_control, n)\n        treatment = np.random.binomial(1, p_treatment, n)\n        diff = treatment.mean() - control.mean()\n        differences.append(diff)\n    \n    mean_diff = np.mean(differences)\n\n    axs[i].hist(differences, bins=30, color='lightblue', edgecolor='black')\n    axs[i].axvline(0, color='red', linestyle='--', label='Zero')\n    axs[i].axvline(mean_diff, color='green', linestyle='-', label=f'Mean = {mean_diff:.4f}')\n    axs[i].set_title(f'Sample Size = {n}')\n    axs[i].set_xlabel('Difference in Donation Rate')\n    axs[i].set_ylabel('Frequency')\n    axs[i].legend()\n\nplt.suptitle('CLT Simulation: Sampling Distribution of Differences', fontsize=14)\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.savefig('clt_histograms_labeled.png')\nplt.show()\n\n\n\nI conclude that as sample size increases, the distribution of the average difference between treatment and control becomes more normal and more centered around the true mean—exactly as predicted by the Central Limit Theorem."
  },
  {
    "objectID": "HW1/project1/hw1_questions.html#conclusion",
    "href": "HW1/project1/hw1_questions.html#conclusion",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conclusion",
    "text": "Conclusion\nThis project gave me a chance to explore how matching donations affect charitable giving by replicating the results from Karlan & List (2007). After digging into the data, I found that people who received a matching offer were definitely more likely to donate—just like the original paper said. Even though the increase was small in percentage terms, it’s meaningful when you’re dealing with tens of thousands of people.\nOne thing that stood out to me was that higher match ratios (like 2:1 or 3:1) didn’t really help any more than the basic 1:1 offer. So the key seems to be just having a match at all—not how big the match is.\nWhen I looked at how much people gave, it turned out that the treatment group gave slightly more overall, but not because they gave more money individually. Instead, the bump came from more people deciding to give, not from people giving larger amounts.\nOverall, this was a great example of how subtle changes in message framing (like offering a match) can change real-world behavior. And it showed me how field experiments and behavioral economics can go hand-in-hand."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Junye's Website",
    "section": "",
    "text": "Hi, my name is Junye Fan. With a bachelor degree of Economics, I am currently pursuing my Master’s degree in Business Analytics at the University of California, San Diego."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Junye's Website",
    "section": "",
    "text": "Hi, my name is Junye Fan. With a bachelor degree of Economics, I am currently pursuing my Master’s degree in Business Analytics at the University of California, San Diego."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Junye's Website",
    "section": "🎓 Education",
    "text": "🎓 Education\n\nUniversity of California, San Diego\nMaster of Science in Business Analytics\nShanghai Normal University\nBachelor of Economics in Economics"
  },
  {
    "objectID": "index.html#my-story",
    "href": "index.html#my-story",
    "title": "Junye's Website",
    "section": "🎥 My Story",
    "text": "🎥 My Story\nMy favorite part of my academic and professional journey has been combining my business knowledge with data analytics to solve real-world problems. During my undergraduate studies in economics, I developed a strong foundation in data analysis techniques, learning key skills in SQL, business statistics and econometrics. However, I realized that hands-on experience was essential to applying these skills effectively, so I tried opportunities to intern with top organizations.\nIn my recent internship at Hellobike, one of China’s largest mobility platforms, I utilized SQL and Tableau to create dashboards that made complex data accessible and actionable for stakeholders. By conducting A/B testing and statistical analysis, I optimized user interface design, resulting in an 8.68% increase in click-through rates.\nThis experience underscored the importance of understanding user needs and translating data insights into clear, data-driven recommendations. Collaborating closely with cross-functional teams, I honed my ability to communicate complex data insights in an easily understandable way, further aligning our strategies with business goals.\nNow, as a Master’s student in Business Analytics, I am deepening my technical expertise and learning to leverage big data tools like Python, SQL, and cloud-hosted databases. I am excited about the opportunity to bring my skills in data analytics and my experience in client-focused solutions to industries and empower consumers and businesses to make informed decisions and thrive."
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Junye's Website",
    "section": "🛠 Skills",
    "text": "🛠 Skills\n\nQuery Languages: SQL\n\nAnalytics & Modeling: Python, R, Alteryx, MATLAB, STATA\n\nFront-End: JavaScript, HTML\n\nData Visualization: Tableau\n\nOthers: A/B Testing, Industry Research"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Junye's Website",
    "section": "📬 Contact",
    "text": "📬 Contact\n📧 E-mail 🔗 LinkedIn"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "HW1.html",
    "href": "HW1.html",
    "title": "My Projects",
    "section": "",
    "text": "Multinomial Logit Model\n\n\n\n\n\n\nJunye Fan\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nJunye Fan\n\n\nMay 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nJunye Fan\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "HW1/project3/hw3.html",
    "href": "HW1/project3/hw3.html",
    "title": "Junye's Website",
    "section": "",
    "text": "import pandas as pd\n\ndf=pd.read_csv('/Users/junyefan/Desktop/Graduate/Course/2025_Spring/MGTA_495_Marketing_Analytics/Week1/quartosite/HW1/project3/conjoint_data.csv')\n\n# Convert categorical variables into dummy variables\ndf['brand'] = df['brand'].astype(str)\ndf['ad'] = df['ad'].astype(str)\n\n# Create dummies for brand (Hulu as reference)\nbrand_dummies = pd.get_dummies(df['brand'], prefix='brand')\nbrand_dummies.drop(columns=['brand_H'], inplace=True)\n\n# Convert ad to binary: Yes = 1, No = 0\ndf['ad_binary'] = df['ad'].map({'Yes': 1, 'No': 0})\n\n# Combine into feature matrix\nX = pd.concat([\n    df[['resp', 'task', 'choice', 'price']],\n    brand_dummies,\n    df['ad_binary']\n], axis=1)\n\n# Rename for clarity\nX.rename(columns={\n    'brand_N': 'brand_netflix',\n    'brand_P': 'brand_prime'\n}, inplace=True)\n\n# Final structure\nX = X[['resp', 'task', 'choice', 'brand_netflix', 'brand_prime', 'ad_binary', 'price']]\n\n\nX\n\n\n\n\n\n\n\n\nresp\ntask\nchoice\nbrand_netflix\nbrand_prime\nad_binary\nprice\n\n\n\n\n0\n1\n1\n1\nTrue\nFalse\n1\n28\n\n\n1\n1\n1\n0\nFalse\nFalse\n1\n16\n\n\n2\n1\n1\n0\nFalse\nTrue\n1\n16\n\n\n3\n1\n2\n0\nTrue\nFalse\n1\n32\n\n\n4\n1\n2\n1\nFalse\nTrue\n1\n16\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2995\n100\n9\n1\nFalse\nFalse\n0\n12\n\n\n2996\n100\n9\n0\nFalse\nTrue\n1\n8\n\n\n2997\n100\n10\n0\nTrue\nFalse\n1\n28\n\n\n2998\n100\n10\n0\nFalse\nFalse\n0\n24\n\n\n2999\n100\n10\n1\nFalse\nFalse\n0\n16\n\n\n\n\n3000 rows × 7 columns\n\n\n\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Define input variables for estimation\nX_vars = ['brand_netflix', 'brand_prime', 'ad_binary', 'price']\nX_mat = X[X_vars].values\ny = X['choice'].values\n\n# Group each choice set (task within respondent) for probability denominator\ngroup_ids = X.groupby(['resp', 'task']).ngroup().values\n\n# Define the negative log-likelihood function\ndef neg_log_likelihood(beta):\n    utilities = X_mat @ beta\n    df_temp = pd.DataFrame({\n        'group': group_ids,\n        'utility': utilities,\n        'choice': y\n    })\n    df_temp['exp_u'] = np.exp(df_temp['utility'].astype(float))\n\n    # Compute denominator of softmax per choice set\n    group_sums = df_temp.groupby('group')['exp_u'].transform('sum')\n    df_temp['log_prob'] = df_temp['utility'] - np.log(group_sums)\n\n    # Only keep log probabilities of chosen alternatives\n    chosen_log_probs = df_temp[df_temp['choice'] == 1]['log_prob']\n    return -np.sum(chosen_log_probs)\n\n# Initial parameter guess\nbeta_init = np.zeros(X_mat.shape[1])\n\n# Minimize the negative log-likelihood\nresult = minimize(neg_log_likelihood, beta_init, method='BFGS')\n\n# Extract results\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Construct 95% confidence intervals\nz = 1.96\nconf_int = np.vstack([beta_hat - z * standard_errors, beta_hat + z * standard_errors]).T\n\n# Combine into a summary table\nparam_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']\nmle_summary = pd.DataFrame({\n    'parameter': param_names,\n    'estimate': beta_hat,\n    'std_error': standard_errors,\n    'ci_lower': conf_int[:, 0],\n    'ci_upper': conf_int[:, 1]\n})\n\nmle_summary.round(4)\n\n\n\n\n\n\n\n\nparameter\nestimate\nstd_error\nci_lower\nci_upper\n\n\n\n\n0\nbeta_netflix\n0.9412\n1.4043\n-1.8112\n3.6936\n\n\n1\nbeta_prime\n0.5016\n1.6607\n-2.7534\n3.7566\n\n\n2\nbeta_ads\n-0.7320\n0.3449\n-1.4080\n-0.0560\n\n\n3\nbeta_price\n-0.0995\n0.0084\n-0.1160\n-0.0830\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Reuse log-likelihood function from earlier\ndef log_likelihood(beta):\n    utilities = X_mat @ beta\n    df_temp = pd.DataFrame({\n        'group': group_ids,\n        'utility': utilities,\n        'choice': y\n    })\n    df_temp['exp_u'] = np.exp(pd.to_numeric(df_temp['utility'], errors='coerce'))\n    group_sums = df_temp.groupby('group')['exp_u'].transform('sum')\n    df_temp['log_prob'] = df_temp['utility'] - np.log(group_sums)\n    return np.sum(df_temp[df_temp['choice'] == 1]['log_prob'])\n\n# Log-prior function\ndef log_prior(beta):\n    return (\n        -0.5 * (beta[0]**2 + beta[1]**2 + beta[2]**2) / 25  # N(0, 5^2)\n        -0.5 * (beta[3]**2)  # N(0, 1^2) for price\n    )\n\n# Log-posterior\ndef log_posterior(beta):\n    return log_likelihood(beta) + log_prior(beta)\n\n# Proposal step: N(0, diag(...))\nproposal_sd = np.array([0.05, 0.05, 0.05, 0.005])\n\n# MCMC settings\nn_steps = 11000\nbeta_draws = np.zeros((n_steps, 4))\nbeta_current = np.zeros(4)\nlog_post_current = log_posterior(beta_current)\n\n# Run Metropolis-Hastings\nfor t in range(1, n_steps):\n    beta_proposal = beta_current + np.random.normal(0, proposal_sd)\n    log_post_proposal = log_posterior(beta_proposal)\n    accept_ratio = np.exp(log_post_proposal - log_post_current)\n\n    if np.random.rand() &lt; accept_ratio:\n        beta_current = beta_proposal\n        log_post_current = log_post_proposal\n\n    beta_draws[t, :] = beta_current\n\n# Drop first 1000 draws (burn-in)\nposterior_samples = beta_draws[1000:, :]\n\n\n\nparam_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']\n\n# Trace plot for beta_price\nplt.figure(figsize=(10, 4))\nplt.plot(posterior_samples[:, 3])\nplt.title(\"Trace Plot: beta_price\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Value\")\nplt.grid(True)\nplt.show()\n\n# Histogram for beta_price\nplt.hist(posterior_samples[:, 3], bins=30, edgecolor='k')\nplt.title(\"Posterior Distribution: beta_price\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\n# Summary statistics\nposterior_df = pd.DataFrame(posterior_samples, columns=param_names)\nposterior_summary = posterior_df.describe(percentiles=[0.025, 0.975]).T\nposterior_summary['mean'] = posterior_df.mean()\nposterior_summary['std'] = posterior_df.std()\nposterior_summary['2.5%'] = posterior_df.quantile(0.025)\nposterior_summary['97.5%'] = posterior_df.quantile(0.975)\n\nposterior_summary[['mean', 'std', '2.5%', '97.5%']].round(4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nstd\n2.5%\n97.5%\n\n\n\n\nbeta_netflix\n0.9427\n0.1096\n0.7156\n1.1490\n\n\nbeta_prime\n0.5026\n0.1060\n0.2870\n0.7043\n\n\nbeta_ads\n-0.7316\n0.0885\n-0.9113\n-0.5526\n\n\nbeta_price\n-0.0998\n0.0064\n-0.1127\n-0.0879\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nparam_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']\n\n# Use a consistent style\nplt.style.use('seaborn-whitegrid')\n\n# 🎯 Trace plot for beta_price\nplt.figure(figsize=(10, 4))\nplt.plot(posterior_samples[:, 3], color='royalblue', linewidth=1)\nplt.title(\"Trace Plot: beta_price\", fontsize=14)\nplt.xlabel(\"Iteration\", fontsize=12)\nplt.ylabel(\"Value\", fontsize=12)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# 🎯 Histogram for beta_price\nplt.figure(figsize=(8, 4))\nplt.hist(posterior_samples[:, 3], bins=30, color='lightcoral', edgecolor='black')\nplt.title(\"Posterior Distribution: beta_price\", fontsize=14)\nplt.xlabel(\"Value\", fontsize=12)\nplt.ylabel(\"Frequency\", fontsize=12)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nFile /opt/miniconda3/lib/python3.12/site-packages/matplotlib/style/core.py:137, in use(style)\n    136 try:\n--&gt; 137     style = _rc_params_in_file(style)\n    138 except OSError as err:\n\nFile /opt/miniconda3/lib/python3.12/site-packages/matplotlib/__init__.py:870, in _rc_params_in_file(fname, transform, fail_on_error)\n    869 rc_temp = {}\n--&gt; 870 with _open_file_or_url(fname) as fd:\n    871     try:\n\nFile /opt/miniconda3/lib/python3.12/contextlib.py:137, in _GeneratorContextManager.__enter__(self)\n    136 try:\n--&gt; 137     return next(self.gen)\n    138 except StopIteration:\n\nFile /opt/miniconda3/lib/python3.12/site-packages/matplotlib/__init__.py:847, in _open_file_or_url(fname)\n    846 fname = os.path.expanduser(fname)\n--&gt; 847 with open(fname, encoding='utf-8') as f:\n    848     yield f\n\nFileNotFoundError: [Errno 2] No such file or directory: 'seaborn-whitegrid'\n\nThe above exception was the direct cause of the following exception:\n\nOSError                                   Traceback (most recent call last)\nCell In[10], line 6\n      3 param_names = ['beta_netflix', 'beta_prime', 'beta_ads', 'beta_price']\n      5 # Use a consistent style\n----&gt; 6 plt.style.use('seaborn-whitegrid')\n      8 # 🎯 Trace plot for beta_price\n      9 plt.figure(figsize=(10, 4))\n\nFile /opt/miniconda3/lib/python3.12/site-packages/matplotlib/style/core.py:139, in use(style)\n    137         style = _rc_params_in_file(style)\n    138     except OSError as err:\n--&gt; 139         raise OSError(\n    140             f\"{style!r} is not a valid package style, path of style \"\n    141             f\"file, URL of style file, or library style name (library \"\n    142             f\"styles are listed in `style.available`)\") from err\n    143 filtered = {}\n    144 for k in style:  # don't trigger RcParams.__getitem__('backend')\n\nOSError: 'seaborn-whitegrid' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# 使用 matplotlib 内置可用样式\nplt.style.use('ggplot')  # 或 'bmh', 'classic', 'seaborn-v0_8-whitegrid'\n\n# Trace plot\nplt.figure(figsize=(10, 4))\nplt.plot(posterior_samples[:, 3], color='royalblue', linewidth=1)\nplt.title(\"Trace Plot: beta_price\", fontsize=14)\nplt.xlabel(\"Iteration\", fontsize=12)\nplt.ylabel(\"Value\", fontsize=12)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Histogram\nplt.figure(figsize=(8, 4))\nplt.hist(posterior_samples[:, 3], bins=30, color='lightcoral', edgecolor='black')\nplt.title(\"Posterior Distribution: beta_price\", fontsize=14)\nplt.xlabel(\"Value\", fontsize=12)\nplt.ylabel(\"Frequency\", fontsize=12)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nposterior_summary[['mean', 'std', '2.5%', '97.5%']] \n\n\n\n\n\n\n\n\nmean\nstd\n2.5%\n97.5%\n\n\n\n\nbeta_netflix\n0.942666\n0.109552\n0.715553\n1.149033\n\n\nbeta_prime\n0.502566\n0.105952\n0.287028\n0.704273\n\n\nbeta_ads\n-0.731626\n0.088473\n-0.911344\n-0.552625\n\n\nbeta_price\n-0.099829\n0.006369\n-0.112747\n-0.087856"
  },
  {
    "objectID": "HW1/project2/hw2_questions.html",
    "href": "HW1/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nThis dataset contains information on 1,500 mature engineering firms and includes variables on patenting activity, geographic location, firm age, and Blueprinty software usage. The primary goal is to assess whether firms using Blueprinty’s software are more successful in obtaining patents.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\npatents\nNumber of patents awarded to the firm over the past 5 years\n\n\nregion\nRegion where the firm is located (e.g., Northeast, Midwest, etc.)\n\n\nage\nNumber of years since the firm’s incorporation\n\n\niscustomer\nBlueprinty customer indicator (1 = firm uses Blueprinty, 0 = not)\n\n\n\n\n\n\n\nBased on the bar chart comparing the average number of patents, we observe that Blueprinty customers have a higher average number of patents awarded over the past five years compared to non-customers. Specifically, customers average 4.13 patents, while non-customers average closer to 3.5 (3.47). This suggests that firms using Blueprinty’s software may be more successful in obtaining patents.\nThis conclusion is further supported by the histogram comparing the full distribution of patent counts across customer groups. The distribution for Blueprinty customers is visibly shifted to the right, indicating a greater concentration of firms with 4 or more patents. In contrast, non-customers are more heavily represented in the 0 to 2 patent range, with relatively fewer firms reaching the higher patent counts observed among customers. The histogram also shows a longer right tail for customers, with more firms reaching double-digit patent counts.\nTaken together, both the difference in means and the shape of the distribution suggest that Blueprinty customers, on average, have higher patenting activity. However, it is important to note that this is a descriptive comparison. These differences may be influenced by other factors, such as firm age or geographic region, which are not yet accounted for in this analysis.\n\n\n\nPatent Count Distribution by Customer Status\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"blueprinty.csv\")\nsns.set_style(\"white\")\n\nplt.figure(figsize=(10, 6))\nax = sns.histplot(\n    data=df,\n    x='patents',\n    hue='iscustomer',\n    bins=30,\n    palette='Set2',\n    multiple='dodge'\n)\n\nfor bar in ax.patches:\n    bar.set_edgecolor(\"black\")\n    bar.set_linewidth(1)\n\nplt.title(\"Patent Count Distribution by Customer Status\")\nplt.xlabel(\"Number of Patents (past 5 years)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\nplt.tight_layout()\nplt.show()\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nWhen comparing company age by customer status, we observe that Blueprinty customers are slightly older than non-customers. On average, customer firms have been incorporated for about 26.9 years, while non-customers average 26.1 years. Although the difference is small, it may suggest that Blueprinty customers are marginally more established or experienced.\n\n\n\nCompany Age Distribution by Customer Status\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nage_means = df.groupby(\"iscustomer\")[\"age\"].mean().rename(index={0: \"Non-Customer\", 1: \"Customer\"})\nprint(\"Mean Company Age:\\n\", age_means)\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x='age', hue='iscustomer', bins=30, palette='Set2', multiple='dodge')\nplt.title(\"Company Age Distribution by Customer Status\")\nplt.xlabel(\"Company Age (Years)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\nplt.tight_layout()\nplt.show()\n\n\n\nRegional differences, however, are much more pronounced. In the Northeast, more than 54% of firms are Blueprinty customers, making it the only region where customers outnumber non-customers. In all other regions—such as the Midwest, South, Southwest, and Northwest—Blueprinty customers represent less than 20% of firms. This shows that Blueprinty has a particularly strong presence in the Northeast, while adoption is much lower in other parts of the country.\n\n\n\nDistribution of Firms by Region and Customer Status\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nsns.histplot(\n    data=df,\n    x='region',\n    hue='iscustomer',\n    multiple='dodge',\n    shrink=0.8,\n    palette='Set2',\n    stat='count',\n    edgecolor='black'\n)\n\nplt.title(\"Distribution of Firms by Region and Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\n\nplt.grid(False)\n\nplt.tight_layout()\nplt.show()\n\n\n\nThese observations highlight important systematic differences between customers and non-customers. Since customer status is not randomly assigned, it’s essential to account for age and regional factors when evaluating the effect of Blueprinty software on patent outcomes.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nGiven that \\(Y \\sim \\text{Poisson}(\\lambda)\\), the likelihood function is:\n\\[\nL(\\lambda \\mid Y) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking logs, the log-likelihood function becomes:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln  # log(y!) for numerical stability\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf  # log likelihood is undefined for λ &lt;= 0\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='darkblue')\nplt.title(\"Log-Likelihood of Poisson Model\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.axvline(np.mean(Y), color='red', linestyle='--', label='Mean of Y')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\nLog-Likelihood of Poisson Model\n\n\n\nLet’s consider the log-likelihood of a Poisson model where \\(Y_i \\sim \\text{Poisson}(\\lambda)\\):\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n\\]\nTaking the first derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell(\\lambda)}{d\\lambda} = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSetting the derivative equal to zero to find the maximum likelihood estimate (MLE):\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\]\nSolve for \\(\\lambda\\):\n\\[\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThus, the MLE for \\(\\lambda\\) is simply the sample mean of \\(Y\\), which intuitively makes sense since the Poisson distribution has its mean equal to \\(\\lambda\\).\n\nWe used numerical optimization to estimate the maximum likelihood value of λ in the Poisson model. The result, λ̂ = 3.6847, matches exactly with the sample mean of the observed data. This confirms our analytical result that the MLE of λ is simply the average of Y in a Poisson setting.\n\n\n\n\n\n\ncode\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\nneg_loglik = lambda lmbda: -poisson_loglikelihood(lmbda, Y)\n\nresult = minimize_scalar(neg_loglik, bounds=(0.01, 20), method='bounded')\n\nlambda_mle = result.x\nprint(f\"MLE of λ (via optimization): {lambda_mle:.4f}\")\nprint(f\"Sample mean of Y (baseline): {np.mean(Y):.4f}\")\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\text{where} \\quad \\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nThis ensures that \\(\\lambda_i &gt; 0\\) for all \\(i\\). The log-likelihood function for this model is:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\left( -\\lambda_i + Y_i \\log(\\lambda_i) - \\log(Y_i!) \\right)\n= \\sum_{i=1}^n \\left( -\\exp(X_i^\\top \\beta) + Y_i X_i^\\top \\beta - \\log(Y_i!) \\right)\n\\]\nWe now implement this in Python.\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\n\n# Feature engineering\ndf[\"age_scaled\"] = df[\"age\"] / 10\ndf[\"age_sq_scaled\"] = (df[\"age\"] ** 2) / 100\n\n# Construct design matrix X\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age_scaled\", \"age_sq_scaled\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\nX_matrix = X.values\nY = df[\"patents\"].values\n\n\nI estimated a Poisson regression model where the number of patents is modeled as a function of firm age (scaled), age squared, customer status, and regional dummy variables. The fitted coefficients and their standard errors are shown in the table below:\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\nIntercept\n-0.509\n0.183\n\n\nAge (scaled)\n1.486\n0.139\n\n\nAge² (scaled)\n-0.297\n0.026\n\n\nIsCustomer\n0.208\n0.031\n\n\nNortheast\n0.029\n0.044\n\n\nNorthwest\n-0.018\n0.054\n\n\nSouth\n0.057\n0.053\n\n\nSouthwest\n0.051\n0.047\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n# Feature scaling\ndf[\"age_scaled\"] = df[\"age\"] / 10\ndf[\"age_sq_scaled\"] = (df[\"age\"] ** 2) / 100\n\n# Construct design matrix\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX = pd.concat([\n    pd.Series(1.0, index=df.index, name=\"intercept\"),\n    df[[\"age_scaled\", \"age_sq_scaled\", \"iscustomer\"]],\n    region_dummies\n], axis=1).astype(float)\nX_matrix = X.values\nY = df[\"patents\"].values\n\n# Define Poisson log-likelihood\ndef poisson_loglikelihood_beta(beta, Y, X):\n    XB = X @ beta\n    lambdas = np.exp(XB)\n    if np.any(np.isnan(lambdas)) or np.any(np.isinf(lambdas)):\n        return np.inf\n    loglik = np.sum(-lambdas + Y * XB - gammaln(Y + 1))\n    return -loglik  # minimize negative log-likelihood\n\n# Optimize\nbeta_start = np.zeros(X.shape[1])\nres = minimize(poisson_loglikelihood_beta, beta_start, args=(Y, X_matrix), method=\"BFGS\")\n\n# Extract coefficient estimates and standard errors\nbeta_hat = res.x\nhessian_inv = res.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Create result table\nresults = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X.columns)\n\nprint(results)\n\n\n\n\n\n\nTo validate our MLE results, I also fit the same Poisson regression model using Python’s statsmodels.api.GLM() function. The results closely match the custom optimization estimates, confirming the consistency and correctness of the likelihood-based approach.\nThe iscustomer coefficient is positive and statistically significant. Since the model uses a log link, we interpret the coefficient of 0.208 as follows:\n\nFirms using Blueprinty are expected to have approximately 23% more patents, all else equal, since \\(e^{0.208} \\approx 1.231\\).\n\nThis suggests that using Blueprinty’s software is associated with increased patent success.\nIn addition, the model suggests that company age has a positive effect on patenting up to a point (since the coefficient for age_scaled is positive), but the negative coefficient on age_sq_scaled indicates diminishing returns as firms get older. Regional effects appear small and are not statistically significant.\nOverall, the model supports the hypothesis that using Blueprinty’s software is associated with higher patenting activity, even after controlling for age and region.\n\n\n\nTo better interpret the practical effect of Blueprinty’s software on patenting outcomes, I simulate a counterfactual scenario. We use our estimated Poisson regression model to compare predicted outcomes for each firm under two conditions:\n\nScenario 1 (X_0): All firms are treated as non-customers (iscustomer = 0)\nScenario 2 (X_1): All firms are treated as Blueprinty customers (iscustomer = 1)\n\nI then compute the expected number of patents under each condition and take the average difference.\n\n\n\n\n\n\ncode\n\n\n\n\n\n# Create counterfactual design matrices\nX_0 = X.copy()\nX_1 = X.copy()\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected number of patents under each scenario\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\n# Calculate average effect\neffect_vector = y_pred_1 - y_pred_0\naverage_effect = np.mean(effect_vector)\nprint(f\"Average predicted increase in patents due to Blueprinty: {average_effect:.4f}\")\n\n\n\n\nBased on our counterfactual simulation using the estimated Poisson regression model, firms that use Blueprinty are predicted to receive 0.79 more patents, on average, over a five-year period compared to if they did not use the software.\n\n\n\nPredicted Patents under Counterfactual Scenarios\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\navg_y_0 = np.mean(y_pred_0)\navg_y_1 = np.mean(y_pred_1)\n\n\ncomparison_df = pd.DataFrame({\n    \"Scenario\": [\"All Non-Customers\", \"All Customers\"],\n    \"Average Predicted Patents\": [avg_y_0, avg_y_1]\n})\n\n\ncolors = sns.color_palette(\"Set2\")\n\nplt.figure(figsize=(6, 5))\nbars = plt.bar(\n    comparison_df[\"Scenario\"],\n    comparison_df[\"Average Predicted Patents\"],\n    color=[colors[0], colors[1]],\n    edgecolor=\"black\"\n)\n\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(\n        bar.get_x() + bar.get_width() / 2,\n        height + 0.05,\n        f\"{height:.2f}\",\n        ha='center',\n        va='bottom',\n        fontsize=11\n    )\n\nplt.title(\"Predicted Average Number of Patents\\nUnder Counterfactual Scenarios\")\nplt.ylabel(\"Average Predicted Patents\")\nplt.ylim(0, max(avg_y_0, avg_y_1) + 1)\nplt.tight_layout()\nplt.show()\n\n\n\nThis effect holds after controlling for firm age and regional differences, suggesting that Blueprinty’s software is associated with a meaningful increase in patenting success."
  },
  {
    "objectID": "HW1/project2/hw2_questions.html#blueprinty-case-study",
    "href": "HW1/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nThis dataset contains information on 1,500 mature engineering firms and includes variables on patenting activity, geographic location, firm age, and Blueprinty software usage. The primary goal is to assess whether firms using Blueprinty’s software are more successful in obtaining patents.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\npatents\nNumber of patents awarded to the firm over the past 5 years\n\n\nregion\nRegion where the firm is located (e.g., Northeast, Midwest, etc.)\n\n\nage\nNumber of years since the firm’s incorporation\n\n\niscustomer\nBlueprinty customer indicator (1 = firm uses Blueprinty, 0 = not)\n\n\n\n\n\n\n\nBased on the bar chart comparing the average number of patents, we observe that Blueprinty customers have a higher average number of patents awarded over the past five years compared to non-customers. Specifically, customers average 4.13 patents, while non-customers average closer to 3.5 (3.47). This suggests that firms using Blueprinty’s software may be more successful in obtaining patents.\nThis conclusion is further supported by the histogram comparing the full distribution of patent counts across customer groups. The distribution for Blueprinty customers is visibly shifted to the right, indicating a greater concentration of firms with 4 or more patents. In contrast, non-customers are more heavily represented in the 0 to 2 patent range, with relatively fewer firms reaching the higher patent counts observed among customers. The histogram also shows a longer right tail for customers, with more firms reaching double-digit patent counts.\nTaken together, both the difference in means and the shape of the distribution suggest that Blueprinty customers, on average, have higher patenting activity. However, it is important to note that this is a descriptive comparison. These differences may be influenced by other factors, such as firm age or geographic region, which are not yet accounted for in this analysis.\n\n\n\nPatent Count Distribution by Customer Status\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"blueprinty.csv\")\nsns.set_style(\"white\")\n\nplt.figure(figsize=(10, 6))\nax = sns.histplot(\n    data=df,\n    x='patents',\n    hue='iscustomer',\n    bins=30,\n    palette='Set2',\n    multiple='dodge'\n)\n\nfor bar in ax.patches:\n    bar.set_edgecolor(\"black\")\n    bar.set_linewidth(1)\n\nplt.title(\"Patent Count Distribution by Customer Status\")\nplt.xlabel(\"Number of Patents (past 5 years)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\nplt.tight_layout()\nplt.show()\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nWhen comparing company age by customer status, we observe that Blueprinty customers are slightly older than non-customers. On average, customer firms have been incorporated for about 26.9 years, while non-customers average 26.1 years. Although the difference is small, it may suggest that Blueprinty customers are marginally more established or experienced.\n\n\n\nCompany Age Distribution by Customer Status\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nage_means = df.groupby(\"iscustomer\")[\"age\"].mean().rename(index={0: \"Non-Customer\", 1: \"Customer\"})\nprint(\"Mean Company Age:\\n\", age_means)\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data=df, x='age', hue='iscustomer', bins=30, palette='Set2', multiple='dodge')\nplt.title(\"Company Age Distribution by Customer Status\")\nplt.xlabel(\"Company Age (Years)\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\nplt.tight_layout()\nplt.show()\n\n\n\nRegional differences, however, are much more pronounced. In the Northeast, more than 54% of firms are Blueprinty customers, making it the only region where customers outnumber non-customers. In all other regions—such as the Midwest, South, Southwest, and Northwest—Blueprinty customers represent less than 20% of firms. This shows that Blueprinty has a particularly strong presence in the Northeast, while adoption is much lower in other parts of the country.\n\n\n\nDistribution of Firms by Region and Customer Status\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nsns.histplot(\n    data=df,\n    x='region',\n    hue='iscustomer',\n    multiple='dodge',\n    shrink=0.8,\n    palette='Set2',\n    stat='count',\n    edgecolor='black'\n)\n\nplt.title(\"Distribution of Firms by Region and Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title='Is Customer', labels=['Non-Customer', 'Customer'])\n\nplt.grid(False)\n\nplt.tight_layout()\nplt.show()\n\n\n\nThese observations highlight important systematic differences between customers and non-customers. Since customer status is not randomly assigned, it’s essential to account for age and regional factors when evaluating the effect of Blueprinty software on patent outcomes.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nGiven that \\(Y \\sim \\text{Poisson}(\\lambda)\\), the likelihood function is:\n\\[\nL(\\lambda \\mid Y) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking logs, the log-likelihood function becomes:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln  # log(y!) for numerical stability\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf  # log likelihood is undefined for λ &lt;= 0\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='darkblue')\nplt.title(\"Log-Likelihood of Poisson Model\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.axvline(np.mean(Y), color='red', linestyle='--', label='Mean of Y')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\nLog-Likelihood of Poisson Model\n\n\n\nLet’s consider the log-likelihood of a Poisson model where \\(Y_i \\sim \\text{Poisson}(\\lambda)\\):\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n\\]\nTaking the first derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell(\\lambda)}{d\\lambda} = \\sum_{i=1}^n \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSetting the derivative equal to zero to find the maximum likelihood estimate (MLE):\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\]\nSolve for \\(\\lambda\\):\n\\[\n\\lambda_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThus, the MLE for \\(\\lambda\\) is simply the sample mean of \\(Y\\), which intuitively makes sense since the Poisson distribution has its mean equal to \\(\\lambda\\).\n\nWe used numerical optimization to estimate the maximum likelihood value of λ in the Poisson model. The result, λ̂ = 3.6847, matches exactly with the sample mean of the observed data. This confirms our analytical result that the MLE of λ is simply the average of Y in a Poisson setting.\n\n\n\n\n\n\ncode\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\nneg_loglik = lambda lmbda: -poisson_loglikelihood(lmbda, Y)\n\nresult = minimize_scalar(neg_loglik, bounds=(0.01, 20), method='bounded')\n\nlambda_mle = result.x\nprint(f\"MLE of λ (via optimization): {lambda_mle:.4f}\")\nprint(f\"Sample mean of Y (baseline): {np.mean(Y):.4f}\")\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\text{where} \\quad \\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nThis ensures that \\(\\lambda_i &gt; 0\\) for all \\(i\\). The log-likelihood function for this model is:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\left( -\\lambda_i + Y_i \\log(\\lambda_i) - \\log(Y_i!) \\right)\n= \\sum_{i=1}^n \\left( -\\exp(X_i^\\top \\beta) + Y_i X_i^\\top \\beta - \\log(Y_i!) \\right)\n\\]\nWe now implement this in Python.\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\n\n# Feature engineering\ndf[\"age_scaled\"] = df[\"age\"] / 10\ndf[\"age_sq_scaled\"] = (df[\"age\"] ** 2) / 100\n\n# Construct design matrix X\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age_scaled\", \"age_sq_scaled\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\nX_matrix = X.values\nY = df[\"patents\"].values\n\n\nI estimated a Poisson regression model where the number of patents is modeled as a function of firm age (scaled), age squared, customer status, and regional dummy variables. The fitted coefficients and their standard errors are shown in the table below:\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\nIntercept\n-0.509\n0.183\n\n\nAge (scaled)\n1.486\n0.139\n\n\nAge² (scaled)\n-0.297\n0.026\n\n\nIsCustomer\n0.208\n0.031\n\n\nNortheast\n0.029\n0.044\n\n\nNorthwest\n-0.018\n0.054\n\n\nSouth\n0.057\n0.053\n\n\nSouthwest\n0.051\n0.047\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n# Feature scaling\ndf[\"age_scaled\"] = df[\"age\"] / 10\ndf[\"age_sq_scaled\"] = (df[\"age\"] ** 2) / 100\n\n# Construct design matrix\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX = pd.concat([\n    pd.Series(1.0, index=df.index, name=\"intercept\"),\n    df[[\"age_scaled\", \"age_sq_scaled\", \"iscustomer\"]],\n    region_dummies\n], axis=1).astype(float)\nX_matrix = X.values\nY = df[\"patents\"].values\n\n# Define Poisson log-likelihood\ndef poisson_loglikelihood_beta(beta, Y, X):\n    XB = X @ beta\n    lambdas = np.exp(XB)\n    if np.any(np.isnan(lambdas)) or np.any(np.isinf(lambdas)):\n        return np.inf\n    loglik = np.sum(-lambdas + Y * XB - gammaln(Y + 1))\n    return -loglik  # minimize negative log-likelihood\n\n# Optimize\nbeta_start = np.zeros(X.shape[1])\nres = minimize(poisson_loglikelihood_beta, beta_start, args=(Y, X_matrix), method=\"BFGS\")\n\n# Extract coefficient estimates and standard errors\nbeta_hat = res.x\nhessian_inv = res.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Create result table\nresults = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X.columns)\n\nprint(results)\n\n\n\n\n\n\nTo validate our MLE results, I also fit the same Poisson regression model using Python’s statsmodels.api.GLM() function. The results closely match the custom optimization estimates, confirming the consistency and correctness of the likelihood-based approach.\nThe iscustomer coefficient is positive and statistically significant. Since the model uses a log link, we interpret the coefficient of 0.208 as follows:\n\nFirms using Blueprinty are expected to have approximately 23% more patents, all else equal, since \\(e^{0.208} \\approx 1.231\\).\n\nThis suggests that using Blueprinty’s software is associated with increased patent success.\nIn addition, the model suggests that company age has a positive effect on patenting up to a point (since the coefficient for age_scaled is positive), but the negative coefficient on age_sq_scaled indicates diminishing returns as firms get older. Regional effects appear small and are not statistically significant.\nOverall, the model supports the hypothesis that using Blueprinty’s software is associated with higher patenting activity, even after controlling for age and region.\n\n\n\nTo better interpret the practical effect of Blueprinty’s software on patenting outcomes, I simulate a counterfactual scenario. We use our estimated Poisson regression model to compare predicted outcomes for each firm under two conditions:\n\nScenario 1 (X_0): All firms are treated as non-customers (iscustomer = 0)\nScenario 2 (X_1): All firms are treated as Blueprinty customers (iscustomer = 1)\n\nI then compute the expected number of patents under each condition and take the average difference.\n\n\n\n\n\n\ncode\n\n\n\n\n\n# Create counterfactual design matrices\nX_0 = X.copy()\nX_1 = X.copy()\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected number of patents under each scenario\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\n# Calculate average effect\neffect_vector = y_pred_1 - y_pred_0\naverage_effect = np.mean(effect_vector)\nprint(f\"Average predicted increase in patents due to Blueprinty: {average_effect:.4f}\")\n\n\n\n\nBased on our counterfactual simulation using the estimated Poisson regression model, firms that use Blueprinty are predicted to receive 0.79 more patents, on average, over a five-year period compared to if they did not use the software.\n\n\n\nPredicted Patents under Counterfactual Scenarios\n\n\n\n\n\n\n\n\nplot code\n\n\n\n\n\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\navg_y_0 = np.mean(y_pred_0)\navg_y_1 = np.mean(y_pred_1)\n\n\ncomparison_df = pd.DataFrame({\n    \"Scenario\": [\"All Non-Customers\", \"All Customers\"],\n    \"Average Predicted Patents\": [avg_y_0, avg_y_1]\n})\n\n\ncolors = sns.color_palette(\"Set2\")\n\nplt.figure(figsize=(6, 5))\nbars = plt.bar(\n    comparison_df[\"Scenario\"],\n    comparison_df[\"Average Predicted Patents\"],\n    color=[colors[0], colors[1]],\n    edgecolor=\"black\"\n)\n\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(\n        bar.get_x() + bar.get_width() / 2,\n        height + 0.05,\n        f\"{height:.2f}\",\n        ha='center',\n        va='bottom',\n        fontsize=11\n    )\n\nplt.title(\"Predicted Average Number of Patents\\nUnder Counterfactual Scenarios\")\nplt.ylabel(\"Average Predicted Patents\")\nplt.ylim(0, max(avg_y_0, avg_y_1) + 1)\nplt.tight_layout()\nplt.show()\n\n\n\nThis effect holds after controlling for firm age and regional differences, suggesting that Blueprinty’s software is associated with a meaningful increase in patenting success."
  },
  {
    "objectID": "HW1/project2/hw2_questions.html#airbnb-case-study",
    "href": "HW1/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\n\nExploratory Data Analysis\n\nThe distribution of the number of reviews is highly right-skewed, with most listings receiving fewer than 20 reviews.\nListings that are instant bookable appear to have more reviews, based on a comparison of medians in the boxplot.\n\n\n\n\nHistogram of Reviews\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nvars_used = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\ndf = df[vars_used]\n\n# Drop rows with missing values\ndf_clean = df.dropna()\n\n# Convert variables\ndf_clean[\"instant_bookable\"] = (df_clean[\"instant_bookable\"] == \"t\").astype(int)\ndf_dummies = pd.get_dummies(df_clean, columns=[\"room_type\"], drop_first=True)\ndf_dummies.columns = df_dummies.columns.str.replace(\" \", \"_\")\n\n# Poisson regression\nformula = (\n    \"number_of_reviews ~ bathrooms + bedrooms + price + \"\n    \"review_scores_cleanliness + review_scores_location + \"\n    \"review_scores_value + instant_bookable + \"\n    \"room_type_Private_room + room_type_Shared_room\"\n)\nmodel = glm(formula=formula, data=df_dummies, family=sm.families.Poisson()).fit()\nresults_df = pd.DataFrame({\n    \"Coefficient\": model.params,\n    \"Std. Error\": model.bse\n})\nprint(results_df)\n\nreviews = df_clean[\"number_of_reviews\"]\n\nplt.figure(figsize=(8, 5))\nsns.histplot(reviews, bins=50, kde=True)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Frequency\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nBoxplot by Instant Bookable\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\nplt.figure(figsize=(8, 5))\nsns.boxplot(x=\"instant_bookable\", y=\"number_of_reviews\", data=df_clean)\nplt.title(\"Number of Reviews by Instant Bookable\")\nplt.xlabel(\"Instant Bookable (0 = No, 1 = Yes)\")\nplt.ylabel(\"Number of Reviews\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nData Cleaning\n\nDropped listings with missing values in any relevant modeling variable.\nConverted instant_bookable to a binary variable (1 for ‘t’, 0 for ‘f’).\nEncoded room_type as dummy variables, using “Entire home/apt” as the reference category.\n\n\n\nPoisson Regression Model\nI used the following covariates to explain variation in the number of reviews: - bathrooms, bedrooms, price - review_scores_cleanliness, review_scores_location, review_scores_value - instant_bookable (binary) - room_type_Private_room, room_type_Shared_room (dummies)\n\n\n\n\n\n\ncode\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import glm\n\ndf = pd.read_csv(\"airbnb.csv\")\nvars_used = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\ndf = df[vars_used]\n\n# Drop rows with missing values\ndf_clean = df.dropna()\n\n# Convert variables\ndf_clean[\"instant_bookable\"] = (df_clean[\"instant_bookable\"] == \"t\").astype(int)\ndf_dummies = pd.get_dummies(df_clean, columns=[\"room_type\"], drop_first=True)\ndf_dummies.columns = df_dummies.columns.str.replace(\" \", \"_\")\n\n# Poisson regression\nformula = (\n    \"number_of_reviews ~ bathrooms + bedrooms + price + \"\n    \"review_scores_cleanliness + review_scores_location + \"\n    \"review_scores_value + instant_bookable + \"\n    \"room_type_Private_room + room_type_Shared_room\"\n)\nmodel = glm(formula=formula, data=df_dummies, family=sm.families.Poisson()).fit()\nresults_df = pd.DataFrame({\n    \"Coefficient\": model.params,\n    \"Std. Error\": model.bse\n})\nprint(results_df)\n\n\n\n\n\nVariable Interpretations\nThe estimated model is:\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nInterpretation\n\n\n\n\nIntercept\n3.572\nThe baseline log expected number of reviews for a reference listing\n\n\nbathrooms\n-0.124\nEach additional bathroom is associated with a ~11.6% decrease in expected reviews: \\(e^{-0.124} \\approx 0.883\\)\n\n\nbedrooms\n+0.075\nEach additional bedroom is associated with ~7.8% increase in reviews: \\(e^{0.075} \\approx 1.078\\)\n\n\nprice\n-0.000014\nThe effect is small and negative; more expensive listings receive slightly fewer reviews\n\n\nreview_scores_cleanliness\n+0.113\nA one-point increase in cleanliness score leads to ~12% more reviews: \\(e^{0.113} \\approx 1.12\\)\n\n\nreview_scores_location\n-0.077\nA higher location score is associated with slightly fewer reviews; possible saturation in popular areas\n\n\nreview_scores_value\n-0.092\nHigher value scores correspond to fewer reviews; possibly reflects different guest expectations\n\n\ninstant_bookable\n+0.334\nInstant booking listings receive ~40% more reviews: \\(e^{0.334} \\approx 1.40\\)\n\n\nroom_type_Private_room\n-0.015\nLittle difference from entire homes; slightly fewer reviews\n\n\nroom_type_Shared_room\n-0.252\nShared rooms get ~22% fewer reviews: \\(e^{-0.252} \\approx 0.78\\)\n\n\n\n\n\nSummary\n\nThe strongest positive driver of review volume is instant booking, which increases expected review counts by roughly 40%.\nCleanliness and number of bedrooms are also positively associated with reviews.\nShared rooms significantly underperform compared to entire homes.\nPrice, location, and value scores show weak or negative associations, possibly due to nonlinear effects or omitted variables.\n\n\nBecause the model uses a log link, the effect of each variable is multiplicative. A coefficient \\(\\beta\\) implies that the number of reviews changes by a factor of \\(e^\\beta\\) for a one-unit increase in that variable, holding all others constant."
  }
]